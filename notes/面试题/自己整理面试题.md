---
typora-copy-images-to: imgs
---



- 多看优秀的场景设计文章

- [牛客网 Java 学习路线总结](https://www.nowcoder.com/discuss/551083645553954816)

- [小傅哥简历指导](https://wx.zsxq.com/dweb2/index/columns/48411118851818)

- [小傅哥Java基础考题自测 100 题（★★★）](https://docs.qq.com/form/page/DT3JKRWN5bkh4U2J2#/fill)

- 性能优化：https://wx.zsxq.com/dweb2/index/topic_detail/214251284118481

- sql 刷题建议牛客网

- Kafka、RocketMQ、RabbitMQ 学习资源推荐：https://articles.zsxq.com/id_i16imeqka5ip.html

- 算法刷哪些题？：https://wx.zsxq.com/dweb2/index/topic_detail/588124821214444

- SpringCloud常见组件：https://articles.zsxq.com/id_mcfcqao0ptha.html

- 技术资源/技术问题：

  1. 支付系统学习资料和开源项目推荐：<https://t.zsxq.com/11wxKtaWK>
  2. 腾讯课堂-马士兵教育-源码五班：<https://t.zsxq.com/11OCvwDA8>
  3. 代码场景设计实践专栏 ：<https://t.zsxq.com/11DvHH3Bk>
  4. 《深入理解缓存原理与实战设计》：<https://t.zsxq.com/11LNrIgaX>
  5. 沈剑老师的架构师之路干货精选：<https://t.zsxq.com/11ndmliJW>
  6. 常见的脱敏工具总结：<https://t.zsxq.com/11rvTAIRR>
  7. OHC（堆外缓存）资料推荐：<https://t.zsxq.com/11G56hsji>
  8. 如何通过IP 地址找到用户的地址：<https://t.zsxq.com/11BDUhHVh>
  9. 转转游戏的账号订单流程重构之路：<https://t.zsxq.com/110G8VIov>
  10. 从1到亿，如何玩好异步消息？CQRS架构下的异步事件治理实践：<https://t.zsxq.com/11hHcUJ2s>
  11. Redis学习路线的补充：[【学习笔记】Redis学习路线的补充](https://t.zsxq.com/115otI0gs)
  12. 算法学习路径：[【学习笔记】算法学习路径](https://t.zsxq.com/117aydfBp)
  13. 计算机网络学习体系&计划：[计算机网络学习体系&计划](https://t.zsxq.com/11vSZ0eQf)
  14. Devops学习资料推荐：<https://t.zsxq.com/115sbW6RX>

- Bilibili 高并发下点赞功能的实现：[文章链接](https://mp.weixin.qq.com/s/mwZQYk1vJu6rOGzxuNeGyg)

- [典型系统设计案例](https://articles.zsxq.com/id_juvseuue9xa3.html)

- [通用架构设计](https://insights.thoughtworks.cn/common-architecture-design/)

- 集合源码

  这周抽时间完善了一下JavaGuide上集合源码分析部分的内容，新增了 LinkedList  和  CopyOnWriteArrayList 核心源码解析，清晰易懂![呲牙](https://wx.zsxq.com/dweb2/assets/images/emoji/expression_14.png)

  1、ArrayList 源码分析：[ArrayList 源码分析 | JavaGuide(Java面试   学习指南)](https://javaguide.cn/java/collection/arraylist-source-code.html)
  2、LinkedList 源码分析：[LinkedList 源码分析 | JavaGuide(Java面试   学习指南)](https://javaguide.cn/java/collection/linkedlist-source-code.html)
  3、HashMap 源码分析：[HashMap 源码分析 | JavaGuide(Java面试   学习指南)](https://javaguide.cn/java/collection/hashmap-source-code.html)
  4、ConcurrentHashMap 源码分析：[ConcurrentHashMap 源码分析 | JavaGuide(Java面试   学习指南)](https://javaguide.cn/java/collection/concurrent-hash-map-source-code.html)
  5、CopyOnWriteArrayList 源码分析：[CopyOnWriteArrayList 源码分析 | JavaGuide(Java面试   学习指...](https://javaguide.cn/java/collection/copyonwritearraylist-source-code.html)

- [两年经验面试题](https://articles.zsxq.com/id_8awjlbk832bx.html)

- 海量数据处理：https://blog.csdn.net/qq_44797267/article/details/120228705、https://blog.csdn.net/v_july_v/article/details/7382693




待办：

- ConcurrentHashMap 源码分析整理



观看记录

Java Guide：2023-05-15 09:03





# 简历编写

- 使用  Redis+Caffeine 多级缓存优化热门数据（如首页、热门商品）的访问，解决了缓存击穿和穿透问题，查询速度毫秒级，QPS 30w+。
- 性能优化指南

很多球友不知道如何从性能角度优化项目，这里推荐几个比较容易实现的点，单体项目和分布式项目都适用，涉及到多线程、JVM、数据库/缓存、数据结构优化这4个常见的性能优化方向：

多线程方向优化：

1、如何在 SpringBoot 中使用异步方法优化 Service 逻辑提高接口响应速度? - 2022: [如何在SpringBoot中使用异步方法优化Service逻辑提高接口响应速度?_springboo...](https://blog.csdn.net/weixin_43441509/article/details/119855613)
2、asyncTool: [asyncTool: 解决任意的多线程并行、串行、阻塞、依赖、回调的并行框架，可以任意组合各线程的执...](https://gitee.com/jd-platform-opensource/asyncTool)（京东零售开源的一个并行框架，里面大量使用到了 CompletableFuture ，可以学习其精华来运用在自己的项目上）
3、CompletableFuture 原理与实践-外卖商家端 API 的异步化 - 美团技术团队- 2022: [CompletableFuture原理与实践-外卖商家端API的异步化 - 知乎](https://zhuanlan.zhihu.com/p/515993095)
4、简述 CompletableFuture 异步任务编排 - 掘金 - 2022: [简述CompletableFuture异步任务编排 - 掘金](https://juejin.cn/post/7168261825165787149)

JVM方向优化：

5、JVM 调优实战 - 掘金 - 2022: [JVM调优-JVM调优实践一 - 掘金](https://juejin.cn/post/7128377003224334373)

数据库/缓存方向：

6、MySQL 索引与查询优化 - 西召 - 2019: [MySQL索引与查询优化 - 掘金](https://juejin.cn/post/6844903818056974350)
7、基于 Spring 接口，集成 Caffeine+Redis 两级缓存 - 码农参上 - 2022: [基于Spring接口，集成Caffeine Redis两级缓存 - 掘金](https://juejin.cn/post/7117497031714865159)
8、J2Cache: [J2Cache: Java 两级缓存框架，可以让应用支持两级缓存框架 ehcache(Caffein...](https://gitee.com/ld/J2Cache)（基于内存和 Redis 的开源两级 Java 缓存框架）
9、MySQL 读写分离实战 - 遇见0和1 - 2023：[MySQL8读写分离集群](https://mp.weixin.qq.com/s/N81ES1TGmBb3DJMyROvGBQ)
10、MaxScale 实现 MySQL读写分离 - 爱可生开源社区 - 2022：[技术分享 | MaxScale 实现 MySQL读写分离](https://mp.weixin.qq.com/s/lInWPsTfrywUjT6VgZCbyQ)

数据结构结构方向优化：

11、换个数据结构，一不小心节约了 591 台机器！ - why 技术 - 2022: [换个数据结构，一不小心节约了 591 台机器！ - why技术 - 博客园](https://www.cnblogs.com/thisiswhy/p/16066548.html)（将系统中的本地缓存实现由 HashMap 替换为了 IntObjectHashMap(这个类出自 Netty)节约了2364C 的服务器资源。）
12、这个队列的思路是真的好，现在它是我简历上的亮点了。 - why 技术 - 2022: [java - 这个队列的思路是真的好，现在它是我简历上的亮点了。 - 个人文章 - SegmentF...](https://segmentfault.com/a/1190000041943100)







# 场景题

## 场景题：匹配系统，一个用户多次匹配，要求不能匹配重复的人，之后双方都点击确认开始聊天。

   答：对于用户不可以匹配重复的人，可以使用 bitmap 来做。





## 实现短信验证码生成和验证两个功能，只能使用Java语言，不能使用Redis等外部存储工具。









## 如果抽奖项目真正应用于实际还有哪些地方需要改进？

答：目前架构、设计、实现上已经做了很多的优化方案，但上线肯定是会有很多的业务场景的细节，需要技术处理。为了更好的适配这些场景，在实现上留出扩展、在系统上添加监控、在日志上做好排查等。

代码的扩展性、系统的可用性





## 敏感词库的设计，要求增删改查敏感词。敏感词文本匹配，敏感词一万个，文本长度在 20 - 1000 

答：使用 trie 树来实现敏感词库的设计，可以利用字符串公共前缀来节约存储空间。

生成 trie 树结构如下：

![1697343847793](imgs/1697343847793.png)

## 1亿数据只有 1gb 内存怎么去重？

答：问题的本质是`海量数据去重`，解决方案有两种 bitmap、布隆过滤器。

方案一：bitmap

对于 1 亿的数据来说，如果直接将所有数据读入内存使用 bitmap 来去重的话，对每条数据使用 1 个 bit 标记是否存在即可，1 亿 bit ≈ 12MB，对于一条数据 a 来说，会在 bitmap 中计算出他所放入的下标 `x`，之后将 `x` 这个位置标记为1，这样判断一个数据是否存在，只占用 1 bit。



bitmap 方案适用场景：

- bitmap 适合值域较小的场景，如果值域较大会导致计算出在 bitmap 数组中的下标过大，比较占用存储空间
- 适合数据密集场景，对于数据稀疏场景比较浪费存储空间，比如数据a下标为0，但是数据b下标为1000000，两个数据中间并没有数据，但是却需要占用存储空间。





方案二：布隆过滤器

当值域较大的情况下，可以使用布隆过滤器进一步压缩 bitmap 的存储空间。

在布隆过滤器中，对一个数据a，布隆过滤器会使用 `k` 个哈希函数，计算出 `k` 个哈希值，在 bitmap 中将这 k 个位置都标记为1，来表示这个数据存在。



布隆过滤器适用场景：

- 适用于不严格去重的场景，因为布隆过滤器的特性会导致存在误判率，当判断为true时，该数据可能在集合中；当判断为 false 时，该数据一定不在集合中。
- Java中可以使用第三方库来实现布隆过滤器，常见的有Google Guava库和Apache Commons库以及Redis。






## 项目的登陆密码怎么存储，用的什么加密算法，为什么用 MD5？

答：项目登陆密码都会通过 `MD5 + 加盐` 操作对明文密码加密存储在数据库中。

MD5 会对每一个铭文密码生成一个对应的固定密码，虽然 MD5 不可逆，但是可以被暴力枚举出来，所以在 MD5 的基础上还会添加加盐操作，通过在密码任意固定位置插入特定的字符串，让散列后的结果和使用原始密码的散列结果不相符。









## 订单到期后，如何关闭订单？

 答：参考文章：https://mp.weixin.qq.com/s/BG1PqUWX0XwJX6aMCXCgvw

   - 方案一：定时任务，定时去扫描所有到期的订单，然后执行关单的动作。

     - 缺点：
       1. 时间不精确，可能订单已经到了超时时间，但是还没有到定时任务执行时间，导致订单关闭时间比超时时间晚。
       2. 无法处理大订单量，如果订单量较大，会导致定时任务执行时间很长，导致后边订单被扫描到的时间很晚。
       3. 对数据库造成压力，定时任务集中扫描表，会大量占用数据库io，可以将定时任务将其他正常业务做好隔离
       4. 分库分表问题，订单系统，在订单量大时会分库分表，在分库分表中进行全表扫描很不推荐
     - 适用场景：
       1. 对过期时间精度要求不高，业务量不大的场景

   - 方案二：JDK自带的延迟队列，`DelayQueue`，在用户创建订单时，把订单加到 `DelayQueue` 中，此外，还需要一个常驻任务不断从队列读取已经超时的订单，并进行关闭，之后再将该订单从队列中删除。

     该方案需要有一个线程添加 `while(true)` 循环，才能确保任务不断执行并及时取出超时订单。

     - 缺点：
       1. 该方案是基于 JVM 内存的，一旦机器重启，会导致数据消失，虽然可以配合数据库的持久化一起使用，但是应用一般都是集群部署，集群中的多台实例的 `DelayQueue` 如何配合也是一个很大的问题。
       2. 当订单量过大时，可能会导致 OOM 的问题。
     - 适用场景：
       1. 单机，订单量不大

   - 方案三：RockerMQ延迟消息，在订单创建好之后，发送一个延迟消息，指定延迟时间，在延迟时间到达之后，消息就会被消费者消费。

     - 缺点：
       1. RocketMQ的延迟时间不支持任意的，只支持：1s、5s、10s、30s，1m、2m等等（商业版支持任意时长）
     - 适用场景：RocketMQ支持延迟时间和我们所需延迟时间正好符合

   - 方案四：RabbitMQ插件，基于 rabbitmq_delayed_message_exchange 插件，该插件从 RabbitMQ 的 3.6.12 版本开始支持，该插件为官方开发的。

     在 RabbitMQ 中，我们设置一个消息，并且不去消费他，当过了存活时间之后，这个消息会变成死信，会被发送到死信队列中。

     在该插件中，消息并不会立即进入队列，而是先将他们保存到一个基于 Erlang 开发的 Mnesia 数据库，再通过一个定时器去查询需要被投递的消息，再投递到 x-delayed-message 队列中。

     - 适用场景：基于 RabbitMQ 插件的方式实现延迟消息，最大延长时间大概为 49 天，超过时间会被立即消费。可用性，性能都不错。

   - 方案五：Redis 过期监听，监听 key 的过期消息，在接收到过期消息之后，进行订单的关单操作。

     - 缺点：
       1. Redis 不保证 key 在过期时会被立即删除，也不保证消息能立即发出，因此存在消息延迟
       2. 在 Redis5.0 之前，这个消息是通过 PUB/SUB 模式发出的，不会进行持久化，如果发送消息时，客户端挂了，之后再恢复的话，这个消息就会彻底丢失。

   - 方案六：Redis 的 zset

     zset 是一个有序集合，每一个元素关联一个 score，通过 score 来对集合中的元素进行排序

     我们可以将（下单时间 + 超时时间） 与订单号分别设置为 score 和 元素值，通过 redis 进行排序之后，再开启 redis 扫描任务，获取 “当前时间 > score” 的任务，扫描到之后取出订单号，进行关单操作。

     - 优点：使用 redis zset 可以借助 redis 的持久化、高可用机制，避免数据丢失。在高并发场景中，可能多个消费者同时获取同一个订单号，一般采用分布式锁进行解决，也可以做幂等性（多个消费者获取同一个订单号也不影响）进行处理。

       ```bash
       # 命令示例
       # 添加两个元素 a、b 分数为 10、25
       127.0.0.1:6379> zadd delay_queue 10 a
       (integer) 1
       127.0.0.1:6379> zadd delay_queue 25 b
       (integer) 1
       # 查询分数为 9-12 的元素
       127.0.0.1:6379> zrangebyscore delay_queue 9 12 limit 0 1
       1) "a"
       ```

   - 方案七：Redission，Redission 中定义了分布式延迟队列 RDelayedQueue，即在 zset 基础上增加了一个基于内存的延迟队列，当我们添加一个元素到延迟队列时，redission 会把 数据+超时时间 放到 zset 中，并且启动一个延时任务，当任务到期时，再去 zset 中把数据取出来进行消费，允许以指定的延迟时长将元素放到目标队列中。

     - 优点：可以解决方案六中的并发问题，稳定性，性能较高

   - 方案八：RocketMQ时间轮（https://mp.weixin.qq.com/s/I91QRel-7CraP7zCRh0ISw）

   - **总体来讲，Redission + Redis、RabbitMQ插件、Redis的zset、RocketMQ延迟消息这几种方案比较推荐**






## 一个系统用户登陆信息保存在服务器A上，服务器B如何获取到Session信息？（分布式 Session 共享的解决方案）

答：将 Session 数据存储到分布式缓存比如 Redis 中，所有的服务器都可以访问。

- 优点：性能优秀、支持横向扩展（Redis集群）
- 缺点：存在数据丢失风险（虽然 Redis 支持数据持久化，但仍可能丢失小部分数据）






## 如果让你来评估项目的QPS的话，你会用什么方式来评估?(补充: 不要做压测，就通过现在的设计以及硬件配置推导OPS应该达到什么水准?)

答：首先需要根据业务提供的推广规模、渠道、人数，来评估。这里根据 28 原则进行评估，**即 80% 的请求访问在 20% 时间内到达**。

假如系统有1000万用户，那么每天来点击页面的占比20%，也就是200万用户访问。

假设平均每个用户点击50次，那么总用有1亿的PV（页面浏览量）

一天24个小时，平均活跃时间段算在5个小时内【24*20%】，那么5个小时预计有8000万点击，也就是平均每秒4500个请求。

4500是一个均值，按照电商类峰值的话，一般是3~4倍均值量，也就是5个小时每秒18000个请求【QPS=1.8万】







## 比如说: 16核64G的机器，普通机械硬盘，这种情况下让你来做秒杀的系统，你会去修改和配置哪些参数?(不考虑redis、kafka等，只考虑springboot的应用)

答：如果这么个机器，一般会拆分4核16G 的4台虚拟机，之后是 JVM、Tomcat 的参数配置。拆分为4台虚拟机之后，相当于是互备容灾。

不存在使用64G内存机器的配置，在云原生时代，大家更倾向于用小机器组成阵列，去扛流量。不够就伸缩，到阿里云去买公有云，这就要求你10分钟能上架应用。（微博的XX出轨应对策略，也是这个意思，加机器抗流量）

因此在答这道题时，需要首先关注的就是配置上的缺陷，之后再去解决JVM配置调优的问题。









## 接上面，SpringBoot和JVM需要配置的参数还有哪些?

答：主要集中在池化和组件的使用配置上，如；线程池、连接池、RPC重试和超时等







## 秒杀场景下用哪种垃圾回收器合适?

答：基本就是G1，但估计想让你解释下 G1 【分代收集、并发标记、区域回收、自适应调整】- 理由；在秒杀场景中，由于请求量大、并发高，需要尽量减少应用的停顿时间，以提高系统的响应速度和吞吐量。







## Full GC卡顿时间长短跟什么有关系? 

答：堆大小、垃圾回收器，此场景快速秒杀就结束了，预计也就在百十毫秒。如果更准确这个就太依赖于环境配置的验证了。





## 如果堆大小为128G的话，Full GC可能停顿多久?





## 微信二维码扫描原理：

答：**流程：**

![1696731916684](imgs/1696731916684.png)

总的来说，PC 端需要进行扫码登陆的原理是通过二维码绑定移动端的身份信息以及PC端的设备信息，根据这两个信息生成 token 给 PC 端，PC 端就登陆成功了。

二维码准备：

1. PC端向服务器发起请求，表示要生成用户二维码，并且把 PC 端设备信息也传递给服务端
2. 服务端收到请求后，生成唯一的二维码 ID，并将二维码 ID 与 PC 端设备信息进行绑定
3. 服务端将二维码 ID 返回给 PC 端
4. PC 端收到二维码 ID 后，生成二维码
5. PC 端为了及时知道二维码的状态（是否已经扫描，扫描后是否已经确认），会不断轮询服务端，请求服务端当前二维码的状态及相关信息

扫描状态切换：

1. 用户扫描二维码后，读取到二维码 ID
2. 向服务端发送请求，并携带移动端的身份信息与二维码 ID
3. 服务端接收之后将身份信息与二维码 ID 进行绑定，生成临时 token，返回给移动端
4. 在移动端扫描完之后，PC 端会轮询二维码状态，修改为已扫描，此时二维码 ID 会与账号信息进行绑定

第三步返回给移动端临时 token 是要保证移动端在下一步操作时，使用这个临时 tokne 作为凭证，保证两步操作是同一部设备发出的，临时 token 只可以使用一次就失效。

登陆确认：

1. 移动端接收到临时 token 后会弹出确认登陆界面，点击确认，移动端会携带临时 token 调用服务端接口
2. 服务端收到确认后，根据二维码 ID 绑定的设备信息与账号信息，生成 PC 端 token
3. PC 端轮询二维码状态，修改为已确认
4. 登陆成功








## 你知道哪些实现业务解耦的方法？

答：解耦是一种很重要的软件工程原则，它可以提高代码的质量和可复用性，降低系统的耦合度和维护成本。

解耦在日常开发中很常见，如 AOP 可以将需要切入的逻辑（日志、事务、权限）从核心业务中分离出来、IOC 可以将对象的创建和依赖管理交给容器。

可以通过 `事件驱动` 实现业务之间的解耦，通过事件驱动的实现方式常用的有两种：

1. 基于发布订阅模式的事件驱动

MQ 就是这样实现解耦，这种方式在解耦的同时，还实现了异步，提高了系统的吞吐量和接口响应速度。

成熟的消息队列的功能一般比较成熟，自带消息持久化、负载均衡、消息高可用。

除此之外 Redis 也有发布订阅功能（pub/sub），但是存在消息丢失、消息堆积等问题，不如专业的消息队列。

2. 基于观察者模式的事件驱动

常见的基于观察者模式的事件驱动框架有：Spring Event、Guava EventBus 等

Spring Event 和 Guava EventBus 默认是同步的，但也能实现异步，只是功能比较鸡肋。

观察者模式就只有观察者和被观察者，两者是直接进行交互的。

Spring Event 示例：

```java
// 事件发布者
@Component
public class CustomSpringEventPublisher {
    @Autowired
    private ApplicationEventPublisher applicationEventPublisher;

    public void publishCustomEvent(final String message) {
        System.out.println("Publishing custom event. ");
        CustomSpringEvent customSpringEvent = new CustomSpringEvent(this, message);
        applicationEventPublisher.publishEvent(customSpringEvent);
    }
}

// 事件监听者
@Component
public class CustomSpringEventListener implements ApplicationListener<CustomSpringEvent> {
    @Override
    public void onApplicationEvent(CustomSpringEvent event) {
        System.out.println("Received spring custom event - " + event.getMessage());
    }
}
```



发布订阅模式和观察者模式对比：

- 发布订阅模式：发布者和订阅者完全解耦，通过中间件进行消息传递；可以利用中间件（MQ、Redis）来实现分布式的消息传递，可应用于跨应用或跨进程的场景；大多数是异步的；
- 观察者模式：需要维护观察信息，被观察者和观察者直接交互；基于对象本身的数据变化来通信，不能使用在跨应用或跨进程的场景；大多数是同步的；






## 接口重试策略如何设计？

常见的重试策略有两种：

1. 固定间隔时间重试：实现简单、但是可能导致重试过于频繁或稀疏，从而影响系统性能。如果重试间隔太短，可能导致雪崩效应；如果太长，可能影响用户体验
2. 梯度间隔重试：根据重试次数去延长重试间隔时间。例如第一次重试间隔1s，第二次2s，第三次4s。能有效提高重试的几率，也能通过梯度增加间隔时间来避免对下游系统造成更大压力。此种策略需要设置合理的上下限值，否则可能导致延长时间过长。

重试策略对分布式系统来说是自私的，客户端认为他的消息很重要，并要求服务端花费更多资源来处理，盲目的重试设计不可取。



重试策略最佳实践：

- 合理设置消费的最大超时时间和次数（尽快向客户端返回成功或失败，不要以超时或者异常抛出来代替消费失败）
- 重试会导致相同的消息进行`重复消费`，消费方应该有一个良好的`幂等设计`



支付系统中补单操作如何完成：https://mp.weixin.qq.com/s/9Z-N3cfWu7oMVJsTDkbb-Q

简单来讲，补单利用 RocketMQ 对操作失败进行补偿操作，但不能一直进行补偿操作，需要设置一个最大重试次数，在多次补偿失败之后，需要延缓补偿频率，这些都通过 RocketMQ 进行实现，这里还存在几个问题：

1. 如果异常消息发送失败，上游没有重试机制，这笔订单就会卡住，因为系统并不知道需要去补偿
2. 在补偿消息时失败
3. 如果重试达到最大次数仍然没有成功，该如何处理？

针对问题1，可以将异常消息落库，存在异常消息表中，记录订单号、当前重试次数、一场分类、记录状态、消息体等字段，设置定时任务去扫描该表进行处理。对当前 MQ 的可用性，异常数据很少出现。

针对问题2，如果补偿失败，会向上抛出 error，利用 RocketMQ 的梯度重试机制，当消费次数上限后会进入死信队列。这种情况一般是网络出现问题，恢复之后，可以从死信队列拉取这些消息再统一处理。如果 MQ 和 DB 都失败了，为极端情况，人工介入即可。

针对问题3，如果达到最大次数仍然没有成功，将他放入异常表。

还可以有一些在业务低峰期的兜底任务，扫描业务表，对未完成的订单进行补偿。**兜底任务可能造成信息的短暂堆积，影响线上补偿流程推进，可以使用独立的队列隔离开。**









## 数据库里有3个字端Uid，type，score，其中type为1代表加分，-1代表扣分，给一堆数据记录，让计算出排名前10的uid。

答：


```mysql
select uid, sum(score*type) as total_score 
from t1 
group by uid 
order by total_score 
desc limit 10 
```







## 如何实现Score的排行榜

答：使用 Redis 的 zset，zset里面的元素是唯一的，有序的，按分数从小到大排序。

```bash
zadd ranking 10 a
zadd ranking 5 b
zadd ranking 15 c
zrevrange ranking 0 9 withscores # 获取排行榜前N名用户  zrevrange是将分数从大到小排序 zrange是将分数从小到大排序
```





## 场景题：实时排行榜，几千万的流量！要高可用高并发

答：假如我们要对前100名用户进行实时排行，在数据库中创建一张用户总分表。总分表里会存入用户头像，姓名，总分，用户id。将总分表的前500名放到 Redis 的 zset 集合中。

- 当用户访问排行榜接口时，会从 Redis 中获取前 100 名用户的信息。
- 当用户的分数发生变化时，会拿当前用户分数和第100名用户的分数对比，如果大于，则放入Redis中。



排行榜只有100名用户，我们将前500名用户都放入 Redis 有必要吗？有必要，数据冗余一些可以避免频繁的更新数据，也能保证数据的准确性（否则，就需要加全局锁保证数据的准确性）。



可以加一个定时器，隔一段时间从数据库重新取数据，避免时间长了，redis中存储的数据越来越多。

![1697594152787](imgs/1697594152787.png)

```java
Map<String, Double> map = new HashMap<>();
for (int i = 0; i < 300000; i++) {
    map.put("userId" + i, Double.valueOf(i));
}
// zadd 批量添加，或者单个添加，或者更新
jedis.zadd("ranking", map);
jedis.zadd("ranking", 10.00, "userA");
User user = new User();
// 单独往hash中添加数据
jedis.hset("user-list", "userA", JSON.toJSONString(user));
Map<String, String> map1 = new HashMap<>();
for (int i = 0; i < 10; i++) {
    map1.put("userId" + (char)('A' + i), JSON.toJSONString(user));
}
// 批量添加
jedis.hset("user-list", map1);
// 设置过期时间
jedis.expire("user-list", 5 * 60);
jedis.expire("ranking", 5 * 60);
// hash获取
jedis.hget("user-list", "userA");
// 查看某个用户排名，zset是按照分数从小到大排列，所以排行榜要使用zrevrank
jedis.zrevrank("ranking","userA")
// 查看前10名，并查出分数
jedis.zrevrangeWithScores("ranking",0,9)
```







## 幂等性如何设计？

答：幂等性的设计有以下几种方案：

**方案一：唯一索引或唯一组合索引**

对订单的幂等性设计，可以使用订单号作为唯一索引，这样如果多次插入的话，就会报错 ` DuplicatedKeyException`， 那么我们就可以捕获该错误，来返回友好提示。



**方案二：乐观锁**

使用乐观锁会给数据库表增加一个`版本号 version`字段，查询数据时，读取到 version，当更新数据时判断数据库版本号和自己拿到的版本号是否相同，相同则更改，每次更新操作对 version 字段加 1。

`update order set name = #{name}, version=#{version}+1 where id=#{id} and version=#{version}`



**方案三：Token + Redis**

针对调用方重试接口的情况，例如重复提交订单，这种幂等性设计可以使用 Token 机制来防止重复提交。

调用方在调用接口时，先向后端请求一个 Token，该 Token 存储在 Redis 中并设置过期时间，在调用时携带上 Token（放入Header存储），后端在 Redis 中检查该 Token 是否存在，如果存在表示是第一次请求，删除token中的缓存**（使用 lua 脚本，保证操作的原子性）**，如果不存在，表示重复请求，直接返回。

如果第一次调用接口失败了，可以通过设计来重新生成 token，再次尝试调用。

```java
public void invoke(){
  String token = genToken();
  // 提交订单信息
  submitOrder(token, order);
}
```











## 定时任务调度的常见实现方案Quartz和Spring的@Schedule和xxl-job(面试的时候一直叫它xxx-job)的区别









## 如果大量请求进来你怎么限流？



**单机环境下：**

单机模式下，Google 开发了 Guava包，其中提供了限流操作，可以使用  `RateLimiter + AOP` 来进行限流操作。

1. RateLimiter 是一个频率限制器，通过配置频率来发放许可，如果 1 秒内可以访问十次，那么这十次许可发送的间隔是完全相同的
2. 并发使用是安全的
3. RateLimiter 还可以去配置先处于一个预热器，每秒增加发放的许可直到达到稳定的频率
4. RateLimiter 不影响请求本身的节流，而是影响下一次请求的节流，比如当前任务如果占用许可较多，到达 RateLimiter 之后，会立即占用，当下一个请求到达 RateLimiter 时就会经历节流，因为上一个请求已经占用大量的许可。

一个小示例用法，如果想要发送一组数据，我们限制他在 5kb 每秒：

```java
// 给每一个字节发放 1 个许可，限制在 5kb 每秒的话，只需要每秒发放 5000 个许可即可
final RateLimiter rateLimiter = RateLimiter.create(5000.0);
void submitPacket(byte[] packet) {
   rateLimiter.acquire(packet.length);
   networkService.send(packet);
}
```

**分布式环境下：**

分布式环境下，就不能向单节点模式那样对单个节点限流，这样 n 个节点时，总流量就是单节点的 n 倍。

在分布式环境中，所有流量都会先打到网关层，那么就可以对网关进行限流：

- Nginx 限流：思想是漏桶算法（将所有请求缓存到一个队列中，以固定速度处理，如果队列满了，就只能丢弃新进入的请i去），即能够强行保证请求实时处理的速度不会超过设置的阈值
- mq 限流
- redis+lua限流：lua脚本保证redis操作的原子性，使用redis中的数据结构进行限流

https://blog.csdn.net/zhouhengzhe/article/details/122406253





## Redis 的 bitmap 实现签到系统？

答：

参考文章：https://juejin.cn/post/6881928046031568903?searchId=2023102521154891E6E96E046E0EEBC31D

**先来看一下如何使用 redis bitmap 的原生命令实现签到功能：**

- 签到

我们先来设计 key：`userid:yyyyMM`，那么假如 usera 在2023年10月3日和2023年10月4日签到的话，使用以下命令：

`setbit key offset value`：

在3日签到的话，偏移量应该设置为2，则签到之后为 001

在4日签到的话，偏移量应该设置为3，则3日、4日签到后为`0011`（第3位和第4位都是1，表示这两天签到了）

在31日签到的话，偏移量应该设置为30，表示向右偏移30位

```bash
127.0.0.1:6379> setbit userx:202310 2 1
(integer) 0
127.0.0.1:6379> setbit userx:202310 3 1
(integer) 0
127.0.0.1:6379> setbit userx:202310 30 1
(integer) 0
```

- 查看2023年10月哪些天签到了，10月有31天，所以使用`u31`（31位无符号整数），后边偏移量为`0` 

```bash
127.0.0.1:6379> bitfield userx:202310 get u31 0
1) (integer) 402653185
```

通过 `bitfield [key] get u31 0` 获取了31位的无符号整数，将该整数`402653185`转为二进制如下：（可以发现从左向右第4位和第5位位1，表示这两天签到了，也就是3号签到的时候，在`setbit key value offset`中，偏移量设置了为3，所以向右偏移3为，在第4位上）

```
402653185 十进制
0011000000000000000000000000001 二进制
```

通过取出来这个无符号整数，我们在代码中就可以通过位运算来判断某一天是否签到，可以看出，第3、4、31位都是1，表明在这三天都进行了签到



那么如果今天是10月26日，我们想知道10月1日-10月26日有哪些天进行签到，使用如下命令：

```bash
127.0.0.1:6379> bitfield userx:202310 get u26 0
1) (integer) 12582912

```

将 `12582912` 转为二进制为：

```
26位无符号整数：
00110000000000000000000000
```



那么我们在计算连续签到的次数就可以使用以下方法：

```java
public Integer getContinuousSignCount() {
  /*
  假如今天是 10月26日
  假如通过 redis 的 bitfield userx:202310 get u26 0 命令得到了从1日-26日的签到数据为：12582912
  */
  Long v = 12582912;
  // 这里 26 为今天的日期
  Integer signCount = 0;
  for (int i = 0; i < 26； i ++) {
    if (v & 1 == 0) return signCount;
    signCount ++;
    v >>= 1;
  }
}
```



**数据库表设计：**来自https://juejin.cn/post/6881928046031568903?searchId=2023102521154891E6E96E046E0EEBC31D

```sql
CREATE TABLE `t_user_integral` (
  `id` varchar(50) NOT NULL COMMENT 'id',
  `user_id` int(11) NOT NULL COMMENT '用户id',
  `integral` int(16) DEFAULT '0' COMMENT '当前积分',
  `integral_total` int(16) DEFAULT '0' COMMENT '累计积分',
  `create_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '创建时间',
  `update_time` datetime DEFAULT NULL ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT='用户积分总表'

CREATE TABLE `t_user_integral_log` (
  `id` varchar(50) NOT NULL COMMENT 'id',
  `user_id` int(11) NOT NULL COMMENT '用户id',
  `integral_type` int(3) DEFAULT NULL COMMENT '积分类型 1.签到 2.连续签到 3.福利任务 4.每日任务 5.补签',
  `integral` int(16) DEFAULT '0' COMMENT '积分',
  `bak` varchar(100) DEFAULT NULL COMMENT '积分补充文案',
  `operation_time` date DEFAULT NULL COMMENT '操作时间(签到和补签的具体日期)',
  `create_time` datetime DEFAULT NULL COMMENT '创建时间',
  PRIMARY KEY (`id`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 ROW_FORMAT=COMPACT COMMENT='用户积分流水表'
```









**扩展：常见限流算法**

- 计数器限流算法：在有效时间内计算请求次数，调用一次+1，调用结束-1。可以使用Redis的incr或其他计数工具实现。
- 滑动窗口限流算法：每过一个步长，整体时间区域滑动一下，以滑动窗口的机制减少临界值带来的超过阈值的问题。
- 漏桶限流算法：恒定速率的限流算法，不论客户端请求量是多少，服务端处理请求的速度都是恒定的。
- 令牌桶限流算法：有一个令牌桶和定时器，在一个时间段内往令牌桶里生成固定数量的令牌，请求就从桶里拿一个令牌，如果令牌没了，就排队或拒绝服务。



#### 待完善





> 写 MQ 时程序宕机了怎么办？







> 26. 服务端出现大量 close_wait 状态，可能的情况？





> 27. Java 程序运行了一周，发现老年代内存溢出，分析一下？







> 28. 4G 的文件，里面是 8 位的手机号码，内存是 200M，怎么实现去重？







> 29. 100g的文件，每行一个url，机器4g的内存，统计出top100的url并输出为一个新的文件。









> 30. ​









# 线上常见问题

https://wx.zsxq.com/dweb2/index/topic_detail/185425252544152

https://articles.zsxq.com/id_izrmpuk43owr.html



> 1. 你在做这个项目的时候遇到了什么问题？（OOM问题、GC问题等等）





> 2. 你用过哪些分析定位 Java 故障/性能的工具？（JDK 自带工具、MAT、Arthas 等等）







> 3. 如果项目遇到了 OOM 问题，你会如何排查？（常用 MAT）





> 4. 有什么办法可以监控到 JVM 的实时运行状态？（Arthas）



> 5. 生产环境有一个接口很慢，如何排查？（Arthas）



> 6. 你是如何定位线上问题的？（说说自己了解的工具，然后根据面试官提示继续深入聊即可）





> 7. 有一个程序占用大量cpu，并且一直运行，怎么排查？







> 8. Redis延迟问题全面排查



https://mp.weixin.qq.com/s/mIc6a9mfEGdaNDD3MmfFsg



> 1. 线上问题：

   环境 suse + redis4 （3台虚拟机，3主3从）

   分析日志： redis 访问超时偶然发生，大部分超时在应用重试后可正常访问。

   排查结果： 容器与 redis 连接正常、网络设备负载正常、cpu 使用率正常、redis 的 qps 正常、redis 无慢查询

   运维给出结论：redis 连接数过多，性能达到瓶颈

   生产处置：

- redis 服务重启，为缓解
- 收集系统日志，低峰期重启 redis 所在虚拟机，现象缓解

   后续排查结果：

   redis 配置文件中 `tcp-backlog` 参数可能会导致该现象。每个 TCP 请求都要经过半连接队列 -> 已完成队列 -> redis 主线程 的排队执行过程，发生问题的机器 `tcp-backlog` 使用默认值时为 511，由于 `redis` 为单线程，出现连接队列满的情况时，后续 tcp 请求将会被丢弃，客户端发生超时。并且 `tcp-backlog` 对应系统层 `net.coire.somaxconn`，该参数虚拟机设置为 2048，将 `tcp-backlog` 改为 2048，该现象消失。

   最终结论：tcp 请求半连接队列打满，导致后续 tcp 请求被丢弃，发生超时

   后续处理：

- 部署虚拟机监控，监控 overflowed 增长情况
- redis 设施升级，改为7主7从，redis 升级为 6







# 项目难点

> 1. 登陆日志原本是同步写入库的，后来为了提升效率，加入了队列，先写队列后消费入库，做解耦，但是队列用了同步操作，有一个 mq 挂了，导致登陆服务不可用，这种事不可以接受的，记录日志不能影响登录，所以将其改为了异步方式








> 2. 讲讲项目难点以及怎么解决的





> 3. 你的rpc轮子怎么划分模块的





> 4. （手写 rpc 项目中）你自定义的序列化方式偏向于Json还是Protobuf？这俩序列化有啥区别？和你的自定义序列化性能差异对比？怎么解决分包粘包？怎么注册注销？讲一下你的SPI实现依赖倒置？动态桩讲一下？可以客户端指定调用ip吗





> 5. 如何跟面试官聊项目？



这个问题挺有意思的，聊项目主打一个交心。
简单说下我的看法：
（1）首先简历上的项目需要比较丰富，这里的丰富不是对项目的功能的堆砌，而是要从以下几个方面进行阐述：
   1.项目背景
   2.项目规模
   3.你负责的核心模块、开发的功能
   4.项目痛点、难点
   5.你做了什么事情，用到了那些技术，解决了什么难题，达成了什么效果，带来了什么效益，还有哪些可以改进的地方
（2）然后你得把你学过的一些技术带入进来，尽量让面试官对你运用的技术感兴趣，然后如何用这些技术解决问题的。
（3）最后你可以说下如何改进方案，引导面试官提供一些其他的建议。

相信经过这么几个来回，面试官会对你“很有意思的”。



# 计算机基础

## TCP 三次握手/四次挥手

| 参数      | 用途                                    |
| ------- | ------------------------------------- |
| SYN     | 用于启动和建立连接时，同步设备之间的序列号。0到2^32 - 1的随机数。 |
| ACK     | 向另一端确认已经收到 SYN，数值为收到 SYN 增一。          |
| SYN-ACK | 确认之前收到了 SYN，数值为自定义值。                  |
| FIN     | 终止连接。                                 |
| RST     | 重置连接。                                 |

### 三次握手

![1699278670892](imgs/1699278670892.png)

三次握手流程为：

1. 第一次握手：client 请求建立连接，发送 SYN 包到主机 B，并进入 SYN_SEND 状态，等待 server  回应
2. 第二次握手：server 向 client 发送 SYN+ACK 包，表示也想和 client 建立连接并确认 client 的报文，并进入 SYN_RECV 状态
3. 第三次握手：client 收到 server 发送的 SYN+ACK 包，向 server 发送 ACK 包

**为什么需要三次握手？**

有很多种说法，这里列出以下两种说法（自我感觉第一种说法比较浅显易懂）：

- 因为经过了三次握手，主机 A 和主机 B 才可以保证自己的发送能力和接收能力都是正常的，那么理论上双方`收发能力`正常，证明网络可靠，可以进行通信。
- 在谢希仁版《计算机网络》中是这样说的，如果 client 发送的第一个 SYN 包并没有丢失，只是在网络中滞留，以致于延误到连接释放以后的某个时间才到达 server。本来这是一个早已失效的报文，但 server 收到此失效报文后，就误认为是 client 再次发出的一个新的连接，于是向 client 发出 SYN+ACK 包，`如果不采用三次握手`，只要 server 发出 SYN+ACK 包，就建立连接，会导致 client 没有发出建立连接的请求，因此不会理会 server 的 SYN+ACK 包，但是 server 却以为新的连接建立好了，并一直等待 client 发送数据，导致资源被浪费



### 四次挥手

![1699277579371](imgs/1699277579371.png)

四次挥手流程为：

1. 第一次挥手：客户端想要终止连接时，向服务端发送 FIN，客户端进入 `FIN_WAIT_1` 状态，表示客户端没有数据要发送给服务端了，但是如果服务端还有数据没有发送完，服务端可以继续发送数据
2. 第二次挥手：服务端收到客户端终止请求后，回复 ACK 确认 FIN，表示已经收到客户端的中断请求，但是服务端还有数据需要发，此时客户端进入 `FIN_WAIT_2` 状态，继续等待服务端的 FIN（中断连接） 报文
3. 第三次挥手：TCP 的连接是相互独立的，当服务端已经发送完数据了，向客户端发送 FIN=1 报文，告诉客户端数据已经发送完毕，准备好关闭连接了，服务端此时进入 `LAST_ACK` 状态
   - 如果服务端一直没有收到客户端返回的 ACK 报文，那么就会触发超时重传，服务端会重传 FIN 报文
4. 第四次挥手：客户端收到服务端的 FIN 后，知道可以关闭连接了，因此向服务端发送 ACK 报文，并进入 TIME_WAIT 状态（如果客户端不向服务端发送 ACK ，那么服务端并不请求客户端是否收到了服务端关闭连接的请求，假如服务端发送给客户端的 FIN 报文丢失，那么客户端就一直以为服务端没有请求关闭连接，导致连接无法关闭）
   - 如果客户端在 2msl 内，再次收到了来自服务端的 FIN 报文，说明服务器端没有收到客户端的 ACK 报文，客户端重新向服务端发送 ACK 报文，计时器重置
   - 如果客户端再 2msl 内没有收到服务端的 FIN 报文，说明服务端正常接收了 ACK 确认报文，客户端可以进入 CLOSED 状态



**为什么客户端的 TIME_WAIT 时间是 2msl 呢？**

首先 msl 是报文的最大生存时间，如果超过 msl，那么报文就会被丢弃

那么客户端等待 2msl 是因为，客户端的 ACK 到达服务端最多花费 1msl，而服务端重发 FIN 报文也最多花费 1msl，因此如果客户端的 ACK 报文丢失，最多 2msl 的时间就可以收到服务端重发的 FIN 报文

**为什么要第四次挥手呢？**
其实很容易，我们想一下如果没有通过第四次挥手对 server 发给 client 的 FIN 报文进行确认，如果这个报文丢失的话，客户端就不知道需要关闭连接了，服务端也不知道报文丢失，也不重发，导致客户端一直认为连接处于打开状态

> 1. 堆和树的区别？应用场景？二叉搜索树是什么？


> 2. 进程间的通信方式？死锁条件？怎么解决？


> 3. 操作系统内存满了怎么办？如何回收？有什么影响


> 4. 什么是僵尸进程？应该怎么去操作？


> 5. 为什么会有线程安全的问题，如何解决？


> 6. 乐观锁和悲观锁？CAS？aba问题是什么，如何解决？


> 7. http 常见的方法和状态码有哪些？502是什么错误？如何排查问题？讲一下反向代理？


> 8. token 和 cookie 的区别？

> 9. https 怎么存放密钥？



> 10. cpu 爆了怎么排查，怎么优化，具体怎么写命令？





> 11. 共享内存，如何控制读取写入的一致性（信号量）





> 12. 进程可以忽视信号吗（可以，不过看级别）





> 13. HTTP协议的组成？报文格式？头和body如何分割？cookie在哪一部分？如何让请求携带cookie？



> 14. 计算机网络分层模型





> 15. TCP的可靠性怎么保障的，怎么保证接收到的内容是正确的





> 16. HTTPS校验的哈希算法是什么



> 17. HTTP 2.0和HTTP 1的区别？





> 18. 什么是队头阻塞？2.0为什么不用UDP？





> 19. HTTPS与HTTP区别



> 20. HTTPS TLS层四次握手





> 21. CA证书的认证





> 22. 建立 TCP 连接后，客户端下线了会发生什么







> 23. 介绍一下知道哪些 IO 模型，IO 多路复用了解吗







> 24. 对网络方面有了解吗？讲解一下TCP/IP协议的网络分层；







> 25. 讲讲ARP，ICMP







> 26. 什么时候不用查ARP表





> 27. ICMP是哪个路由器回的，什么地方用了icmp，traceroute怎么做的。















# Java 基础



## 创建对象构造方法执行顺序

构造方法、构造代码块、静态代码块加载顺序，以及子类继承父类加载顺序

```java
public class A {
    public A() {
        System.out.println("A构造方法");
    }
    {
        System.out.println("A构造代码块");
    }
    static {
        System.out.println("A静态代码块");
    }
}

public class B extends A{
    public B() {
        System.out.println("B构造方法");
    }
    {
        System.out.println("B构造代码块");
    }
    static {
        System.out.println("B静态代码块");
    }
    public static void main(String[] args) {
        new B();
    }
    /**
     * A静态代码块
     * B静态代码块
     * A构造代码块
     * A构造方法
     * B构造代码块
     * B构造方法
     */
}
```





## 了解泛型吗？

参考文章：https://blog.csdn.net/qq_43546676/article/details/128790980

泛型就是在编译时检查类型安全，并且不需要强制进行类型转换

**泛型擦除了解吗？**

泛型擦除即在编译生成的字节码中，所有声明泛型的地方都会被擦除，擦除之后设置的类型会根据是否指定泛型上界而不同：

- 如果没有指定泛型上界，则所有的泛型类型在编译之后都替换为 Object 类型

  即在 `generic.set("张三")` 时，会将 String 类型的参数擦除为 Object 类型

  通过反编译指令 `javap -c` 得到字节码，发现在 11 行 set 值类型为 Object，在 15 行 get 值类型为 Object，在 18 行编译器会插入 `checkcast` 语句将 Object 类型转为 String 类型

  ```java
  public class GenericTest<T> {
      private T t;
      public T get(){
          return t;
      }
      public void set(T t) {
          this.t = t;
      }
      public static void main(String[] args) {
          GenericTest<String> generic = new GenericTest<>();
          generic.set("张三");
          generic.get();
      }
  }
  // 通过 javap -c 反编译得到字节码指令
    public static void main(java.lang.String[]);
      Code:
         0: new           #3                  // class com/example/nettystudy/AlgorithmTest/GenericTest
         3: dup
         4: invokespecial #4                  // Method "<init>":()V
         7: astore_1
         8: aload_1
         9: ldc           #5                  // String 张三
        11: invokevirtual #6                  // Method set:(Ljava/lang/Object;)V
        14: aload_1
        15: invokevirtual #7                  // Method get:()Ljava/lang/Object;
        18: checkcast     #8                  // class java/lang/String
        21: astore_2
        22: return

  ```

  ​

- 如果指定泛型上界，则所有的泛型类型在编译之后都替换为 String 类型（也就是上界的类型）

  可以发现在字节码第 11 行和第 15 行即 set 和 get 时，类型都为 String 类型，而不是 Object 类型

  ```java
  public class GenericTest<T extends String> {
      private T t;
      public T get(){
          return t;
      }
      public void set(T t) {
          this.t = t;
      }
      public static void main(String[] args) {
          GenericTest<String> generic = new GenericTest<>();
          generic.set("张三");
          String s = generic.get();
      }
  }
  // 通过 javap -c 反编译得到字节码
  public static void main(java.lang.String[]);
    Code:
       0: new           #3                  // class com/example/nettystudy/AlgorithmTest/GenericTest
       3: dup
       4: invokespecial #4                  // Method "<init>":()V
       7: astore_1
       8: aload_1
       9: ldc           #5                  // String 张三
      11: invokevirtual #6                  // Method set:(Ljava/lang/String;)V
      14: aload_1
      15: invokevirtual #7                  // Method get:()Ljava/lang/String;
      18: astore_2
      19: return
  ```

  ​



## JDK 动态代理和 CGLIB 动态代理对比

https://www.bilibili.com/video/BV1tY411Z799?p=5&vd_source=168e9ae5ac7571bee732d704f9e4afbd

1. JDK 动态代理只能代理实现了接口的类，而 CGLIB 可以代理未实现任何接口的类。另外CGLIB 动态代理是通过生成一个被代理类的子类来拦截被代理类的方法调用，因此不能代理声明为final 类型的类和方法
2. 就二者的效率来说，大部分情况都是JDK 动态代理更优秀，随着 JDK 版本的升级，这个优势更加明显。
3. JDK 动态代理利用了拦截器、反射机制生成一个代理接口的匿名类，在调用具体方法前调用 InvokeHandler 来处理；CGLIB 动态代理利用了 ASM 框架，将代理对象类的 class 文件加载进来，通过修改其字节码生成子类来处理



**JDK动态代理底层原理：**

假如目前有一个接口 `HelloService（包含一个 say() 方法，需要被增强）`、实现类`HelloServiceImpl`、增强类`MyInvocationHandler`

在 JDK 动态代理中，生成的代理类 `$Proxy1` 是继承 Proxy 并且实现 `HelloService` 接口，当调用代理类的方法时，会进入到拦截器 `MyInvocationHandler` 的 invoke 方法中，下边为代理类生成代码：

```java
// 生成代理对象
HelloService helloService = (HelloService) Proxy.newProxyInstance(MyInvocationHandler.class.getClassLoader(), new Class[]{HelloService.class}, new MyInvocationHandler());
helloService.say();
```

通过上述代码拿到的 helloService 对象其实就是 JDK 动态代理对象，我们可以通过添加 VM options 来将动态代理对象保存下来，添加 VM options 如下：

`-Dsun.misc.ProxyGenerator.saveGeneratedFiles=true`

之后生成的动态代理对象如下（这里为了更直观的看代理类，因此只保留了最关键的代码），say() 其实就是定义在 HelloService 中需要被增强的方法，那么当调用 `helloService.say()` 时，其实就是调用 `$Proxy1.say()` 方法，在该方法中会调用 `h.invoke()` 方法，这里的 h 就是我们自己定义的 `MyInvocationHandler` 拦截器，之后就会进入到拦截器的 `invoke` 方法，

```java
import com.example.nettystudy.JdkProxyTest.HelloService;
import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;
import java.lang.reflect.Proxy;
import java.lang.reflect.UndeclaredThrowableException;

public final class $Proxy1 extends Proxy implements HelloService {
    private static Method m1;
    private static Method m2;
    private static Method m3;
    private static Method m0;

    ...
    
    public final void say() throws  {
        try {
            super.h.invoke(this, m3, (Object[])null);
        } catch (RuntimeException | Error var2) {
            throw var2;
        } catch (Throwable var3) {
            throw new UndeclaredThrowableException(var3);
        }
    }

    ...
}

```



下边来看一下拦截器的 invoke 方法，该方法有 3 个参数，第一个参数 proxy 也就是上边的代理类对象， method 就是接口中的 say 方法，那么在拦截器中就会执行我们自己添加的增强操作了

```java
public class MyInvocationHandler implements InvocationHandler {

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("方法执行前");
        // 这里 HelloServiceImpl 是被代理对象，被代理对象执行方法
        Object result = method.invoke(new HelloServiceImpl(), args);
        System.out.println("方法执行后");
        return result;
    }
}

```









> 1. hashcode 和 equals 区别？只重写 equals 行不行？
> 2. Collection 和 List 详细讲一下？arraylist和linkedkist ？ArrayList扩容？
> 3. hash map 和 hash table 的区别？hashmap 操作的时间复杂度？HashMap底层数据结构，扩容（可以从哈希函数说起，扩容不要忘记考虑负载因子）？HashMap为什么总是保证数组个数为2的幂次方（我觉得有两个角度：取余用&代替，扩容方便）









> 4. 序列化，String 和枚举类有什么区别，如果序列值一样会有什么问题？





> 5. 排序的稳定性，解释一下？ 



> 6. 为什么 ConcurrentHashMap 的 key 和 value 不支持 Null 值?

key 和 value 不能为 null 主要是为了避免二义性。null 是一个特殊的值，表示没有对象或没有引用。如果你用null作为键，那么你就无法区分这个键是否存在于ConcurrentHashMap中，还是根本没有这个键。同样，如果你用null作为值，那么你就无法区分这个值是否是真正存储在ConcurrentHashMap中的，还是因为找不到对应的键而返回的。



多线程环境下，存在一个线程操作该ConcurrentHashMap时，其他的线程将该 ConcurrentHashMap 修改的情况，所以无法通过 containsKey(key) 来判断否存在这个键值对，也就没办法解决二义性问题了。



于此相比，HashMap 可以存储 null 的 key 和 value，但是 null 作为键只有一个，作为值可以有多个。如果传入null作为参数，就会返回hash值为0的位置的值。单线程环境下，不存在一个线程操作该HashMap时，其他的线程将该HashMap修改的情况，所以可以通过contains(key)来做判断是否存在这个键值对，从而做相应的处理，也就不存在二义性问题。



**那么为什么 ConcurrentHashMap 源码不设计成可以判断是否存在 null 值的 key？**

如果 key 为 null，那么就会带来很多不必要的麻烦和开销。比如，你需要用额外的数据结构或者标志位来记录哪些key是null的，而且在多线程环境下，还要保证对这些额外的数据结构或者标志位的操作也是线程安全的。而且，key为null的意义也不大，因为它并不能表示任何有用的信息。





**执行containsKey()后，在调用get()方法之前可能会被其他线程修改或者删除，这算是不可重复读，那这算是线程不安全吗？**

ConcurrentHashMap 是线程安全的，但它不能保证所有的复合操作都是原子性的。如果需要保证复合操作的原子性，就要使用额外的同步或协调机制。这并不违反线程安全的定义，而是属于不同层次的一致性要求。

containsKey() 和 get() 方法都是单独的操作，它们之间没有同步保证。因此，如果在调用 containsKey() 后，另一个线程修改或删除了相应的键值对，那么 get() 方法可能会返回 null 或者过期的值。这确实是不可重复读的情况，但这并不违反线程安全的定义。







> 7. Map这种结构在java里有有哪些实现的对象呢？





> 8. Java多态的底层实现





> 9. 讲讲jdk1.8的垃圾回收



> 10. HashMap树化时除了remove还有什么时候链表化





> 11. 双亲委派的实现原理





> 12. 说一下 hashmap 的 put 过程





> 13. 作为 map 的 key 需要重写哪些方法？





> 14. JVM，JDK，JRE三者的之间的联系？







> 15. 方法重载和方法重写区别？





> 16. 接口和抽象类之间的区别？







> 17. 创建对象的几种方式







> 18. 这三行代码jvm做了什么事情
>
> String a = "123";
>
> String b = new("456");
>
> String c = a + b;





> 19. hashmap，为什么要转成红黑树，不是一开始就用（红黑树的缺点）









> 20. 双向链表的缺点？







# 设计模式

> 1. 你对设计模式的了解？最近用了什么 





> 2. 为什么用代理模式，用继承不行吗 









# MySQL



## 为什么 mysql 删了行记录，反而磁盘空间没有减少？

答：

在 mysql 中，当使用 delete 删除数据时，mysql 会将删除的数据标记为已删除，但是并不去磁盘上真正进行删除，而是在需要使用这片存储空间时，再将其从磁盘上清理掉，这是 MySQL 使用`延迟清理`的方式。 



**延迟清理的优点：**

- 如果 mysql 立即删除数据，会导致磁盘上产生大量的碎片，使用`延迟清理`可以减少磁盘碎片，提高磁盘的读写效率
- 如果删除数据时立即清理磁盘上的数据，会消耗大量的性能。（如果一个大表存在索引，只删除其中一行，整个索引结构就会发生变化）

**延迟清理的缺点：**

- 这些被标记为删除的数据，就是数据空洞，不仅浪费空间，还影响查询效率。

  mysql 是以数据页为单位来存储和读取数据，如果一个表有大量的数据空洞，那么 mysql 读取一个数据页，可能被标记删除的数据就占据了大量的空间，导致需要读取很多个数据页，影响查询效率



**如何回收未使用空间：**

`optimize table 表名`



## 索引的结构？

答：

索引是存储在引擎层而不是服务层，所以不同存储引擎的索引的工作方式也不同，我们只需要重点关注 InnoDB 存储引擎和 InnoDB 存储引擎中的索引实现，以下如果没有特殊说明，则都为 InnoDB 引擎。



mysql 支持两种索引结构： `B-tree` 和 `HASH` 

- B-tree 索引

B-tree 索引结构使用 B+ 树来进行实现，结构如下图（粉色区域存放索引数据，白色区域存放下一级磁盘文件地址）：

![1698208834191](imgs/1698208834191.png)

B-tree 索引（B+ 树实现）的一些特点：

- B+ 树叶子节点之间按索引数据的大小顺序建立了双向链表指针，适合按照范围查找
- 使用 B+ 树非叶子节点只存储索引，在 B 树中，每个节点的索引和数据都在一起，因此使用 B+ 树时，通过一次磁盘 IO 拿到相同大小的存储页，B+ 树可以比 B 树拿到的索引更多，因此减少了磁盘 IO 的次数。
- B+ 树查询性能更稳定，因为数据只保存在叶子节点，每次查询数据，磁盘 IO 的次数是稳定的






## 为什么索引能提高查询速度？

答：

索引可以让服务器快速定位到表的指定位置，索引有以下几个优点：

- 索引大大减少了服务器需要扫描的数据量
- 索引可以帮助服务器避免排序和临时表
- 索引可以将随机 IO 变为顺序 IO


## 前缀索引和索引的选择性？

答：

**索引的选择性：**指的是不重复的索引值与数据表的记录总数的比值。

索引的选择性越高，查询效率也越高，因为选择性高的索引可以让 mysql 在查找时过滤掉更多的行。`唯一索引`的选择性是1，这也是最好的索引选择性，性能也是最好的



**前缀索引：**

有时候为了提高索引的性能，并且节省索引的空间，只对字段的前一部分字符进行索引，但是存在的缺点就是：`降低了索引的选择性`







**如何选择前缀索引的长度呢？**

前缀索引的长度选择我们要兼顾索引的选择性和存储索引的空间两个方面，因此既不能太长也不能太短，可以通过计算`不同前缀索引长度的选择性`，找到最接近`完整列的选择性`的前缀长度，通过以下 sql 进行计算`不同前缀索引长度的选择性`：

```sql
select 
count(distinct left(title, 6)) / count(*) as sel6,
count(distinct left(title, 7)) / count(*) as sel7,
count(distinct left(title, 8)) / count(*) as sel8,
count(distinct left(title, 9)) / count(*) as sel9,
count(distinct left(title, 10)) / count(*) as sel10,
count(distinct left(title, 11)) / count(*) as sel11,
count(distinct left(title, 12)) / count(*) as sel12,
count(distinct left(title, 13)) / count(*) as sel13,
count(distinct left(title, 14)) / count(*) as sel14,
count(distinct left(title, 15)) / count(*) as sel15,
count(distinct left(title, 16)) / count(*) as sel16,
count(distinct left(title, 17)) / count(*) as sel17,
count(distinct left(title, 18)) / count(*) as sel18,
count(distinct left(title, 19)) / count(*) as sel19,
count(distinct left(title, 20)) / count(*) as sel20,
count(distinct left(title, 21)) / count(*) as sel21
from interview_experience_article 
```

计算结果如下：

![1698218076575](imgs/1698218076575.png)

再计算`完整列的选择性`：

```sql
select count(distinct title)/count(*)  from interview_experience_article 
```

计算结果如下：

![1698218008368](imgs/1698218008368.png)



完整列的选择性是 0.6627，而前缀索引在长度为 16 的时候选择性为（`sel16=0.6592`），就已经很接近完整列的选择性了，此使再增加前缀索引的长度，选择性的提升幅度就已经很小了，因此在本例中，可以选择前缀索引长度为 16



**本例中的数据是随便找的一些文本数据，类型是 text**



**如何创建前缀索引：**

```sql
alter table table_name add key(title(16))
```



## 如何选择合适的索引顺序？

答：

来源于《高性能MySQL》（第4版）

对于选择合适的索引顺序来说，有一条重要的`经验法则`：**将选择性最高的列放到索引的最前列**

在通常境况下，这条法则会有所帮助，但是存在一些特殊情况：

对于下面这个查询语句来说：

```sql
select count(distinct threadId) as count_value
from message
where (groupId = 10137) and (userId = 1288826) and (anonymous = 0)
order by priority desc, modifiedDate desc
```

explain 的结果如下（只列出使用了哪个索引）：

```yaml
id: 1
key: ix_groupId_userId
```

可以看出选择了索引（groupId, userId），看起来比较合理，但是我们还没有考虑（groupId、userId）所匹配到的数据的行数：

```sql
select count(*), sum(groupId=10137), sum(userId=1288826), sum(anonymous=0)
from message
```

结果如下：

```sql
count(*): 4142217
sum(groupId=10137): 4092654
sum(userId=1288826): 1288496
sum(anonymous=0): 4141934
```

可以发现通过 groupId 和 userId 基本上没有筛选出来多少条数据

 **因此上边说的经验法则一般情况下都适用，但是在特殊形况下，可能会摧毁整个应用的性能**



上边这种情况的出现是因为这些数据是从其他应用迁移过来的，迁移的时候把所有的消息都赋予了管理组的用户，因此导致这样查询出来的数据量非常大，这个案例的解决情况是修改应用程序的代码：**区分这类特殊用户和组，禁止针对这类用户和组执行这个查询**





## 聚簇索引和非聚簇索引的区别？非聚集索引一定回表查询吗？

答：

聚簇索引并不是一种单独的索引类型，而是一种数据存储方式。

当表里有聚簇索引时，它的数据行实际上存放在索引的叶子节点中。

`聚簇`表示数据行和相邻和键值存储在一起

InnoDB 根据主键来聚簇数据，如果没有定义主键的话，InnoDB 会隐式定义一个主键来作为聚簇索引，

**聚簇索引的优点：**

- 数据访问更快。聚簇索引将数据和索引保存在同一个 B-tree 中，获取数据比非聚簇索引更快
- 使用覆盖索引扫描的查询可以直接使用叶节点的主键值



**聚簇索引的缺点：**

- 提升了 IO 密集型应用的性能。（如果数据全部放在内存中的话，不需要执行 IO 操作，聚集索引就没有什么优势了）

- 插入速度严重依赖于插入顺序。按照主键的顺序插入行是将数据加载到 InnoDB 表中最快的方式。

  如果不是按照逐渐顺序加载数据，在加载完之后最好使用 `optimize table` 重新组织一下表，该操作会重建表。重建操作能更新索引统计数据并释放聚簇索引中的未使用的空间。

  可以使用`show table status like '[table_name]' `查看优化前后表占用的存储空间

- 更新聚集索引的代价很高。因为会强制 InnoDB 将每个被更新的行移动到新的位置

- 基于聚簇索引的表在插入新行是或者主键被更新到只需要移动行的时候，可能面临 `页分裂` 的问题，当行的主键值需要插入某个已经满了的页中时，存储引擎会将该页分裂成两个页面来存储，也就是页分裂操作，页分裂会导致`表占用更多的磁盘空间`

- 聚簇索引可能会导致全表扫描变慢，尤其是行比较稀疏或者由于页分裂导致数据存储不连续的时候

- 二级索引（也是非聚簇索引）可能比想象的要更大，因为在二级索引的叶子节点存储了指向行的主键列。

- 二级索引访问需要两次索引查找，而不是一次。

  二级索引中，叶子节点保存的是指向行的主键值，那么如果通过二级索引进行查找，找到二级索引的叶子节点，会先获取对应数据的主键值，然后再根据这个值去聚簇索引中查找对应的行数据。（`两次索引查找`）



## 二级索引是什么？为什么已经有了聚集索引还需要使用二级索引？

答：

`二级索引`是非主键索引，也是`非聚集索引`（索引和数据分开存放），也就是在非主键的字段上创建的索引就是二级索引。

比如我们给一张表里的 name 字段加了一个索引，在插入数据的时候，就会重新创建一棵 B+ 树，在这棵 B+ 树中，就来存放 name 的二级索引。

即在二级索引中，索引是 name 值，数据（data）存放的是主键的值，第一次索引查找获取了主键值，之后根据主键值再去聚集索引中进行第二次查找，才可以找到对应的数据。

**常见的二级索引：**

- 唯一索引
- 普通索引
- 前缀索引：只适用于字符串类型的字段，取字符串的前几位字符作为前缀索引。




**为什么已经有了聚簇索引还需要使用二级索引？**

聚簇索引的叶子节点存储了完整的数据，而二级索引只存储了主键值，因此二级索引更节省空间。

如果需要为表建立多个索引的话，都是用聚簇索引的话，将占用大量的存储空间。







## 为什么在 InnoDB 表中按逐渐顺序插入行速度更快呢？

答：

向表里插入数据，主键可以选择整数自增 ID 或者 UUID。

- 如果选择自增 ID 作为主键

那么在向表中插入数据时，插入的每一条新数据都在上一条数据的后边，当达到页的最大填充因子（InnoDB 默认的最大填充因子是页大小的 15/16，留出部分空间用于以后修改）时，下一条记录就会被写入到新的页中。



- 如果选择 UUID 作为主键

在插入数据时，由于新插入的数据的主键的不一定比之前的大，所以 InnoDB 需要为新插入的数据找到一个合适的位置——通常是已有数据的中间位置，有以下缺点：

1. 写入的目标也可能已经刷到磁盘上并从内存中删除，或者还没有被加载到内存中，那么 InnoDB 在插入之前，需要先将目标页读取到内存中。`这会导致大量随机 IO`
2. 写入数据是乱序的，所以 InnoDB 会频繁执行页分裂操作
3. 由于频繁的页分裂，页会变得稀疏并且被不规则地填充，最终数据会有碎片



**什么时候使用自增 ID 作为主键反而更糟？**

在高并发地工作负载中，并发插入可能导致间隙锁竞争。







## 了解覆盖索引吗？

答：

覆盖索引：一个索引包含（或说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。

覆盖索引是非常有用的工具，能够极大的提高性能，只需要查询索引而不需要回表，好处有很多：



Mysql回表指的是在InnoDB存储引擎下，二级索引查询到的索引列，如果需要查找所有列的数据，则需要到[主键](https://so.csdn.net/so/search?q=%E4%B8%BB%E9%94%AE&spm=1001.2101.3001.7020)索引里面去取出数据。这个过程就称为回表

- **索引条目通常远小于数据行的大小，如果只需要读取索引，mysql可以大幅减少数据访问量。**对缓存的负载很重要，可以减少数据拷贝花费的时间。覆盖索引对IO密集型应用也很有帮助，索引比数据更小，放到内存中更节省空间。
- 因为索引是按照顺序存放的（至少在单个页内是如此），所以对于IO密集型的范围查询，会比随机从磁盘读取每一行数据的IO要少得多。
- 由于InnoDB的聚簇索引，覆盖索引对InnoDB表特别有用。InnoDB的二级索引在叶子节点中保存了行的主键值，所以如果二级索引能够覆盖查询，则可以避免对主键索引的二次查询。







## 了解索引扫描吗？

答：

MySQL有两种方法生成有序结果：

- 通过排序操作
- 按照索引顺序扫描

如果 explain 出来的 type 列值为 "index" 的话，说明是按照索引扫描了。



**索引扫描本身的速度是很快的。但是如果索引不能覆盖查询所需的全部列的话，那在每次查询索引时都需要回表再查询其他字段，这样的话，按索引顺序读取的速度通常比顺序地全表扫描要慢。如下图，select \*时没有使用索引，select age时使用了索引。**



```sql
explain select age from user order by age; # 结果1
explain select * from user order by age; # 结果2
```





![1698384995928](imgs/1698384995928.png)



**设计：**设计的时候，尽可能让同一个索引既满足排序，又用于查找行，这样是最好的。

只有当索引的列顺序和`order by`子句的顺序完全一致时，MySQL才能使用索引来对结果进行排序，如果查询需要关联多张表时，只有`order by`子句引用的字段全部为第一个表时，才能使用索引做排序。

`order by`查询时，需要满足索引的最左前缀要求，否则MySQL需要执行排序操作，无法利用索引进行排序。

`order by`有一种情况可以不满足索引的最左前缀的要求：前导列为常量。（即如果age,name为索引列，那么`select * from user where age = 30 order by name`，使用where将age指定为常量，这时也是可以使用索引排序的）



## 索引这么多优点，为什么不对表中的每一个列创建一个索引呢？使用索引一定提高查询性能吗？

答：

如果出现过多的重复索引和未使用索引，会影响插入、删除、更新的性能。

例如，如果创建了一个主键id，再去向id上添加索引，那么就添加了重复的索引，因为MySQL的主键限制也是通过索引实现的。

冗余索引是：如果创建了索引（A, B），再创建索引（A）就是冗余索引，因为（A）是（A, B）的前缀索引。

还有一种情况是，（A, ID）其中ID是主键，也是冗余索引，因为在 InnoDB 中，二级索引的叶子节点中已经包含主键值了。



**使用索引一定提高查询性能吗？**

不一定

- 在数据量比较小的表中，使用全表扫描比使用索引扫描速度更快，并且可以直接获取到全量数据
- 索引虽然提高了查询性能，但是在插入、删除、更新的过程中也是需要进行维护的





## 最左前缀匹配原则？

答：

最左前缀原则：规定了联合索引在何种查询中才能生效。

规则如下：

- 如果想使用联合索引，联合索引的最左边的列必须作为过滤条件，否则联合索引不生效

如下图：

![1698391069588](imgs/1698391069588.png)

```sql
假如索引为：(name, age, position)
select * from employee where name = 'Bill' and age = 31;
select * from employee where age = 30 and position = 'dev';
select * from employee where position = 'manager';
```

对于上边三条 sql 语句，只有第一条 sql 语句走了联合索引。

**为什么联合索引需要遵循最左前缀原则呢？**

因为索引的排序是根据第一个索引、第二个索引依次排序的，假如我们单独使用第二个索引 age 而不使用第一个索引 name 的话，我们去查询age为30的数据，会发现age为30的数据散落在链表中，并不是有序的，所以使用联合索引需要遵循最左前缀原则。







## 索引下推？

答：

在索引遍历过程中，对索引中包含的所有字段先做判断，过滤掉不符合条件的记录之后再回表，可以有效减少回表次数

比如：

```sql
索引：（name, age, positioni）
SELECT * FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager';
```

对上面这条 sql 语句就是用了`索引下推`，经过索引下推优化后，在联合索引（name，age，position）中，匹配到名字是 LiLei 开头的索引之后，同时还会在索引中过滤 age、position 两个字段的值是否符合，最后会拿着过滤完剩下的索引对应的主键进行回表，查询完整数据

（MySQL5.6 之前没有索引下推，因此匹配到 name 为 LiLei 开头的索引之后，会直接拿到主键，进行回表查询）



**优点：**

- 索引下推可以有效减少回表次数
- 对于 InnoDB 引擎的表，索引下推只能`用于二级索引`，因为 InnoDB 的主键索引的叶子节点存储的是全行数据，如果在主键索引上使用索引下推并不会减少回表次数 




## 了解 Explain 执行计划吗？

答：

explain 语句可以帮助我们查看查询语句的具体执行计划。



explain 查出来的各列含义如下：

- id：在一个大的查询语句中，每个 select 关键字都对应一个唯一的 id

- select_type：select 关键字对应的那个查询的类型

  - simple：简单查询。表示查询不包含子查询和union

  - primary：复杂查询中最外层的 select

  - subquery：包含在 select 中的子查询（不在 from 子句中）

  - derived：包含在 from 子句中的子查询。MySQL会将结果存放在一个临时表中，也称为派生表（derived的英文含义）

    ```sql
    set session optimizer_switch='derived_merge=off';  #关闭 mysql5.7 新特性对衍生表的合并优化
    explain select (select 1 from employees where id = 5) from (select * from account where id = 3) der;
    set session optimizer_switch='derived_merge=on'; #还原默认配置
    ```

    ![1698718620195](imgs/1698718620195.png)

  - union：在 union 中的第二个和随后的 select

    ```sql
    explain select 1 union all select 1;
    ```

    ​

- partitions：匹配的分区信息

- type：表示访问类型，即 MySQL 决定如何查找表中的行。从最优到最差分别为：`system > const > eq_ref > ref > range > index > ALL`

  一般来说得保证查询达到 range 级别，最好达到 ref

  - Null：表示 MySQL 在优化阶段分解查询语句，执行时不需要再访问表或索引。例如 `explain select min(id) from account;` 在索引列中取最小值，单独查询索引即可，执行时不需要再访问表

  - system：当表中只有一条记录并且该表使用的存储引擎的统计数据是精确的，比如 `explain select * from test;` 在 `test` 表中只有一条数据，如果 test 表使用 MyISAM 存储引擎，则 type 为 system；如果 test 表使用 InnoDB 存储引擎，则 type 为 ALL

  - const：const 表示代价时常数级别，当根据主键索引、唯一索引、二级索引与常数进行等值匹配时，对单表访问就是 const，只匹配到一行数据，很快.

    `explain select * from account where id = 1 `

  - eq_ref：primary key 或 unique key 索引的所有部分被连接使用 ，最多只会返回一条符合条件的记录。这可能是在 const 之外最好的联接类型了，简单的 select 查询不会出现这种 type。

    `explain select * from t1 left join t2 on t1.id=t2.id`

  - ref：相比于 eq_ref，不使用唯一索引，使用普通索引或者唯一索引的部分前缀，索引值和某个值相比较，可能找到多个符合条件的行

    name 是普通索引（非唯一索引），`explain select * from account where name = 'abc'`

  - range：范围扫描通常出现在 `in()`、`between`、`>`、`<`等操作

  - index：扫描全索引就能拿到结果，一般是扫描某个二级索引，会直接对二级索引的叶子节点遍历和扫描。这种查询一般为使用了覆盖索引，二级索引比较小，所以通常比 ALL 快一些

  - ALL：全表扫描，扫描聚簇索引的所有叶子节点，通常情况下这需要增加索引来进行优化

- possible_keys：可能用到的索引

  查询时可能出现 possible_keys 有列，但是 key 显示 Null 的情况，这是因为表中数据不多，MySQL 认为索引帮助不大，选择了全表扫描

  如果该列是 Null，说明没有相关索引，可以通过添加索引来提高查询性能

- key：实际上使用的索引

  如果为 Null 表示没有使用索引，可以使用 `force index`、`ignore index` 来强制使用索引

- key_len：实际使用到的索引长度

  key_len 计算规则如下：

  - 字符串，char(n)、varchar(n) 在 5.0.3 之后，n 代表字符数，而不是字节数，如果是 utf-8，一个数字或字母占 1 个字节，1 个汉字占 3 个字节
    - char(n)：如果存汉字，长度为 3n 字节
    - varchar(n)：
      - 如果存汉字（utf8），长度为 3n+2 字节，加的 2 字节用来存储字符串长度（varchar 是变长字符串）
      - 如果存汉字（utf8-mb4），长度为 4n+2 字节
  - 数值类型
    - tinyint：1 字节
    - smallint：2 字节
    - int：4 字节
    - bigint：8 字节
  - 时间类型：
    - date：3 字节
    - timestamp：4 字节
    - datetime：8 字节
  - 如果字段允许为 Null，则还需要 1 字节记录是否为 Null

  **计算示例：**

  - 设置索引：`idx_balance(balance)`，`explain select name from account where balance = '111' ;`

    该 SQL key_len = 5，4 个字节用于存储 balance（int，4B），1 个字节记录是否为 Null

  - 设置索引：idx_name(name)，name 字段编码为 uft8-mb4，长度为varchar(10)，`explain select name from account where name = 'abc';`

    该 SQL key_len = 43，name 索引长度为 10，使用 utf8-mb4 存储汉字的话，1 个汉字占 4 个字节，长度为 10 所占用字节为 4 * 10 = `40`，还需要 `2` 个字节存储 varchar 的长度，name 字段可以为空，因此还需要 `1` 个字节记录是否为 Null，因此 name 索引的长度为 `40 + 2 + 1 = 43`

    如果是 utf-8 编码，1 个汉字是占 3 个字节的。

- ref：当使用索引列等值查询时，与索引列进行等值匹配的对象信息，常见的 ref 值有：const（常量），字段名（例如：film.id）

- rows：预估的需要读取的记录条数，并不是结果集中的行数

- Extra：—些额外的信息，常见的重要值如下：

  - Using index：使用覆盖索引
  - Using where：使用 where 语句来处理结果，并且查询的列未被索引覆盖
  - Using index condition：查询的列不完全被索引覆盖，where 条件中是一个前导列的范围
    - 示例：索引（name，balance） `explain select *from account where name > 'a';`
  - Using temporary：mysql 需要创建一张临时表来处理查询。出现这种情况需要使用索引进行优化
    - 示例：name 字段没有索引，此时创建一张临时表来 distinct，`explain select distinct name from account`
  - Using filesort：使用外部排序而不是索引排序，数据较少时在内存中排序，数据较大时在磁盘中排序，一般情况下也是需要考虑使用索引进行优化
    - 示例：name 字段没有索引，`explain select name from account order by name`
  - Select tables optimized away：使用聚合函数来访问存在索引的某个字段
    - 示例：`explain select min(id) from account;`



## MySQL 的锁

答：

MySQL 的锁

从数据操作的粒度分的话，分为`表锁`和`行锁`

从数据操作的类型分的话，分为`读锁`和`写锁`



### 表锁

每次操作锁住整张表，锁粒度大，性能低



- 手动增加表锁（可以给表加读锁或写锁，如果加读锁，其他会话可以读，但是无法写；如果加写锁。其他会话的读写都会被阻塞）

`lock table 表名 read(write);` 

- 查看表上加过的锁

`show open tables;`

- 删除表锁

`unlock tables;`



### 行锁

每次操作锁住一行数据，锁力度小，性能高



**InnoDB与MYISAM的最大不同有两点：**

- InnoDB支持事务
- InnoDB支持行级锁





### 总结

- MyISAM 在执行查询语句 select 前，会自动给涉及的所有表加读锁,在执行 update、insert、delete 操作会自动给涉及的表加写锁。

- InnoDB 在执行查询语句 select  时(非串行隔离级别)，不会加锁。但是 update、insert、delete 操作会加行锁。

- 简而言之，就是读锁会阻塞写，但是不会阻塞读。而写锁则会把读和写都阻塞。

- **锁主要是加在索引上，如果对非索引字段更新，行锁可能会变表锁：**

  假如 account 表有 3 个字段（id, name, balance），我们在 name、balance 字段上并没有设置索引

  session1 执行：

  ```sql
  mysql> begin;
  Query OK, 0 rows affected (0.00 sec)

  mysql> select * from account;
  +----+------+---------+
  | id | name | balance |
  +----+------+---------+
  |  1 | zs   |     777 |
  |  2 | ls   |     800 |
  |  3 | ww   |     777 |
  |  4 | abc  |     999 |
  | 10 | zzz  |    2000 |
  | 20 | mc   |    1500 |
  +----+------+---------+
  6 rows in set (0.01 sec)

  mysql> update account set balance = 666 where name='zs';
  Query OK, 1 row affected (0.00 sec)
  Rows matched: 1  Changed: 1  Warnings: 0
  ```

  此时 session2 执行（发现执行阻塞，经过一段时间后，返回结果锁等待超时，证明 session1 在没有索引的字段上加锁，导致`行锁升级为表锁`，因此 session2 无法对表中其他数据做修改）：

  ```sql
  mysql> begin;
  Query OK, 0 rows affected (0.00 sec)

  mysql> update account set balance = 111 where name='abc';
  RROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction
  ```

  **`InnoDB 的行锁是针对索引加的锁，不是针对记录加的锁。并且该索引不能失效，否则会从行锁升级为表锁`**






## select for update 了解吗？

答：

`select for update` 即排他锁，根据 where 条件的不同，对数据加的锁又分为行锁和表锁：

- 如果 where 字段使用到了索引，则会添加行锁
- 如果 where 字段没有使用索引，则会添加表锁




## 并发事务带来的问题

答：

- `脏写：`多个事务更新同一行，每个事务不知道其他事务的存在，最后的更新覆盖了其他事务所做的更新
- `脏读：`事务 A 读取到了事务 B 已经修改但是没有提交的数据，此时如果事务 B 回滚，事务 A 读取的则为脏数据
- `不可重复读：`事务 A 内部相同的查询语句在不同时刻读出的结果不一致，在事务 A 的两次相同的查询期间，有其他事务修改了数据并且提交了
- `幻读：`当事务 A 读取到了事务 B 提交的新增数据

**不可重复读和幻读很类似，都是事务 A 读取到了事务 B 新提交的数据，区别为：**

- 不可重复读是读取了其他事务更改的数据，针对 `update` 操作
- 幻读是读取了其他事务新增的数据，针对 `insert` 和 `delete` 操作





## MySQL 的事务隔离级别了解吗？

答：

MySQL 的事务隔离级别分为：

- 读未提交：事务 A 会读取到事务 B 更新但没有提交的数据。如果事务 B 回滚，事务 A 产生了脏读

- 读已提交：事务 A 会读取到事务 B 更新且提交的数据。事务 A 在事务 B 提交前后两次查询结果不同，产生不可重复读

- 可重复读：保证事务 A 中多次查询数据一致。**`可重复读是 MySQL 的默认事务隔离级别`**。可重复读可能会造成`幻读` ，事务A进行了多次查询，但是事务B在事务A查询过程中新增了数据，事务A虽然查询不到事务B中的数据，但是可以对事务B中的数据进行更新

  演示可重复读：

  1. 先创建一个事务 A，查询表

     ```bash
     mysql> start transaction;
     Query OK, 0 rows affected (0.00 sec)

     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |     150 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)
     ```

     ​

  2. 此时再去创建事务 B，修改数据让第一条数据的 balance 减去 50，并且提交事务

     ```bash
     mysql> start transaction;
     Query OK, 0 rows affected (0.00 sec)

     mysql> update account set balance = balance - 50 where id = 1;
     Query OK, 1 row affected (0.00 sec)
     Rows matched: 1  Changed: 1  Warnings: 0

     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |     100 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)

     mysql> commit;
     Query OK, 0 rows affected (0.00 sec)

     ```

  3. 在事务 B 提交之后，在事务 A 中查询数据，发现与上次查询数据一致（可重复读）

     ```bash
     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |     150 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)
     ```

  4. 在事务 A 中让第一条数据的 balance 减去 50，由于在事务 B 中已经减去 1 次 50 了，所以在事务 A 中的 update account set balance = balance - 50 where id = 1 语句中的 balance 值使用的是上边步骤 2 中的 100，所以事务 A 中更新之后的 balance 值为 50。`可重复读的隔离级别下使用了 MVCC 机制，select 操作不会更新版本号，是快照读（历史版本）；insert、update 和 delete 会更新版本号，是当前读（当前版本）` 

     ```bash
     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |     150 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.01 sec)

     mysql> update account set balance = balance - 50 where id = 1;
     Query OK, 1 row affected (0.00 sec)
     Rows matched: 1  Changed: 1  Warnings: 0

     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |      50 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)

     ```

  5. 验证出现幻读，重新打开事务 B，插入一条数据

     ```bash
     mysql> start transaction;
     Query OK, 0 rows affected (0.00 sec)

     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |      50 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)

     mysql> insert into account values(4, 'abc', 300);
     Query OK, 1 row affected (0.00 sec)
     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |      50 |
     |  2 | ls   |     200 |
     |  4 | abc  |     300 |
     +----+------+---------+
     3 rows in set (0.00 sec)

     mysql> commit;
     Query OK, 0 rows affected (0.01 sec)
     ```

  6. 在事务 A 中查询，发现没有看到事务 B 新插入的数据，但是事务 A 可以直接更新事务 B 中插入的数据（`说明事务 A 是可以感知到事务 B 插入的数据，因此发生了幻读`），更新之后，事务 A 再次查询就查询到了事务 B 的数据。

     ```bash
     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |      50 |
     |  2 | ls   |     200 |
     +----+------+---------+
     2 rows in set (0.00 sec)

     mysql> update account set balance = 888 where id = 4;
     Query OK, 1 row affected (0.00 sec)
     Rows matched: 1  Changed: 1  Warnings: 0

     mysql> select * from account;
     +----+------+---------+
     | id | name | balance |
     +----+------+---------+
     |  1 | zs   |      50 |
     |  2 | ls   |     200 |
     |  4 | abc  |     888 |
     +----+------+---------+
     3 rows in set (0.00 sec)
     ```

     ​

- 可串行化：并发性能低，不常使用







## MySQL 的间隙锁了解吗？

答：

间隙锁，用于锁两个值之间的空隙，在 InnoDB 中，默认的隔离级别是可重复读，并且`通过间隙锁来防止在这个隔离级别上产生幻读`

间隙锁：InnoDB 不仅锁定在查询中涉及的行，还会对索引结构中的间隙进行锁定，在一些情况下可以解决幻读情况



假设表中数据如下：

```bash
mysql> select * from account;
+----+------+---------+
| id | name | balance |
+----+------+---------+
|  1 | zs   |      50 |
|  2 | ls   |     200 |
|  3 | ww   |     210 |
|  4 | abc  |     888 |
| 10 | zzz  |    2000 |
| 20 | mc   |    1500 |
+----+------+---------+
6 rows in set (0.01 sec)
```



那么间隙就有 3 个区间：(4, 10), (10, 20), (20, 正无穷)

如果在事务 A 中执行更新语句：`update account set name = 'hello' where id > 8 and id < 18;`，则在其他事务中就无法在这个范围所包含的所有行记录以及行记录所在的间隙里插入或修改数据，即 id 在 `(4, 20]` 区间，都无法修改数据。





**间隙锁在可重复隔离级别下才会生效**



演示过程如下：

1. 事务 A 先开启事务，并且更新

   ```bash
   mysql> start transaction;
   Query OK, 0 rows affected (0.00 sec)

   mysql> select * from account;
   +----+------+---------+
   | id | name | balance |
   +----+------+---------+
   |  1 | zs   |      50 |
   |  2 | ls   |     200 |
   |  3 | ww   |     210 |
   |  4 | abc  |     888 |
   | 10 | zzz  |    2000 |
   | 20 | mc   |    1500 |
   +----+------+---------+
   6 rows in set (0.00 sec)

   mysql> update account set name = hello where id > 8 and id < 18;
   ERROR 1054 (42S22): Unknown column 'hello' in 'field list'
   mysql> update account set name = 'hello' where id > 8 and id < 18;
   Query OK, 1 row affected (0.00 sec)
   Rows matched: 1  Changed: 1  Warnings: 0
   ```

2. 事务 B 开启事务，在间隙中插入数据，发现插入被阻塞

   ```bash
   mysql> start transaction;
   Query OK, 0 rows affected (0.00 sec)

   mysql> select * from account;
   +----+------+---------+
   | id | name | balance |
   +----+------+---------+
   |  1 | zs   |      50 |
   |  2 | ls   |     200 |
   |  3 | ww   |     210 |
   |  4 | abc  |     888 |
   | 10 | zzz  |    2000 |
   | 20 | mc   |    1500 |
   +----+------+---------+
   6 rows in set (0.00 sec)

   mysql> insert into account values(6, 'world', 150);
   ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction
   mysql> insert into account values(6, 'world', 150);
   ```

   ​









## 设置 MySQL 的事务隔离级别

答：

通过以下命令设置：

```sql
show variables like 'tx_isolation'; # 查看事务隔离级别
set tx_isolation='read-uncommitted'; # 设置读未提交
set tx_isolation='read-committed'; # 设置读已提交
set tx_isolation='repeatable-read'; # 设置可重复读
set tx_isolation='serializable'; # 设置可串行化
```







## 什么是慢 sql，如何查找，如何优化？

答：











> 13. mysql 的三大日志？

> 14. mysql 的事务隔离级别？各自解决了什么问题？mvcc的流程？

> 15. mysql 性能怎么优化？

索引（比如覆盖索引、最左前缀匹配原则）、表结构（选择合适的字段属性和数据类型）、SQL基本编写规范、优化慢SQL（慢SQL定位、Explain 命令使用）、分库分表和读写分离、加强运维（比如通过一些监控工具监控慢 SQL）

可以先聊慢SQL定位以及EXPLAIN 命令的应用，再聊索引、表结构以及SQL基本编写规范，分库分表和读写分离这些最后考虑。如非迫不得已，一定不需要选择分库分表，带来的问题不少

读写分离：https://javaguide.cn/high-performance/read-and-write-separation-and-library-subtable.html

https://javaguide.cn/database/mysql/mysql-high-performance-optimization-specification-recommendations.html


> 16. 索引失效七种？



> 17. mysql 的三大 log 的执行时机？redolog刷盘时机？



> 18. 一条模糊查询语句，查询速度越来越慢怎么排查？

从两个方面进行排查：

1. 确认是否建立了索引
2. 确认索引是否生效

> 19. MySQL如何保证事务的一致性？





> 20. 一张 MySQL 数据库表存多少条数据合适？





> 21. B+ 树索引有哪些好处？还有哪些数据结构索引？







> 22. InnoDB 怎么实现的事务？





> 23. MySQL 执行一个 update 语句的全流程？（客户端到服务器）





> 24. MySQL ACID 怎么实现的？





> 25. MySQL 新旧数据同步，怎么切流量使同步更平滑？







> 26. 分布式数据库的主从怎么做的？（读写分离）







> 27. 如果主数据库崩了怎么办？





## 介绍一下 join、left join、right join 区别？

答：

- join 和 inner join 是相同的，inner join 可以简写为 join，表示两个表的交集
- left join 是左连接，以左表为主，查询左表所有数据，显示右表与左表相关联部分的数据
- right join 是右连接，以右表为主，查询右表所有数据，显示左表与右表相关联部分的数据

下边我们为测试数据：

**t1 表数据：** 

![1698727323906](imgs/1698727323906.png)

**t2 表数据：**

![1698727335433](imgs/1698727335433.png)



**测试结果：**

```sql
select * from t1 join t2 on t1.id = t2.id; # 结果1
select * from t1 left join t2 on t1.id = t2.id; # 结果2
select * from t1 right join t2 on t1.id = t2.id; # 结果3
```

![1698727481314](imgs/1698727481314.png)

![1698727487706](imgs/1698727487706.png)

![1698727494906](imgs/1698727494906.png)











## 介绍一下Union和union all的区别？

答：

使用union关键字时，可以给出多条select 语句，并将它们的结果合成单个结果集合并时两个表对应的列数和数据类型必须相同。

- union 对两个结果集进行并集操作，并且去重和排序
- union all 只是合并多个查询结果，不会进行去重和排序，效率比 union 高



> 29. mysql 什么语句会触发什么样的锁？（这里挖坑了，说select查询时会用表锁，实际上比较少用，alter这种ddl语句会用）





> 30. 什么是幻读？脏读等等，，，







> 31. 加行锁一定要设计索引吗？









> 32. 知道项目用的什么事务模型吗？





> 33. b+树与b树区别，为什么选择b+树

实际场景中，我们需要的是一个时间复杂度稳定的数据结构，而不是忽高忽低





> 34. select for update



> 35. 数据库如何做乐观锁？







> 36. 如何保证mysql的RR（用锁和MVCC）





> 37. 调试过数据库参数吗





> 38. 讲讲红黑树





> 39. mysql主从数据库设计





> 40. mydql刷盘原理





> 41. 什么情况下加什么行锁







> 42. SELECT COUNT(*)怎么走索引









> 43. 已经用了MQ了为什么还会出现mysql连接数过高，怎么优化mysql的性能瓶颈，怎么分库分表？

mysql 连接数过高的原因？？？









> 44. MySql怎么删除数据。







> 45. delete和truncate的区别（不会）







> 46. 写查询语句的时候应该从哪些方面考虑来注意性能。







> 47. 什么是联合索引，为什么要建联合索引？









> 48. a,b,c,d，四个字段，查询语句的where条件a=b，orderby c。（mysql翻页越翻越慢怎么优化，满足a=b的字段很多，怎么高效的排序，分页查询）







> 49. SQL explain 会输出哪些信息？







> 50. sql怎么手动加锁










## 如何判断是否使用索引？

答：

### 建表 SQL

```sql
CREATE TABLE `employees` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(24) NOT NULL DEFAULT '' COMMENT '姓名',
`age` int(11) NOT NULL DEFAULT '0' COMMENT '年龄',
`position` varchar(20) NOT NULL DEFAULT '' COMMENT '职位',
`hire_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '入职时间',
PRIMARY KEY (`id`),
KEY `idx_name_age_position` (`name`,`age`,`position`) USING BTREE
) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT='员工记录表';

INSERT INTO employees(name,age,position,hire_time) VALUES('LiLei',22,'manager',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('HanMeimei', 23,'dev',NOW());
INSERT INTO employees(name,age,position,hire_time) VALUES('Lucy',23,'dev',NOW());

 ‐‐ 插入一些示例数据
drop procedure if exists insert_emp;
delimiter ;;
create procedure insert_emp()
begin
declare i int;
set i=1;
while(i<=100000)do
insert into employees(name,age,position) values(CONCAT('zqy',i),i,'dev');
set i=i+1;
end while;
end;;
delimiter ;
call insert_emp()
```



### 1、联合索引第一个字段用范围不走索引

```sql
EXPLAIN SELECT * FROM employees WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698410787474](imgs/1698410787474.png)



`结论：`type 为 ALL 表示进行了全表扫描，mysql 内部可能认为第一个字段使用范围，结果集可能会很大，如果走索引的话需要回表导致效率不高，因此直接使用全表扫描



### 2、强制走索引

```sql
EXPLAIN SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698410943905](imgs/1698410943905.png)



`结论：`虽然走了索引，扫描了 50103 行，相比于上边不走索引扫描的行数少了一半，但是查找效率不一定比全表扫描高，因为回表导致效率不高。



**可以使用以下代码测试：**

```sql
set global query_cache_size=0;
set global query_cache_type=0;
SELECT * FROM employees WHERE name > 'LiLei' limit 1000;
> OK
> 时间: 0.408s
SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' limit 1000;
> OK
> 时间: 0.479s
SELECT * FROM employees WHERE name > 'LiLei' limit 5000;
> OK
> 时间: 0.969s
SELECT * FROM employees force index(idx_name_age_position) WHERE name > 'LiLei' limit 5000;
> OK
> 时间: 0.827s
```

`结论：`在查询 1000 条数据的话，全表扫描还是比走索引消耗时间短的，但是当查询 5000 条数据时，还是走索引效率高





### 3、覆盖索引优化

```sql
EXPLAIN SELECT name,age,position FROM employees WHERE name > 'LiLei' AND age = 22 AND position ='manager';
```

![1698411792445](imgs/1698411792445.png)

`结论：`使用覆盖索引不需要回表，效率更高





### 4、in、or

in和or在表数据量比较大的情况会走索引，在表记录不多的情况下会选择全表扫描

```sql
EXPLAIN SELECT * FROM employees WHERE name in ('LiLei','HanMeimei','Lucy') AND age = 22 AND position='manager'; # 结果1
EXPLAIN SELECT * FROM employees WHERE (name = 'LiLei' or name = 'HanMeimei') AND age = 22 AND position='manager'; # 结果2
```

![1698412051558](imgs/1698412051558.png)

![1698412060745](imgs/1698412060745.png)

`结论：`in、or 的查询的 type 都是 range，表示使用一个索引来检索给定范围的行



给原来的 employee 表复制为一张新表 employee_copy ，里边只保留 3 条记录

![1698412157784](imgs/1698412157784.png)

```sql
EXPLAIN SELECT * FROM employees_copy WHERE name in ('LiLei','HanMeimei','Lucy') AND age = 22 AND position ='manager';
EXPLAIN SELECT * FROM employees_copy WHERE (name = 'LiLei' or name = 'HanMeimei') AND age = 22 AND position ='manager';
```

![1698412186513](imgs/1698412186513.png)

![1698412193177](imgs/1698412193177.png)

`结论：`in、or 的查询的 type 都是 ALL，表示进行了全表扫描，没有走索引





### 5、like KK% 一般情况都会走索引

```sql
 EXPLAIN SELECT * FROM employees WHERE name like 'LiLei%' AND age = 22 AND position ='manager';
```

![1698412776696](imgs/1698412776696.png)

```sql
EXPLAIN SELECT * FROM employees_copy WHERE name like 'LiLei%' AND age = 22 AND position ='manager';
```

![1698412788320](imgs/1698412788320.png)





## 常见的 SQL 优化

答：

### order by、group by 优化

下边是 8 种使用 order by 的情况，我们通过分析以下案例，可以判断出如何使用 order by 和 where 进行配合可以走`using index condition（索引排序）`而不是 `using filesort（文件排序）`



- **case1**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' and position = 'dev' order by age;
```

![1698414238215](imgs/1698414238215.png)

`分析：`查询用到了 name 索引，从 key_len=74 也能看出，age 索引列用在排序过程中，因此 Extra 字段里没有 using filesort



- **case2**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by position;
```

![1698414291344](imgs/1698414291344.png)



`分析：`从 explain 执行结果来看，key_len = 74，查询使用了 name 索引，由于用了 position 进行排序，跳过了 age，出现了 Using filesort





- **case3**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by age, position;
```

![1698414875454](imgs/1698414875454.png)

`分析：`查找只用到索引name，age和position用于排序，与联合索引顺序一致，因此无 using filesort。





- **case4**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' order by position, age;
```

![1698417882580](imgs/1698417882580.png)

`分析：`因为索引的创建顺序为 name,age,position，但是排序的时候 age 和 position 颠倒位置了，和索引创建顺序不一致，因此出现了 using filesort





- **case5**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'LiLei' and age = 18 order by position, age;
```

![1698415102710](imgs/1698415102710.png)

`分析：`与 case 4 相比，Extra 中并未出现 using filesort，并且查询使用索引 name，age，排序先根据 position 索引排序，索引使用顺序与联合索引顺序一致，因此使用了索引排序





- **case6**

```sql
EXPLAIN SELECT * FROM employees WHERE name = 'zqy' order by age asc, position desc;
```

![1698415273825](imgs/1698415273825.png)

`分析：`虽然排序字段列与联合索引顺序一样，但是这里的 position desc 变成了降序排序，`导致与联合索引的排序方式不同`，因此产生了 using filesort







- **case7**

```sql
EXPLAIN SELECT * FROM employees WHERE name in ('LiLei', 'zqy') order by age, position;
```

![1698415435334](imgs/1698415435334.png)

`分析：`先使用索引 name 拿到 LiLei，zqy 的数据，之后需要根据 age、position 排序，但是根据 name 所拿到的数据对于 age、position 两个字段来说是无序的，所以需要使用到 filesort。

> 为什么根据 name in 拿到的数据对于 age、position 来说是无序的：
>
> 对于下图来说，如果取出 name in (Bill, LiLei) 的数据，那么对于 age、position 字段显然不是有序的，因此肯定无法使用索引扫描排序



![1698416520734](imgs/1698416520734.png)





- **case8**

```sql
EXPLAIN SELECT * FROM employees WHERE name > 'a' order by name;
```

![1698416982916](imgs/1698416982916.png)

`分析：`对于上边这条 sql 来说，是 select * 因此 mysql 判断不走索引，直接全表扫描更快，因此出现了 using filesort

```sql
EXPLAIN SELECT name FROM employees WHERE name > 'a' order by name;
```

![1698416970884](imgs/1698416970884.png)

`分析：`因此可以使用`覆盖索引`来优化，只通过索引查询就可以查出我们需要的数据，不需要回表，通过覆盖索引优化，因此没有出现 using filesort





#### 优化总结

1. MySQL支持两种方式的排序 filesort 和 index，Using index 是指 MySQL 扫描索引本身完成排序。index 效率高，filesort 效率低。
2. order by满足两种情况会使用Using index。
   - order by语句使用索引最左前列。
   - 使用where子句与order by子句条件列组合满足索引最左前列。
3. 尽量在索引列上完成排序，遵循索引建立（索引创建的顺序）时的最左前缀法则。
4. 如果order by的条件不在索引列上，就会产生Using filesort。
5. 能用覆盖索引尽量用覆盖索引
6. group by 与 order by 很类似，其实质是先排序后分组，遵照索引创建顺序的最左前缀法则。对于 group by 的优化如果不需要排序的可以加上 order by null 禁止排序。注意，where 高于 having，能写在 where 中的限定条件就不要去 having 限定了。





### 分页查询优化

我们实现分页功能可能会用以下 sql：

```sql
select * from employees limit 10000, 10;
```

该 sql 表示从 employees 表的第 10001 行开始的 10 行数据，虽然只查询了 10 条数据，但是会先去读取 10010 条记录，再抛弃前 10000 条数据，因此如果查询的数据比较靠后，效率非常低



#### 1、根据自增且连续的主键排序的分页查询

该优化必须保证主键是自增的，并且主键连续，中间没有断层。



**未优化 sql** 

```sql
select * from employees limit 9000, 5;
```

**结果：**

![1698420438962](imgs/1698420438962.png)

**执行计划：**

![1698420480234](imgs/1698420480234.png)



因为 id 是连续且自增的，所以可以直接通过 id 判断拿到 id 比 9000 大的 5 条数据，效率更高：





**优化后 sql**

```sql
select * from employees where id > 9000 limit 5;
```



**结果**

![1698420450449](imgs/1698420450449.png)

**执行计划：** 

![1698420463722](imgs/1698420463722.png)





**总结**

- 如果主键空缺，则不能使用该优化方法



#### 2、根据非主键字段排序的分页查询



**未优化 sql**

```sql
select * from employees order by name limit 9000, 5;
> OK
> 时间: 0.066s
```

![1698421203659](imgs/1698421203659.png)

```sql
explain select * from employees order by name limit 9000, 5;
```

![1698421230058](imgs/1698421230058.png)

根据`执行计划`得，使用了全表扫描（type=ALL），并且 Extra 列为 using filesort，原因是联合索引为（name，age，position），但是使用了 select \* 中有的列并不在联合索引中，如果使用索引还需要回表，因此 mysql 直接进行全表扫描





**优化 sql**

`优化的点在于：`让在排序时返回的字段尽量为覆盖索引，这样就会走索引并且还会使用索引排序

先让排序和分页操作查出主键，再根据主键查到对应记录

```sql
select * from employees e inner join (select id from employees order by name limit 9000, 5) ed on e.id = ed.id;
> OK
> 时间: 0.032s
```

![1698421454673](imgs/1698421454673.png)

```sql
explain select * from employees e inner join (select id from employees order by name limit 9000, 5) ed on e.id = ed.id;
```

![1698421472937](imgs/1698421472937.png)

根据`执行计划`得，优化后查询走了索引，并且排序使用了索引排序





**总结**

- 优化后，sql 语句的执行时间时原 sql 的一半



```sql
CREATE TABLE `t1` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `a` int(11) DEFAULT NULL,
    `b` int(11) DEFAULT NULL,
    PRIMARY KEY (`id`),
    KEY `idx_a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

create table t2 like t1;  
‐‐ 插入一些示例数据
‐‐ 往t1表插入1万行记录
drop procedure if exists insert_t1;
delimiter ;;
create procedure insert_t1()
begin
	declare i int;
	set i=1;
	while(i<=10000)do
		insert into t1(a,b) values(i,i);
		set i=i+1;
	end while;
end;;
delimiter ;
call insert_t1();

‐‐ 往t2表插入100行记录
drop procedure if exists insert_t2;
delimiter ;;
create procedure insert_t2()
begin
	declare i int;
	set i=1;
	while(i<=100)do
		insert into t2(a,b) values(i,i);
		set i=i+1;
	end while;
end;;
delimiter ;
call insert_t2();
```





### in 和 exists 优化

原则：小表驱动大表

`in：`当 B 表的数据集小于 A 表的数据集时，使用 `in`

```sql
select * from A where id in (select id from B)
```



`exists：`当 A 表的数据集小于 B 表的数据集时，使用 `exists`

将主查询 A 的数据放到子查询 B 中做条件验证，根据验证结果（true 或 false）来决定主查询的数据是否保留

```sql
select * from A where exists (select 1 from B where B.id = A.id)
```



**总结**

- exists 只返回 true 或 false，因此子查询中的 select * 也可以用 select 1 替换





### count(\*)查询优化

```sql
‐‐ 临时关闭mysql查询缓存，为了查看sql多次执行的真实时间
set global query_cache_size=0;
set global query_cache_type=0;
EXPLAIN select count(1) from employees;
EXPLAIN select count(id) from employees;
EXPLAIN select count(name) from employees;
EXPLAIN select count(*) from employees;
```

![1698458884241](imgs/1698458884241.png)



`分析：`4 条 sql 语句的执行计划一样，说明这 4 个 sql 的执行效率差不多





**总结**

- 当字段有索引，执行效率：`count(*) ≈ count(1) > count(字段) > count(主键id)`

  如果字段有索引，走二级索引，二级索引存储的数据比主键索引少，所以 `count(字段)` 比 `count(主键id)` 效率更高

- 当字段无索引，执行效率：`count(*) ≈ count(1) > count(主键id) > count(字段)`

- `count(1)` 和 `count(*)` 比较

  - `count(1)` 不需要取出字段统计，使用常量 1 做统计，`count(字段)` 还需要取出字段，所以理论上 `count(1)` 比 `count(字段)` 快

  - `count(*)` 是例外，mysql 并不会把全部字段取出来，会忽略所有的列直接，效率很高，所以不需要用

    `count(字段)` 或 `count(常量)` 来替代 `count(*)`

- 为什么对于 `count(id)`，mysql最终选择辅助索引而不是主键聚集索引？因为二级索引相对主键索引存储数据更少，检索

  性能应该更高，mysql内部做了点优化（应该是在5.7版本才优化）。










## 了解 MVCC 吗？

答：

MVCC（Multi-Version Concurrency Control） 是用来保证 MySQL 的事务隔离性的，对一行数据的读和写两个操作默认是不会通过加锁互斥来保证隔离性，避免了频繁加锁互斥，而在串行化隔离级别为了保证较高的隔离性是通过将所有操作加锁互斥来实现的。



**MySQL 在`读已提交`和`可重复读`隔离级别下都实现了 MVCC 机制，ReadView 生成规则为：**

- 在读已提交隔离级别下，ReadView 生成的时机是每个 Select 生成一个 ReadView
- 在可重复读隔离级别下，ReadView 生成的时机是每个事务生成一个 ReadView



MVCC 是基于 **undolog**、**版本链**、**readview** 实现的。

![MVCC](imgs/MVCC.png)

在每次更新或者删除数据时，都会将操作记录在 `undo 日志`中，每条 `undo 日志` 通过 `roll_pointer` 进行关联，构成了数据的`版本链`



**ReadView** 中包含以下参数：

- m_ids：表示生成 ReadView 时，当前系统中活跃（未提交）的事务 id 数组
- min_trx_id：表示生成 ReadView 时，当前系统中活跃的事务中最小的事务 id，也就是 m_ids 中的最小值
- max_trx_id：表示生成 ReadView 时，已经创建的最大事务 id`（事务创建时，事务 id 是自增的）`
- creator_trx_id：表示生成 ReadView 的事务的事务 id



那么在事务里的 sql 查询会和 ReadView 进行对比，来判断是否取该行的数据：

1. 如果 row 的 trx_id < min_trx_id，表示这一行数据的事务 id 比 ReadView 中活跃事务的最小 id 还要小，表示这行数据是已提交事务生成的，因此该行数据可见
2. 如果 row 的 trx_id > max_id，表示这一行数据是由将来启动的事务生成的，不可见（如果 row 的 trx_id 就是当前事务自己的 id，则可见）
3. 如果 row 的 min_id <= trx_id <= max_id，则有两种情况：
   1. 如果 trx_id 在 ReadView 的活跃事务 id 数组（m_ids）中，则表明该事务还未提交，则该行数据不可见
   2. 如果不在，则表明该事务已经提交，可见



**注意：**

- 执行 start transaction 之后，并不会立即生成事务 id，而是在该事务中，第一次修改 InnoDB 时才会为该事务生成事务 id
- MVCC 机制就是通过 ReadView 和 undo 日志进行对比，拿到当前事务可见的数据




## 了解 BufferPool 缓存机制吗？

https://blog.csdn.net/mingyuli/article/details/120347093

https://www.processon.com/view/6080de691e08534b2ef0218b

![1699193158522](imgs/1699193158522.png)

答：

Buffer Pool 本质就是数据库的一个内存组件，MySQL 的增删改查都是直接操作 BufferPool 的，一般设置 BufferPool 的大小为机器内存的 60% ，Buffer Pool 的大小在 `/etc/my.cnf` 中进行配置：

![1698594156950](imgs/1698594156950.png)



**为什么不直接更新磁盘上的数据，而是需要设置一套复杂的机制来执行 SQL 呢？**

因为针对数据库数据的读写其实是随机的读写，而对于日志文件的读写是顺序的读写，而顺序读写和随机读写速度差距在 2~3 个数量级，磁盘的顺序 IO 几乎可以和操作内存相媲美。

通过 BufferPool 可以保证每个更新请求都是更新内存 BufferPool，然后顺序写日志文件，同时可以保证各种异常情况下的数据一致性，正是通过这套配置，才能让我们的 MySQL 数据库在较高配置的机器上每秒可以抗下几千的读写请求



**为什么数据库数据的读写是随机 IO 呢？**

因为数据库存储在磁盘中的数据是会被删除的，我们在写的时候就算一直顺序写，但是如果后边删除了中间的一些数据，那么在之后读就不能顺序读了，因为中间有一些数据已经不存在了



**InnoDB SQL 执行流程：**

1. 加载数据页，把需要修改数据所在的数据页，缓存到 BufferPool，`BufferPool 中缓存的其实就是一个个的数据页`

2. 修改前记录，写 undo 日志，记录更改前数据，如果事务执行失败，使用 undo 日志进行数据回滚

3. 更新 Buffer Pool 中的数据

4. 准备提交事务，写  redo 日志，保存操作记录。redo 日志用来恢复 BufferPool 中的数据

5. 准备提交事务，写 bin-log 日志，保存操作记录。bin-log 日志用来恢复磁盘数据

6. 事务提交完成，此时 bin-log 日志写入成功，并在 redo 日志中记录 commit 标记

   redo 日志作用：`恢复 BufferPool 中的数据`，bin-log 日志是用于恢复磁盘中的数据，其中 redo 日志和 undo 日志是 InnoDB 引擎特有的，而 bin-log 是属于 Server 层的，与引擎无关

   在 redo 日志中记录 commit 标记是为了保证事务提交之后，redo 与 binlog 数据一致，那么想一下如果 BufferPool 缓存中数据更新完毕，但是在数据库将修改后的数据刷到磁盘之前，数据库宕机了，会不会造成 BufferPool 和磁盘的数据不一致呢？

   其实不会，因此当数据库宕机恢复之后，会使用 redo 日志中的数据恢复 BufferPool 中的数据，那么 BufferPool 的数据就是更新后的数据了，等待刷回磁盘即可

7. 数据持久化，IO 线程不定期把 Buffer Pool 中的数据随机写入到磁盘，完成持久化





**MySQL 的预读机制：**

当从磁盘上加载一个数据页时，MySQL 可能会连带着把这个数据页相邻的其他数据页也加载到缓存里去。



**触发 MySQL 的预读机制的场景？**

1. 线性预读：参数 `innodb_read_ahead_threshold` 默认值是 56，表示如果顺序的访问了一个区里的多个数据页，访问的数据页的数量超过了这个阈值，就会触发预读机制，把下一个相邻区中的所有数据页都加载到缓存里去

   查看默认值：`show variables like 'innodb_read_ahead_threshold'`

   ![1698630976445](imgs/1698630976445.png)

2. 随机预读：如果 Buffer Pool 里缓存了一个区里的 13 个连续的数据页，而且这些数据页都是比较频繁会被访问的，此时就会直接触发预读机制，把这个区里的其他的数据页都加载到缓存里去。`性能不稳定，在 5.5 中已经被废弃，默认是 OFF`

   `show variables like 'innodb_random_read_ahead'`

   ![1698630938748](imgs/1698630938748.png)

   ​

**LRU 优化——冷热分离**

![1698639424959](imgs/1698639424959.png)

MySQL 通过使用 LRU 来判断哪些缓存页经常访问，哪些缓存页不常访问，来判断当 BufferPool 缓存被占满之后去淘汰哪些缓存页。

在 MySQL 的 LRU 链表中，采取了 `冷热数据分离的思想` ，LRU 链表被拆为了两部分，一部分是热数据，一部分是冷数据，冷数据默认占比 37%，由 `innodb_old_blocks_pct` 参数控制

> 查看参数：`show variables like 'innodb_old_blocks_pct'`，默认是37





`原理：数据页第一次被加载到缓存页之后，这个缓存页被放在 LRU 链表的冷数据区域的头部，在 1s（可配置） 之后，如果这个缓存页再次配访问，该缓存页才会被移动到热数据区域的头部。`

> 查看参数：`show variables like 'innodb_old_blocks_time'` ，默认是 1000 毫秒（配置多长时间之后访问该缓存页，才将该缓存页加入热数据区域头部）



**为什么 LRU 要进行冷热分离？**

如果不这样优化，在 LRU 只使用一个链表，那么在预读机制中多加载的一些缓存页，可能就在刚加载进缓存时使用一下，之后就不再使用了，如果被放在 LRU 链表头部了，会将频繁访问的缓存页挤在 LRU 链表尾部，最后被淘汰。预读机制和全表扫描加载进来的一大堆缓存页，此时都在冷数据区域里，跟热数据区域里的频繁访问的缓存页时没有关系的。



**LRU 中热数据区域访问的一些优化：**

一般在热数据区域头部的缓存页可能是经常被访问的，所以频繁移动性能不太好，所以 MySQL 对于热数据区域的访问优化了一下，只有在热数据区域的后 3/4 部分的缓存页被访问了，才会被移动到链表头部去（这样就

不会出现链表头部数据频繁交替访问，导致频繁移动链表头部数据）。





**什么时间将缓存页刷入磁盘呢？**

会有一个后台线程运行定时任务，定时将 LRU 链表的冷数据区域尾部的一些缓存页刷入磁盘里去，清空这几个缓存页，将他们加入到 free 链表中（free 链表存放的就是 BufferPool 中的空缓存页的地址）

并且这个后台线程也会在 MySQL 空闲时，将 flush 链表（flush 链表存放的是 BufferPool 中被修改过的缓存页，也称为脏页，脏页都是需要刷回磁盘的）中的缓存页都刷入磁盘中




# SQL语句

https://javaguide.cn/database/sql/sql-questions-01.html#%E6%A3%80%E7%B4%A2%E9%A1%BE%E5%AE%A2%E5%90%8D%E7%A7%B0%E5%B9%B6%E4%B8%94%E6%8E%92%E5%BA%8F










# 锁

> 1. 什么是滑块锁?





> 2. synchronized 锁升级？





> 3. 公平与不公锁的区别







> 4. 介绍一下volatile？







> 5. 比较一下悲观锁和乐观锁？乐观锁的使用场景







> 6. 分布式锁

1、分布式锁介绍：[https://javaguide.cn/distributed-system/distribute...](https://javaguide.cn/distributed-system/distributed-lock.html)
2、分布式锁常见实现方案总结：[https://javaguide.cn/distributed-system/distribute...](https://javaguide.cn/distributed-system/distributed-lock-implementations.html)





> 7. 说一下synchronized的实现原理？












# 消息队列

> 1. 除了使用 mq 解耦发奖流程外，有没有别的解决方案？

   答：异步MQ的方案非常成熟，也没有见到其他更合适的方案（如果面试官问了，可以反问他）

   这里mq的用途可以看作是中转分流的任务。中转分流不止MQ可以做，网关也是在中转分流，只不过是没有存储功能。

   无非就是你设计一个能分发任务的路由层，保证任务分发前别丢了，就行。这就是思想，出题人要的是这个。然后再谈具体实现，这又是基本功了。



> 2. RabbitMQ 的消息可靠性



> 3. RabbitMQ 的消息幂等性



> 4. 为什么用 MQ 不用 HTTP？

削峰、解耦、异步





> 5. rocketmq怎么保证消费顺序性



> 6. 怎么解决重复消费





> 7. mq 能解决什么问题







# Netty

参考文章：

https://blog.csdn.net/crazymakercircle/article/details/124588880

https://www.yuque.com/snailclimb/mf2z3k/wlr1b0

https://blog.csdn.net/zhiyikeji/article/details/131131367

《Netty原理解析与开发实战》



## Netty 是什么呢？Netty 用于做什么呢？

答：

Netty 是一个 NIO 客户服务端框架，可以`快速开发网络应用程序`，如协议服务端和客户端，极大简化了网络编程，如 TCP 和 UDP 套接字服务（来自官网） 

热门开源项目如 Dubbo、RocketMQ 底层都是用了 Netty





## Netty怎么实现高性能设计？

答：

Netty 高性能的三个方面：

1. 传输：用什么样的通道将数据发送给对方，BIO、NIO 或者 AIO，IO 模型在很大程度上决定了框架的性能。`IO模型的选择`
2. 协议：采用什么样的通信协议，HTTP 或者内部私有协议。协议的选择不同，性能模型也不同。相比于公有协议，内部私有协议的性能通常可以被设计的更优。`协议的选择`
3. 线程：数据报如何读取？读取之后的编解码在哪个线程进行，编解码后的消息如何派发，Reactor 线程模型的不同，对性能的影响也非常大。`线程模型的选择`






## 介绍一下 AIO、BIO 和 NIO？

答：

- **AIO：**

从 Java.1.7 开始，Java 提供了 AIO（异步IO），Java 的 AIO 也被称为 “NIO.2”

Java AIO 采用`订阅-通知`模式，应用程序向操作系统注册 IO 监听，之后继续做自己的事情，当操作系统发生 IO 事件并且已经准备好数据时，主动通知应用程序，应用程序再进行相关处理

（Linux 平台没有这种异步 IO 技术，而是使用 epoll 对异步 IO 进行模拟）



- **BIO：**

BIO 即同步阻塞 IO，服务端实现模式为一个连接对应一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理

`BIO简单工作流程：`

1. 服务器端启动一个 ServerSocket
2. 客户端启动 Socket 对服务器进行通信，默认情况下服务器端需要对每个客户端建立一个线程与之通讯
3. 客户端发出请求后, 先咨询服务器是否有线程响应，如果没有则会等待，或者被拒绝
4. 如果有响应，客户端线程会等待请求结束后，再继续执行



`BIO存在问题：`

1. 当并发量较大时，需要创建大量线程来处理连接，比较占用系统资源
2. 连接建立之后，如果当前线程暂时没有数据可读，则线程会阻塞在 Read 操作上，造成线程资源浪费



- **NIO：**

从 Java1.4 开始，Java 提供了 NIO，NIO 即 “Non-blocking IO”（同步非阻塞IO）

NIO 的几个核心概念：

1. Channel、Buffer：BIO是基于字节流或者字符流的进行操作，而NIO 是基于`缓冲区`和`通道`进行操作的，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中

2. Selector：选择器用于监听多个通道的事件（如，连接打开，数据到达），因此，单个线程可以监听多个数据通道，极大提升了单机的并发能力

   当 Channel 上的 IO 事件未到达时，线程会在 select 方法被挂起，让出 CPU 资源，直到监听到 Channel 有 IO 事件发生，才会进行相应的处理



- **NIO和BIO有什么区别？**

1. NIO是以`块`的方式处理数据，BIO是以`字节流或者字符流`的形式去处理数据。 
2. NIO是通过`缓存区和通道`的方式处理数据，BIO是通过`InputStream和OutputStream流`的方式处理数据。 
3. NIO的通道是双向的，BIO流的方向只能是单向的。
4. NIO采用的多路复用的同步非阻塞IO模型，BIO采用的是普通的同步阻塞IO模型。
5. NIO的效率比BIO要高，NIO适用于网络IO，BIO适用于文件IO。



**NIO如何实现了同步非阻塞？**

通过 Selector 和 Channel 来进行实现，一个线程使用一个 Selector 监听多个 Channel 上的 IO 事件，通过配置监听的通道Channel为非阻塞，那么当Channel上的IO事件还未到达时，线程会在select方法被挂起，让出CPU资源。直到监听到Channel有IO事件发生时，才会进行相应的响应和处理。







## 介绍一下 Netty 使用的线程模型？

答：

Netty 主要基于主从 Reactor 多线程模型，其中主从 Reactor 多线程模型将 Reactor 分为两部分：

- mainReactor：监听 Server Socket，用来处理网络 IO 连接建立操作，将建立的 SocketChannel 指定注册给 subReactor
- subReactor：和建立起来的 socket 做数据交互和业务处理操作



因为客户端的连接数量相对来说比较少，而数据的读和写会比较多一点，使用 mainReactor 只接受客户端连接，由其他线程 subReactor 负责读和写，将业务处理剥离出，让线程池来处理，降低了 Reactor 的性能开销



**扩展：单 Reactor 单线程模型、单 Reactor 多线程模型**

- 单 Reactor 单线程模型

  通过 1 个线程负责客户端连接、网络数据的读写、业务处理

  `缓存 Redis 就是单 Reactor 单线程模型`

- 单 Reactor 多线程模型

  通过 1 个线程负责客户端的连接、网络数据的读写，将业务处理剥离出去，通过线程池来进行处理



**三种 Reactor 模型的优缺点：**

- 单 Reactor 单线程模型是单线程进行业务处理，当负载过重时，处理速度将会变慢，影响系统性能，因此引出单 Reactor 多线程模型
- 单 Reactor 多线程模型时多个线程处理业务，业务处理速度上来了，但是单 Reactor 承担了所有时间的监听和响应，可能存在性能问题。当有数百万客户端进行连接或者服务端需要对客户端握手进行安全认证，认证本身非常消耗性能，因此出现了主从 Reactor 多线程模型
- 主从 Reactor 多线程模型中 1 个主 Reactor 只用来处理网络 IO 的连接建立操作，而对于接入认证、IP 黑白名单过滤、握手等操作由从 Reactor 进行处理，这样进一步提升性能，在主从 Reactor 多线程模型中，从 Reactor 有多个，可以与 CPU 个数相同








## TCP 粘包、拆包是什么？如何解决？

答：

TCP本身的机制决定了一定会有粘包、拆包，因为 TCP 传输协议时基于数据流传输的，而流化的数据没有界限，因此 TCP 作为传输层协议并不了解上层业务数据的具体含义，会根据 TCP 缓冲区的实际情况进行数据包的划分，所以业务上认为的一个完整的包，可能被 TCP 拆成多个包或者把多个小的包封装成一个大的包进行发送。



**产生原因：**

- 粘包：客户端发送的包的大小比socket的缓存小或者接收方读取socket缓存不及时，因此多个包一起发送了
- 拆包：客户端发送的包的大小比socket的缓存大或者发送的数据大于协议的MTU（最大传输单元）必须拆包，那么这个包就被拆分成了多个包进行发送



**解决方法：**

有三种方式：

- 通过指定分隔符来进行分割
- 通过指定固定长度来进行分割
- 上边两种方式灵活性不好，因此常用的是通过指定接收数据的长度来解决，也就是`LengthFieldBasedFrameDecoder()`这个类








## Netty 中常用组件?

答：

- `Channel`：Netty 网络操作抽象类，包括了基本的 IO 操作，如 bind、connect、read、write 等等
- `EventLoop`：主要是配合 Channel 处理 IO 操作，用来处理连接的生命周期中所发生的事件
- `ChannelFuture`：Netty 中的所有 IO 操作都是异步的，我们通过 ChannelFuture 的 addListener() 注册一个 ChannelFutureListener 监听事件，当操作执行完毕后，监听就会返回结果
- `ChannelHandler`：作为处理器，用于处理入站和出战的数据
- `ChannelHandlerContext`：用于包裹 ChannelHandler，维护了 pipeline 这个双向链表中的 pre 和 next 指针，这样可以方便的找到与其相邻的 ChannelHandler，并且过滤出一些符合执行条件的 ChannelHandler，Netty 的异步事件在 pipeline 中传播就是依靠 ChannelHandlerContext
- `ChannelPipeline`：每一个 Channel 都会分配一个 ChannelPipeline，pipeline 是一个双向链表的结构，Netty 中产生的 IO 异步事件都会在这个 pipeline 中传播








## Netty 如何发送消息？

答：

- 有两种发送消息的方式：
  - 直接写入 Channel，消息从 ChannelPipeline 的尾节点开始向前传播至头节点，代码 `channelHandlerContext.channel().write()` 
  - 使 write 事件从当前 ChannelHandler 开始沿着 pipeline 向前传播，代码`channelHandlerContext.write()`



**这里解释一下，上边发送消息为什么是向前传播：**

在 Netty 中，IO 异步事件基本上分为两类：inbound（入站） 事件、outbound（出站） 事件，那么入站事件是沿着 pipeline 的头结点一直向后传播，因此出站事件就是沿着 pipeline 的尾结点一直向前传播，而上边发送消息也就是出站事件，因此是沿着 pipeline `从后向前`进行传播 







## 直接内存比堆内存快在了哪里？

答：

首先直接内存不是 Java 虚拟机中的内存，是直接向系统内存申请的空间，来源于 NIO，通过 Java 堆中的 DirectByteBuffer 来进行操作。

直接内存相比于堆内存，避免了数据的二次拷贝。

- 我们先来分析`不使用直接内存`的情况，我们发送数据需要将数据先写入 Socket 的缓冲区内，那么如果数据存储在 JVM 的堆内存中的话，会先将堆内存中的数据复制一份到直接内存中，再将直接内存中的数据写入到 Socket 缓冲区中，之后进行数据的发送 

  - **`为什么不能直接将 JVM 堆内存中的数据写入 Socket 缓冲区中呢？`**

    在 JVM 堆内存中有 GC 机制，GC 后可能会导致堆内存中数据位置发生变化，那么如果直接将 JVM 堆内存中的数据写入 Socket 缓冲区中，如果写入过程中发生 GC，导致我们需要写入的数据位置发生变化，就会将错误的数据写入 Socket 缓冲区

- 那么如果使用直接内存的时候，我们将`数据直接存放在直接内存中`，在堆内存中只存放了对直接内存中数据的引用，这样在发送数据时，直接将数据从直接内存取出，放入 Socket 缓冲区中即可，`减少了一次堆内存到直接内存的拷贝`  

  ![1697978301568](imgs/1697978301568.png)










## 什么是 Netty 的零拷贝？什么是 TCP 缓冲区？

答：

Netty 的零拷贝主要包含三个方面：

- Netty 的接收和发送使用堆外（直接内存）进行 Socket 读写，不需要进行字节缓冲区的二次拷贝。 

- Netty 提供 CompositeByteBuf 组合缓冲区类，可以将多个 ByteBuf合并为一个逻辑上的ByteBufer，避免了各个ByteBufer之间的拷贝，将几个小buffer合并成一个大buffer的繁琐操作。

- Netty 的文件传输使用了 `FileChannel` 的 `transferTo` 方法，该方法底层使用了 `sendfile` 函数实现了 cpu 零拷贝。

  `sendfile` 函数通过网络发送数据的流程为：

  磁盘 `----DMA拷贝---->`文件读取缓冲区 `----CPU拷贝---->` 套接字发送缓冲区(SO_SNDBUF) `----DMA拷贝---->` 网络设备缓冲区(网卡) 

  图片流程如下：（3次拷贝、2次上下文切换，这里上下文切换是在用户空间发起write操作，此时用户态切换为内核态，write调用完毕后又会从内核态切换回用户态）

  ![1697979874757](imgs/1697979874757.png)



**什么是TCP 缓冲区？**

每个 TCP 的 Socket 的内核中都有一个发送缓冲区（SO_SNDBUF）和一个接收缓冲区（SO_RECVBUF）`（在通过 TCP 需要进行网络数据传输时，数据都是会写入 Socket 的缓冲区中的，之后再通过网络协议发送出去）` 









## 了解 Netty 中的 ByteBuf 类吗？

答：

在 Java NIO 编程中，Java 提供了 ByteBuffer 作为字节缓冲区类型（缓冲区可以理解为一段内存区域），来表示一个连续的字节序列。

Netty 中并没有使用 Java 的 ByteBuffer，而是使用了新的缓冲类型 ByteBuf，特性如下：

- 允许自定义缓冲类型

- 复合缓冲类型中内置的透明的零拷贝实现

- 开箱即用的动态缓冲类型，具有像 StringBuffer 一样的动态缓冲能力

- 不再需要调用 flip() 方法

  Java 的 ByteBuffer 类中，需要使用 flip() 来进行读写两种模式的切换

- 正常情况下具有比 ByteBuffer 更快的响应速度



**Java 中的 ByteBuffer：**

主要需要注意有 3 个属性：position、limit、capacity

- capacity：当前数组的容量大小
- position：写入模式的可写入数据的下标，读取模式的可读取数据下标
- limit：写入模式的可写入数组大小，读取模式的最多可以读取数据的下标

假如说数组容量是 10，那么三个值初始值为：

```
position = 0
limit = 10
capacity = 10
```

假如写入 4 个字节的数据，此时三个值如下：

```
position = 4
limit = 10
capacity = 10
```

如果切换到读取数据模式（使用 `flip()`），会改变上边的三个值，会从 position 的位置开始读取数据到 limit 的位置

```
position = 0
limit = 4
capacity = 10
```



**Netty 中的 ByteBuf：**

ByteBuf 主要使用两个指针来完成缓冲区的读写操作，分别是： `readIndex` 和 `writeIndex`

- 当写入数据时，writeIndex 会增加
- 当读取数据时，readIndex 会增加，但不会超过 writeIndex



ByteBuf 的使用：

```java
public static void main(String[] args) {
    ByteBuf buffer = Unpooled.buffer(10);
    System.out.println("----------初始化ByteBuf----------");
    printByteBuffer(buffer);

    System.out.println("----------ByteBuf写入数据----------");
    String str = "hello world!";
    buffer.writeBytes(str.getBytes());
    printByteBuffer(buffer);

    System.out.println("----------ByteBuf读取数据----------");
    while (buffer.isReadable()) {
        System.out.print((char)buffer.readByte());
    }
    System.out.println();
    printByteBuffer(buffer);


    System.out.println("----------ByteBuf释放无用空间----------");
    buffer.discardReadBytes();
    printByteBuffer(buffer);

    System.out.println("----------ByteBuf清空----------");
    buffer.clear();
    printByteBuffer(buffer);
}
private static void printByteBuffer(ByteBuf buffer) {
    System.out.println("readerIndex:" + buffer.readerIndex());
    System.out.println("writerIndex:" + buffer.writerIndex());
    System.out.println("capacity:" + buffer.capacity());
}
/**输出**/
----------初始化ByteBuf----------
readerIndex:0
writerIndex:0
capacity:10
----------ByteBuf写入数据----------
readerIndex:0
writerIndex:12
capacity:64
----------ByteBuf读取数据----------
hello world!
readerIndex:12
writerIndex:12
capacity:64
----------ByteBuf释放无用空间----------
readerIndex:0
writerIndex:0
capacity:64
----------ByteBuf清空----------
readerIndex:0
writerIndex:0
capacity:64
```



**ByteBuf 的 3 种使用模式：**

ByteBuf 共有 3 种使用模式：

- 堆缓冲区模式（Heap Buffer）

  堆缓冲区模式又称为 “支撑数据”，其数据存放在 JVM 的`堆空间`

  `优点：` 

  - 数据在 JVM 堆中存储，可以快速创建和释放，并且提供了数组直接快速访问的方法

  `缺点：` 

  - 每次数据与 IO 进行传输时，都需要将数据复制到直接缓冲区（这里为什么要将数据复制到直接缓冲区的原因在上边的 `直接内存比堆内存快在了哪里？` 问题中已经讲过） 

  `创建代码：` 

  ```java
  ByteBuf buffer = Unpooled.buffer(10);
  ```

- 直接缓冲区模式（Direct Buffer）

  直接缓冲区模式属于堆外分配的直接内存，不占用堆的容量

  `优点：` 

  - 使用 socket 传输数据时性能很好，避免了数据从 JVM 堆内存复制到直接缓冲区

  `缺点：`

  - 相比于堆缓冲区，直接缓冲区分配内存空间和释放更为昂贵

  `创建代码：`

  ```java
  ByteBuf buffer = Unpooled.directBuffer(10);
  ```

- 复合缓冲区模式（Composite Buffer）

  本质上类似于提供一个或多个 ByteBuf 的组合视图

  `优点：` 

  - 提供一种方式让使用者自由组合多个 ByteBuf，避免了复制和分配新的缓冲区

  `缺点：` 

  - 不支持访问其支撑数据，如果要访问，需要先将内容复制到堆内存，再进行访问

  `创建代码：`

  ```java
  public static void main(String[] args) {
  //        AnnotationConfigApplicationContext context = new AnnotationConfigApplicationContext(Test.class);
      // 创建一个堆缓冲区
      ByteBuf heapBuf = Unpooled.buffer(2);
      String str1 = "hi";
      heapBuf.writeBytes(str1.getBytes());
      // 创建一个直接缓冲区
      ByteBuf directBuf = Unpooled.directBuffer(5);
      String str2 = "nihao";
      directBuf.writeBytes(str2.getBytes());
      // 创建一个复合缓冲区
      CompositeByteBuf compositeByteBuf = Unpooled.compositeBuffer(10);
      compositeByteBuf.addComponents(heapBuf, directBuf);
      // 检查是否支持支撑数组，发现并不支持
      if (!compositeByteBuf.hasArray()) {
          for (ByteBuf buf : compositeByteBuf) {
              // 第一个字节偏移量
              int offset = buf.readerIndex();
              // 总共数据长度
              int length = buf.readableBytes();
              byte[] bytes = new byte[length];
              // 不支持访问支撑数组，需要将内容复制到堆内存中，即 bytes 数组中，才可以进行访问
              buf.getBytes(offset, bytes);
              printByteBuffer(bytes, offset, length);
          }
      }
  }

  private static void printByteBuffer(byte[] array, int offset, int length) {
      System.out.println("array:" + array);
      System.out.println("array->String:" + new String(array));
      System.out.println("offset:" + offset);
      System.out.println("len:" + length);
  }
  /**输出**/
  array:[B@4f8e5cde
  array->String:hi
  offset:0
  len:2
  array:[B@504bae78
  array->String:nihao
  offset:0
  len:5
  ```

  ​





## Netty 中 ByteBuf 如何分配？有池化的操作吗？

答：

ByteBuf 的分配接口定义在了 `ByteBufAllocator` 中，他的直接抽象类是 `AbstractByteBufAllocator`，而 `AbstractByteBufAllocator` 有两种实现：`PooledByteBufAllocator` 和 `UnpooledByteBufAllocator`

![1698045359020](imgs/1698045359020.png)

- PooledByteBufAllocator 提供了池化的操作，将 ByteBuf 实例放入池中，提升了性能，将内存碎片化减到了最小UnpooledByteBufAllocator。（这个实现采用了一种内存分配的高效策略，成为 jemalloc，已经被好几种现代操作系统所采用）
- UnpooledByteBufAllocator 在每次创建缓冲区时，都会返回一个新的 ByteBuf 实例，这些实例由 JVM 负责 gc 回收








## NioEventLoopGroup 默认启动了多少线程？

答：

NioEventLoopGroup 是一个多线程的事件循环器，默认启动了电脑可用线程数的两倍，在调用 NioEventLoopGroup 的构造方法之后，如果不传入线程数，最后启动的默认线程数的计算公式为：

![1698047197939](imgs/1698047197939.png)













## Netty 如何解决 Selector 空轮询 Bug 的策略

答：

空轮询 Bug 也就是网络上发生了唤醒 Selector 的事件，但是 Selector 去取事件取不到，就一直去取，发生了空轮询，导致 CPU 使用率达到 100%



**Netty解决机制：**

判断如果发生了 N（默认是512次） 次空轮询，就新建一个Selector，把原来Selector事件都迁移过来









## 为什么没有使用 Netty 5？

答：

Netty版本分别是netty3.x、netty4.x、netty5.x
Netty5出现重大bug，已经被官网废弃，目前推荐使用Netty4.x稳定版本



> 了解 Netty 序列化吗？





> 如何设计一个内存池或者内存分配器？

答：

https://www.cnblogs.com/crazymakercircle/p/16181994.html



聊聊：如何设计一个Java对象池，减少GC和内存分配消耗
重要性：对对象池透彻理解，并且具备实操能力，也是编程高手的标志之一。

对象池顾名思义就是存放对象的池，与我们常听到的线程池、数据库连接池、http连接池等一样，都是典型的池化设计思想。

对象池的优点就是可以集中管理池中对象，减少频繁创建和销毁长期使用的对象，从而提升复用性，以节约资源的消耗，可以有效避免频繁为对象分配内存和释放堆中内存，进而减轻jvm垃圾收集器的负担，避免内存抖动。

Apache Common Pool2 是Apache提供的一个通用对象池技术实现，可以方便定制化自己需要的对象池，大名鼎鼎的 Redis 客户端 Jedis 内部连接池就是基于它来实现的。

说明：此题是一个实操性质的题目，后续尼恩带大家参考netty对象池，从0到1，架构、设计、实现一个高性能对象池组件

关于对象池的知识，请参见Netty内存池（史上最全 + 5W字长文）
————————————————
版权声明：本文为CSDN博主「40岁资深老架构师尼恩」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/crazymakercircle/article/details/124588880


# JVM

> 1. 写代码时候有没有什么方式尽量减少Full GC的概率?

   答：

1. 避免一次性加载大量数据加载到内存，比如excel导入导出，jdbc数据库查询
2. 避免大对象的代码处理业务链流程过长，比如aop中获取到了对象参数，大对象捕获到了，导致对象生命周期变长了，没及时释放。
3. 禁止使用system.gc方法
4. 避免在使用threalocal后，未主动调用remove方法，尽量避免大对象的使用，以及频繁的创建和销毁。更要避免全局锁的竞争等。



> 2. 什么是gc，gc怎么排查，怎么手动让JAVA虚拟机OOM？ OOM的场景？





> 3. Full GC 耗时很长，怎么排查，怎么优化，具体到怎么写命令？







> 4. staic和final关键字结合jvm来讲





> 5. 讲讲垃圾回收和死亡对象判断方法，具体讲讲可达性分析



> 6. 发生频繁的fullgc可能由什么原因导致





> 7. CMS收集流程





> 8. 了解过 jvm 内存模型吗



> 9. 能不能说一下堆区具体怎么划分，为什么这样划分





> 10. JVM每个区具有什么功能？





> 11. GC底层算法，常见的垃圾回收器，双亲委派的类加载流程；





> 12. 标记整理法的缺点是什么？





> 13. 元空间是起到什么作用？



> 14. 自定义类加载的过程是什么？









> 15. 怎么释放一个用完的大对象的内存空间？







> 16. 对象的生命周期（new一个对象的过程）？









> 17. 什么时候会有内存泄漏，怎么排查？

答：

首先内存泄漏是堆中的一些对象不会再被使用了，但是无法被垃圾收集器回收，如果不进行处理，最终会导致抛出 `java.lang.OutOfMemoryError` 异常。

内存泄露：

- 不需要使用的对象被其他对象不正确的引用，导致无法回收。
- 对象生命周期过长



内存泄漏的8中情况：

1. 大量使用静态集合类（`HashMap、LinkedList`等），静态变量的生命周期和JVM程序一致，在程序结束之前，静态变量不会被释放，导致内存泄漏。（属于生命周期过长）

2. 单例模式的静态特性，也会导致生命周期过长，如果单例对象持有外部对象的引用，会导致外部对象不会被回收。

3. 内部类持有外部类：每个非静态内部类都会持有外部类的隐式引用，假如`a`为非静态内部类，`b`为`a`的外部类，如果`b`包含了大量对象的引用，非常占用内存空间，那么如果我们创建了非静态内部类`a`，此时即使`b`对象不再被使用了，也无法回收，占用内存空间，导致内存泄漏。

   解决办法：如果内部类不需要访问外部类的成员信息，可以考虑转换为静态内部类。

4. 各种连接（数据库连接、网络连接和IO连接）未及时关闭，导致大量对象无法回收，造成内存泄漏。

5. 变量不合理的作用域：一个变量的定义的作用范围大于其适用范围，很有可能造成内存泄露。

6. 改变哈希值：

   当一个对象被存储进HashSet集合中以后，就不能修改这个对象中的那些参与计算哈希值的字段了。

   否则，对象修改后的哈希值与最初存入HashSet集合时的哈希值就不同了，这种情况下，即使在contains方法使用该对象的当前引用作为参数去HashSet集合中检索对象，也将返回找不到对象的结果，这也会导致HashSet集合中无法单独删除当前对象，造成内存泄漏

   为什么改变哈希值之后找不到对象？因为根据存入时的哈希值去寻找放入的位置，而改变哈希值之后，再去查找就按照新的哈希值所对应的位置去查找，肯定找不到。

   这也是 String 为什么被设置成了不可变类型，我们可以放心的把 String 存入 HashSet，或者把String当作 HashMap 的 key 值。

   ```java
   public class ChangeHashCodeTest {
       public static void main(String[] args) {
           HashSet<Point> set = new HashSet<>();
           Point cc = new Point();
           cc.setX(10); // hashCode = 10
           set.add(cc);
           cc.setX(20); // hashCode = 20
           System.out.println("set remove = " + set.remove(cc));
           set.add(cc);
           System.out.println("set.size = " + set.size());
           /**
            * 输出：
            * set remove = false
            * set.size = 2
            */
       }
   }
   class Point {
       int x;
       public int getX() {
           return x;
       }
       public void setX(int x) {
           this.x = x;
       }
       @Override
       public boolean equals(Object o) {
           if (this == o) return true;
           if (o == null || getClass() != o.getClass()) return false;
           Point point = (Point) o;
           return x == point.x;
       }
       @Override 
       public int hashCode() {
           return x;
       }
   }
   ```

7. 缓存泄露：

   一旦把对象引用放入到缓存中，他就很容易遗忘。比如：之前项目在一次上线的时候，应用启动奇慢，就是因为代码会加载一个表的数据到缓存中，测试环境只有几百条数据，而生产环境有几百万的数据。

   对于这个问题，可以使用WeakHashMap代表缓存，此Map的特点是：当除了自身有key的引用外，此key没有其他引用，那么此map会自动丢弃此值。

8. ThreadLocal

   ThreadLocal的实现中，每个Thread维护一个ThreadLocalMap映射表，key是ThreadLocal实例本身，value是真正需要存储的Object。

   ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统GC时，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，就没有办法访问这些key为null的Entry的value。

   如果当前线程迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value永远无法回收，造成内存泄漏。

   如何解决此问题？

   第一，使用ThreadLocal提供的remove方法，可对当前线程中的value值进行移除；

   第二，不要使用ThreadLocal.set(null) 的方式清除value，它实际上并没有清除值，而是查找与当前线程关联的Map并将键值对分别设置为当前线程和null。

   第三，最好将ThreadLocal视为需要在finally块中关闭的资源，以确保即使在发生异常的情况下也始终关闭该资源。

   ```java
   try {
       threadLocal.set(System.nanoTime());
   } finally {
       threadLocal.remove();
   }
   ```

   ​



**排查内存泄漏：**

可以查看`泄露对象`到`GC Roots`的引用链，找到泄露对象在哪里被引用导致无法被回收





> 18. JVM常见配置



堆设置

```yaml
-Xms3550m   初始堆大小 
-Xmx3550m   最大堆大小 
-XX:NewSize=1024     设置年轻代大小 
-XX:NewRatio=4       设置年轻代和年老代的比值.如:为3,表示年轻代与年老代比值为1:3,年轻代占整个年轻代年老代和的1/4 
-XX:SurvivorRatio=8  设置年轻代中Eden区与一个Survivor区的比值，默认为8
-XX:MaxPermSize=256m 设置持久代大小
```



收集器设置

```
-XX:+UseSerialGC:设置串行收集器 
-XX:+UseParallelGC:设置并行收集器 
-XX:+UseParalledlOldGC:设置并行年老代收集器 
-XX:+UseConcMarkSweepGC:设置并发收集器
```



垃圾回收统计信息

```
-XX:+PrintGC 
-XX:+PrintGCDetails 
-XX:+PrintGCTimeStamps 
-Xloggc:filename
```



并行收集器设置

```
-XX:ParallelGCThreads=n:设置并行收集器收集时使用的CPU数.并行收集线程数. 
-XX:MaxGCPauseMillis=n:设置并行收集最大暂停时间 
-XX:GCTimeRatio=n:设置垃圾回收时间占程序运行时间的百分比.公式为1/(1+n)
```



并发收集器设置

```
-XX:+CMSIncrementalMode:设置为增量模式.适用于单CPU情况. 
-XX:ParallelGCThreads=n:设置并发收集器年轻代收集方式为并行收集时,使用的CPU数.并行收集线程数.
```



> 19. JVM的堆配置过大的副作用有哪些?



答：JVM的堆内存配置过大，可能要面临的问题有：

- 回收大块堆内存而导致的长时间的时间停顿。
- 如果因为程序设计失误，将大对象从磁盘读取到内存中，可能会导致大对象在分配时直接进入老年代，没有在 Minor GC 中被清理掉。这样会导致频繁的发生 Full GC，给用户的体验是程序每个几分钟就停顿十几秒，非常卡顿。

**扩展：JVM的堆配置过小的副作用有哪些？**

- Minor GC 过于频繁




> 20. 如果出现堆内存溢出 `java.lang.OutOfMemoryError  Java heap space`，该如何解决？

答：解决思路如下：

- 首先需要拿到`堆转储快照`进行分析，查看导致 OOM 的对象是否有必要存在，并且分析清除是因为哪些对象导致了 OOM
- 如果是内存泄漏导致 OOM，可以查看`泄露对象`到`GC Roots`的引用链，找到泄露对象在哪里被引用导致无法被回收
- 如果不是内存泄漏，那么说明内存中的对象都是存活的，导致 OOM，这时应该检查虚拟机的堆内存设置是否有向上调整的空间。并且检查是否存在`对象生命周期过长`、`存储结构不合理`的情况，减少程序运行中的内存消耗。 



**扩展说明：因为存储结构不合理导致堆内存溢出（来自于《深入理解Java虚拟机第3版》）**

举例：使用 `HashMap<Long, Long>` 存储大量的数据，会导致浪费大量的空间，因为 HashMap 的空间效率使用太低。

对于一个 `HashMap<Long, Long>`来说，有效数据只有 Key、Value 的两个 long 型数据，占 16字节，long 数据被包装为 java.lang.Long 对象后，就分别具有 8 字节的 Mark Word、8字节的 Klass 指针、8字节的 long 型数值。两个 Long 对象组成 Map.Entry 之后，又多了 16 字节的对象头、8字节的 next 字段、4字节的 int 型的 hash 字段、4字节的空白填充（为了对齐）还有 HashMap 中对这个 Entry 的 8 字节的引用，这样实际占用的内存为：`(Long(24byte) * 2) + Entry(32byte) + HashMap Ref(8byte)=88byte`，空间效率仅仅为  16byte / 88byte = 18%。




# Redis

Redis 常见面试题：https://javaguide.cn/database/redis/redis-questions-02.html

https://blog.csdn.net/qq_53868937/article/details/130806921





## Redisson 分布式锁？在项目中哪里使用？多久会进行释放？如何加强一个分布式锁？

答：

首先入门级别的分布式锁是通过 `setnx` 进行实现，使用 `setnx` 实现有四个注意点

1. 需要设置锁的超时时间（如果不设置，在释放锁时，如果机器宕机，会导致锁无法释放）

2. 需要设置一个唯一 ID，表示这个锁是哪个用户添加的，必须由添加锁的用户释放

   （如果不设置，线程1在执行任务时，可能锁的超时时间已经达到，被自动释放，此时线程2加锁，开始执行业务，但正好线程1执行完毕，释放锁，由于没有唯一ID表示，线程1将线程2加的锁给释放掉了）

3. 需要`锁续命` 

   有可能锁的过期时间设置的太短，导致业务没有执行完毕，锁就被自动释放，因此要使用锁续命来解决（大概逻辑是使用子线程执行定时任务，定时任务间隔时间要小于 key 的过期时间，子线程隔一段时间判断主线程是否在执行，如果在执行，就重新设置一下过期时间）

4. 可重入问题：setnx 实现的分布式锁不可重入，这样获取锁的线程在重复进入相同锁的代码块中会造成死锁



而在 Redission 中已经帮我们实现好了分布式锁，下来看一下 Redission 中的分布式锁：



**Redission 中获取锁逻辑：**

在 Redission 中加锁，通过一系列调用会到达下边这个方法

他的可重入锁的原理也就是使用 hash 结构来存储锁，key 表示锁是否存在，如果已经存在，表示需要重复访问同一把锁，会将 value + 1，即每次重入一次 value 就加 1，退出一次 value 就减 1

下列方法有三个参数分别为：

- KEYS[1] ： 锁名称
- ARGV[1]： 锁失效时间
- ARGV[2]： id + “:” + threadId; 锁的小key

```java
    <T> RFuture<T> tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand<T> command) {
        internalLockLeaseTime = unit.toMillis(leaseTime);

        return commandExecutor.evalWriteAsync(getName(), LongCodec.INSTANCE, command,
                  "if (redis.call('exists', KEYS[1]) == 0) then " +
                      "redis.call('hset', KEYS[1], ARGV[2], 1); " +
                      "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                      "return nil; " +
                  "end; " +
                  "if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then " +
                      "redis.call('hincrby', KEYS[1], ARGV[2], 1); " +
                      "redis.call('pexpire', KEYS[1], ARGV[1]); " +
                      "return nil; " +
                  "end; " +
                  "return redis.call('pttl', KEYS[1]);",
                    Collections.<Object>singletonList(getName()), internalLockLeaseTime, getLockName(threadId));
    }
```





**Redission 中锁续命原理：**

Redission 底层有个看门狗机制，加锁成功后会有一个定时任务，默认锁的失效时间是 30s，该定时任务每隔锁失效时间的 1/3 就会去续约锁时间，也就是每隔 10s 进行锁续命



**如何加强一个分布式锁？**

也就是如何提升一个分布式锁的性能，分布式锁本质上是将并行操作改为串行，那么我们可以通过使用`分段锁`来提升性能，比如说有 1000 个库存的话，读入到缓存中将分为 10 份进行存储，即 `product_stock_1 = 100, product_stock_2 = 100, ...`，给每一份都加上所，那么多个线程来竞争这 10 把锁，比原来竞争 1 把锁的性能提高 10 倍









## zset 的底层实现？为什么不用红黑树？

答：

zset 的底层实现是：`压缩列表 + 跳表`



**什么时候使用压缩列表？**

- 有序集合保存的元素个数要小于 128 个；
- 有序集合保存的所有元素成员的长度都必须小于 64 字节。

否则使用跳表



跳表中每个节点都有多个跳跃指针，因此每个节点的平均跳跃长度较长，可以一次跳过多个节点，当找到大于或等于目标元素的节点后，再使用普通指针开始移动（可以向后移动，也可以向前移动，跳表含有前边节点的指针）寻找目标元素，跳表可以在 `O(logn)` 的时间内遍历跳表



**跳表结构图：**

![1697874023019](imgs/1697874023019.png)





**为什么不用红黑树？**

- 跳表和红黑树的查找时间复杂度都是`O(logn)`，但是红黑树比跳表的插入/删除效率更低
  - 跳表在插入或删除时，只需考虑相邻节点，而红黑树需考虑节点的旋转问题，焦虑较低
- 跳表实现比红黑树更简单






## zset 几个命令的时间复杂度？ 

答：

- `zadd`：O(logn)，添加一个元素的时间复杂度是 O(logn)（因为插入元素的话，时间开销都在查找插入位置上，在 zset 中，查找时间复杂度是 O(logn)，因此插入复杂度同是）
- `zrange`：O(logn + m) ，n 是集合中元素数量，m 是指定范围内的用户数量








## redis 里面的命令，比如 setnx 和 setex 还有 zset 中的命令？

答：

zset 中的常用命令为：

- `zadd <key> <score1> <value1> <score2> <value2> ...` 

  向集合 key 中添加元素

- `zrange <key> <start> <stop> [withscores]`

  查找下标在 start 和 stop 之间的元素，如果后边带上 `withscores` 参数，会将分数也查询出来

- `zrevrange <key> <start> <stop> [withscores]` 

  将分数从大到小进行查询，和 zrange 查询顺序相反

- `zrangebyscore <key> <min> <max> [withscores]`

  返回集合 key 中所有 score 介于 min 和 max 之间的成员，如果后边带上 `withscores` 参数，会将分数也查询出来

- `setex <key> <seconds> <value>`

  设置 key、value 并且设置过期时间

- `setnx <key> <value>`

  仅当 key 不存在时，才将 key 的值设置为 value，成功返回1，失败返回0

  ​








## 缓存怎么保证数据的一致性？

答：

那么保证数据一致性就需要保证数据库和缓存同时进行更新，那么就分为两种情况先更新数据库还是先更新缓存，由于更新缓存成本比较高（因为写入数据库的值有时候不是直接写入缓存而是经过一系列计算之后才写入缓存，因此当数据修改时不更新缓存，直接将缓存删除），那么就分为了 `先删除缓存再更新数据库` 和 `先更新数据库，再删除缓存` 两种情况：

- `先删除缓存，再更新数据库`  （操作简单）

  这种情况造成的缓存不一致为：线程 A 先删除缓存，再去更新数据库，在线程 A 更新数据库之前，如果线程 B 去读取缓存，发现并不存在，去读取数据库，此时读取的是旧数据，再将旧数据写入缓存，此时缓存存储的就是脏数据了。

  使用`更新数据库 + 延时双删`可以解决此情况的数据不一致，在延时双删中，会删除两次缓存，分为以下几步：

  ```bash
  1. 删除缓存
  2. 更新数据库
  3. 睡眠  Thread.sleep()
  4. 再删除缓存
  ```

  即延时双删在线程 A 更新完数据库之后，休眠一段时间，再去删除缓存中可能存在的脏数据。

  这样第二次删除缓存可能导致线程 A 执行时间过长，第二次可以使用`异步`去删除缓存 



- `先更新数据库，再删除缓存` 

  这种情况可能因为线程 A 没有及时删除缓存或者删除缓存失败而导致线程 B 读取到旧数据

  当缓存删除失败时，可以使用消息队列来重试：

  1. 先将要删除的缓存值或者是要更新的数据库值暂存到消息队列中
  2. 当程序没有成功删除缓存值或者更新数据库值时，从消息队列中读取这些值，再次进行删除或更新
  3. 如果成功删除缓存或者更新数据库，要将这些值从消息队列中取出，以免重复操作

  ​

**高级扩展点**：`Canal 监听日志更新 + 定时任务缓存处理`

如果在系统中引入 MQ 会导致系统耦合度过高，可以通过 Canal 监控 MySQL 的 binlog，当发现数据库的数据发生变化后，就去同步缓存，就可以达到最终的数据一致性了



**分布式环境下，本地缓存保证数据一致性：**

`使用 MQ 来保证`

分布式环境下，由于本地缓存只在本应用内部有效，当其中一台应用的本地缓存更新之后，我们需要保证其他应用的本地缓存也同步进行更新，因此需要通过 MQ 来保证最终数据一致性

更新流程为：

当应用 1 收到请求更新数据库，同时应用 1 更新本地缓存，并且发送更新 MQ 广播消息，让其他的应用也更新本地缓存，达到数据一致性











## 什么时候做缓存的清理？

   答：设置5~10分钟的过期时间







## 了解 redis 中的大key吗？多大算是大key呢？如何解决？

答：

redis 的大 key 指的是 key 对应的 value 所占用的内存比较大。

对于 string 类型来说，一般情况下超过 10KB 则认为是大 key；对于set、zset、hash 等类型来说，一般数据超过5000条即认为是大 key

bigkey 除了会消耗更多的内存空间和带宽，还会对性能造成比较大的影响。应该避免系统中出现 bigkey。

**查找大key：**

1. 使用命令 `redis-cli -p 6379 --bigkeys -i 3` 在 redis 中查找大 key，通过 `-i` 控制扫描频率，表示扫描过程中每次扫描后休息的时间间隔为 3 秒。
2. 分析 RDB 文件来找出大 key。



**解决大key：**

1. 将大key切割为多个小key。（不推荐，这样需要修改业务代码）
2. 如果大key不是热点key，手动删除。redis4.0+ 使用 `unlink` 命令异步删除传入的key






## 了解 redis 中的热 key 吗？

答：

热 key 指的是在 redis 接收的读写请求中，某个 key 就占了一般甚至更多的请求，称为热 key。

处理热 key 会占用大量的 CPU 和带宽，如果在某一时间内大量访问热key，请求数量超过 redis 的处理能力或者热key正好在缓存中过期，就会导致线上服务崩溃。

需要对系统中的热 key 进行优化，确保系统的高可用和稳定性。



**查找热key：**

1. 使用 `redis-cli --hotkeys` 来查找热key，使用该命令时，需要设置 `maxmemory-policy` 为 LRU 算法，否则会报错。

2. 使用 `monitor` 命令来实时查看 redis 实例的操作情况，包括读写删除等操作。

   该命令对 redis 性能影响较大，谨慎使用。

   使用 `monitor` 命令并输出重定向至文件，在关闭 `monitor` 命令后对文件进行分析，即可找到这段时间内的热 key。



**解决热key：**

1. 使用二级缓存。将热 key 存储一份到 JVM 本地内存中（可以使用Caffeine）
2. 使用 redis 集群：在多个 redis 实例上都存储一份热 key，分散热 key 的访问请求
3. 读写分离：主节点处理写请求，从节点处理读请求














## redis用的什么版本，6.0多线程体现在哪里？redis的性能瓶颈在哪里？

答：自己项目中的 redis 使用的 6.2.6版本。redis的性能瓶颈在于内存、网络IO处理速度。

redis6.0 引入了多线程，引入多线程的目的是为了提高`网络IO`的处理性能.

redis 的多线程只是用来处理网络请求，对于读写命令，还是使用单线程进行处理，因此并不存在线程安全的问题。



6.0多线程默认关闭，在 redis.conf 中设置 `io-threads-do-reads yes` 进行开启，并且设置线程个数`io-threads  6` 

**主线程与IO线程的协作流程：**

1. 服务端和客户端建立Socket连接，并分配处理线程

   主线程负责接收建立连接请求。当有客户端请求和实例建立Socket连接时，主线程会创建和客户端的连接，并把 Socket 放入全局等待队列中。 紧接着，主线程通过轮询方法把Socket连接分配给IO线程。

2. IO线程读取并解析请求

   主线程一旦把Socket分配给IO线程，就会进入阻塞状态，等待IO线程完成客户端请求读取和解析。因为有多个IO线程在并行处理，所以，这个过程很快就可以完成。

3. 主线程执行请求操作

   等到IO线程解析完请求，主线程还是会以单线程的方式执行这些命令操作

   ![1697527337506](imgs/1697527337506.png)












## redis基本数据结构？

答：

有 5 中基本数据结构：字符串、list列表、hash字典、set集合、zset有序集合



**基本数据结构添加数据命令：**

- 字符串： `set key value [ex seconds | px milliseconds] [nx | xx]`

  nx：指定 key 不存在才会设置成功

  xx：指定的 key 必须存在才会设置成功，用于更新 key

- list：`lpush key value [value...]`  `rpush key value [value...]`

- hash： `hset key field value` 将 key 中的 field 的值设置为 value

- set： `sadd key value [value...]`

- zset：`zadd key score value` 向 key 中添加一个 value 和 score，根据 score 排序

**数据结构基本介绍：**

- list 列表是链表，不是数据
- set 集合内部的键值对是无序且唯一的



**基本数据结构应用场景：**

- 字符串

  - 限速器：防止 DoS 攻击，对 ip 进行访问次数限制，但是无法防止 DDoS 攻击，因为 DDoS 是分布式拒绝服务，使用了不同 ip 不断访问服务器

    ```java
    // 等价于 set 192.168.55.1 ex 60 nx
    // 如果该ip不存在，指定key为ip，value为1，过期时间为60秒
    Boolean exists = redis.set(ip, 1, "ex 60", "nx");
    if(exists != null || redis.incr(ip) <= 5) {
      // 通过访问
    } else {
      // 限流
    }
    ```

- list

  - 栈  `lpush + lpop` 实现
  - 列表  `rpush + lpop`  或 `lpush + rpop` 实现
  - 阻塞式消息队列 `lpush + brpop` 实现

- hash

  - 存储对象数据：key 为对象名称，value 为描述对象属性的 Map，对象属性的修改在 Redis 中就可直接完成

- set

  - 去重操作

- zset

  - 用户排行榜
  - 用户点赞统计








> 17. redis数据结构 sds优化









> 21. Redis如何保证原子性，Lua脚本











> 23. 什么情况使用 redis 反而降低性能







## 谈谈Redis 的持久化策略？

`Redis`的确是将数据存储在内存的，但是也会有相关的持久化机制将内存持久化备份到磁盘，以便于重启时数据能够重新恢复到内存中，避免数据丢失的风险。而`Redis`持久化机制有三种：`AOF`、`RDB`、`混合型持久化（4.x版本后提供）`



- **RDB持久化**

  `关闭 RDB 持久化只需要将 save 保存策略注释掉即可`

  RDB持久化的方式有两种：

  - 手动save：阻塞当前 Redis，直到持久化完成，可能造成长时间阻塞，线上不建议使用。

    Redis 中 save 操作的配置：从右向左条件主键变弱，如果60s发生了10000次写操作，就进行持久化，如果没有达到，在300s时，如果有100次写操作就会持久化，如果没有达到在3600s，如果有一次写操作就会持久化

    ![1697512675854](imgs/1697512675854.png)

    ​

  - 手动bgsave：Redis 进程执行 `fork` 创建子进程进行持久化，阻塞事件很短。在执行`Redis-cli shutdown`关闭`Redis`服务时或执行`flushall`命令时，如果没有开启`AOF`持久化，自动执行`bgsave`

  ​

  `bgsave子进程工作原理：` 

  由子进程继承父进程所有资源，且父进程不能拒绝子进程继承，bgsave子进程先将内存中的全量数据copy到磁盘的一个`RDB临时文件`，持久化完成后将该临时文件替换原来的`dump.rdb`文件。

  如果持久化过程中出现了新的写请求，则系统会将内存中发生数据修改的物理块copy出一个副本，bgsave 子进程会把这个副本数据写入 RDB 文件，在这个过程中，主线程仍然可以直接修改原来的数据，`fork` 使用了 `写时复制技术（Copy-On-Write）`。

  ​

  `写时复制技术：` 

  目的是避免不必要的内存拷贝。

  在Linux系统中，调用 `fork` 系统调用创建子进程时，并不会把父进程所有占用的内存页复制一份，而是与父进程共用相同的**内存页**，而当子进程或者父进程对**内存页**进行修改时才会进行复制 —— 这就是著名的 `写时复制` 机制。

  那么bgsave中的写时复制技术即如果在持久化过程中，写入了新的数据，此时再去将元数据重新拷贝一份进行修改。

  ​

  `优点：` 

  - 使用单独子进程持久化，保证 redis 高性能。
  - RDB 持久化存储压缩的二进制文件，适用于备份、全量复制，可用于灾难备份，同时`RDB`文件的加载速度远超于`AOF`文件。

  `缺点：`

  - 没有实时持久化，可能造成数据丢失。
  - 备份时占用内存，因为`Redis` 在备份时会独立创建一个子进程，将数据写入到一个临时文件（需要的内存是原本的两倍）
  - `RDB`文件保存的二进制文件存在新老版本不兼容的问题。

- **AOF持久化**

  默认AOF没有开启，可在`redis.conf`中配置

  ![1697513867726](imgs/1697513867726.png)

  Redis7发生了重大变化，原来只有一个appendonly.aof文件，现在具有了三类多个文件：

  - 基本文件：RDB格式或AOF格式。存放RDB转为AOF当时内存的快照数据。该文件可以有多个。
  - 增量文件：以操作日志形式记录转为AOF后的写入操作。该文件可以有多个。
  - 清单文件：维护AOF文件的创建顺序，保证激活时的应用顺序。该文件只可以有1个。

  ​

  aof 文件中存储的 resp 协议数据格式，如果执行命令`set a hello`，aof文件内容如下：`（*3代表有3条命令，$5代表有5个字符）`

  ```bash
  *3
  $3
  set
  $1
  a
  $5
  hello
  ```

  AOF持久化时，其实是先写入缓存中，之后再同步到磁盘中，同步策略有三种：

  - `appendfsync always`：每次写入都同步到磁盘，最安全，但影响性能。
  - `appendfsync everysec`（推荐、默认配置）：每秒同步一次，最多丢失1秒的数据。
  - ``appendfsync no` ：`Redis`并不直接调用文件同步，而是交给操作系统来处理，操作系统可以根据`buffer`填充情况/通道空闲时间等择机触发同步；这是一种普通的文件操作方式。性能较好，在物理服务器故障时，数据丢失量会因`OS`配置有关。

  `优点：` 

  - 数据丢失风险较低，后台线程处理持久化，不影响客户端请求处理的线程。

  `缺点：`

  - 文件体积由于保存的是所有命令会比`RDB`大上很多，而且数据恢复时也需要重新执行指令，在重启时恢复数据的时间往往会慢很多。

  `AOF的重写（Rewrite）机制：`

  - 为了防止AOF文件太大占用大量磁盘空间，降低性能，Redis引入了Rewrite机制对AOF文件进行压缩

    Rewrite就是对AOF文件进行重写整理。当开启Rewrite，主进程redis-server创建出一个子进程bgrewriteaof，由该子进程完成rewrite过程。

    首先会对现有aof文件进行重写，将计算结果写到一个临时文件，写入完毕后，再重命名为原aof文件，进行覆盖。

  ​

    `配置AOF重写频率`

  ```bash
  # auto‐aof‐rewrite‐min‐size 64mb //aof文件至少要达到64M才会自动重写，文件太小恢复速度本来就很快，重写的意义不大
  # auto‐aof‐rewrite‐percentage 100 //aof文件自上一次重写后文件大小增长了100%则再次触发重写
  ```

  `AOF的持久化流程图：` 

  ![未命名文件](imgs/未命名文件-7514695086.png)


  

​    






- **混合持久化开启**

  默认开启，即AOF持久化的基本文件时的基本文件是RDB格式的。（必须先开启aof）

  ![1697513943813](imgs/1697513943813.png)

  **混合持久化重写aof文件流程**：aof 在重写时，不再将内存数据转为 resp 数据写入 aof 文件，而是将之前的内存数据做 RDB 快照处理，将 `RDB快照+AOF增量数据` 存在一起写入新的 AOF 文件，完成后覆盖原有的 AOF 文件。

  ​

  **Redis重启加载数据流程：**

  1. 先加载 RDB 数据到内存中
  2. 再重放增量 AOF 日志，加载 AOF 增量数据

  `优点：`

  - 结合了 RDB 和 AOF，既保证了重启 Redis 的性能，又降低数据丢失风险

  `缺点：`

  - AOF 文件中添加了 RDB 格式的内容，使得 AOF 文件的可读性变得很差； 








## Redis 的数据备份策略



1. 写crontab定时调度脚本，每小时都copy一份rdb或aof的备份到一个目录中去，仅仅保留最近48小时的备份


2. 每天都保留一份当日的数据备份到一个目录中去，可以保留最近1个月的备份
3. 每次copy备份的时候，都把太旧的备份给删了
4. 每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏



> 25. 在缓存一致性过程中如果MQ宕机了怎么办？







> 26. redis批处理？









## Redis内存过期策略

答：

Redis中提供了8种内存淘汰策略：

`volatile-lru：`针对设置了过期时间的key，使用LRU算法进行淘汰 
`allkeys-lru：`针对所有key使用LRU算法进行淘汰 
`volatile-lfu：`针对设置了过期时间的key，使用LFU算法进行淘汰 
`allkeys-lfu：`针对所有key使用LFU算法进行淘汰 
`volatile-random: `从设置了过期时间的key中随机删除  

`allkeys-random: `从所有key中随机删除 
`volatile-ttl：`删除生存时间最近的一个键 
`noeviction`（默认策略）：不删除键，返回错误OOM，只能读取不能写入 



总结一下：也就是从过期的key或者所有的key中使用 LRU或者LFU或者随机 删除策略进行淘汰







## 缓存雪崩是什么？如何解决？

答：缓存雪崩造成的原因是：大量缓存数据在同一时间过期或者Redis宕机，此时如果有大量的请求无法在Redis中处理，会直接访问数据库，从而导致数据库的压力骤增，甚至数据库宕机



出现原因：缓存过期、Redis故障

`缓存过期解决：`

1. 给过期时间加上一个随机数
2. 互斥锁，当缓存失效时，加互斥锁，保证同一时间只有一个请求来构建缓存
3. 缓存预热，在系统启动前，提前将热点数据加载到缓存中，避免大量请求同时访问数据库

`Redis故障解决：`

1. 服务熔断或请求限流
2. 构建Redis缓存高可靠集群





## 缓存穿透是什么？如何解决？

答：缓存穿透造成的原因是访问数据库中不存在的数据，即数据库和缓存都不命中

缓存穿透就是访问大量数据库中不存在的设备，每次都需要去数据库中查询，失去了缓存保护后端存储的意义。

`造成原因：`

- 自身代码问题
- 恶意攻击

`解决方案有两种：`

1. 如果访问数据库中不存在的数据，则将该数据设置为字符串`{}`并且放入缓存，避免访问不存在的数据而大量请求打到数据库，缓存 kv 格式为：`empty_cache_key: {}`

```java
// 设置key为空缓存
redisUtil.set(productCacheKey, "{}", 60 + new Random().nextInt(30), TimeUnit.SECONDS);
// 如果访问到空缓存，重新刷新空缓存的过期时间
redisUtil.expire(productCacheKey, 60 + new Random().nextInt(30), TimeUnit.SECONDS);
```

2. 布隆过滤器：在使用布隆过滤器时，先将所有数据hash到一个位图中，之后接收客户端请求时，先去布隆过滤器中判断数据是否存在，如果不存在，则直接返回空，不会请求数据库。

   在SpringBoot中，我们可以使用Guava提供的布隆过滤器实现缓存穿透的解决方案。








## 缓存击穿是什么？如何解决？

答：缓存击穿造成的原因是：热点key失效

同一时间批量添加数据，并且数据的过期时间相同，大量数据同一时间缓存失效可能导致大量请求直达数据库，如果请求过多，数据库会挂掉。

`解决：`

批量添加数据的话，在设置的过期时间上再加上一个随机时间即可。

```java
// 过期时间 = cache_timeout + new Random().nextInt(5) * 60 * 60;
```

还可通过互斥锁解决









## Redis 的 key 删除策略了解吗？

答：

- 惰性删除：`key`过期后任然留在内存中不做处理，当有请求操作这个`key`的时候，会检查这个`key`是否过期，如果过期则删除，否则返回`key`对应的数据信息。（惰性删除对CPU是友好的，因为只有在读取的时候检测到过期了才会将其删除。但对内存是不友好，如果过期键后续不被访问，那么这些过期键将积累在缓存中，对内存消耗是比较大的。）
- 定期删除：`Redis`数据库默认每隔`100ms`就会进行随机抽取一些设置过期时间的`key`进行检测，过期则删除。（定期删除是定时删除和惰性删除的一个折中方案。可以根据实际场景自定义这个间隔时间，在CPU资源和内存资源上作出权衡。）
- `Redis`默认采用定期+惰性删除策略。








## Redis 的主从机制有哪些好处？哪里存在问题？

答：

`主从机制好处：`

- 主从机制为后续的高可用机制打下了基础，可以将数据同步到多个从节点，做到灾备的效果
- 通过主写从读的形式实现读写分离，提高 Redis 吞吐量。

`主从机制同样存在一些问题：`

- 如果主节点宕机，需手动从 slave 选择一个新的 master，同时需修改应用方的主节点地址，还需要命令所有从节点去复制新的主节点。
- 写入性能受主节点限制
- 木桶效应：整个`Redis`节点群能够存储的数据容量受到所有节点中内存最小的那台限制，比如一主两从架构：`master=32GB、slave1=32GB、slave2=16GB`，那么整个`Redis`节点群能够存储的最大容量为`16GB`








## Redis Sentinel有什么作用？



答：

在主从集群基础上，使用 Sentinel（哨兵） 角色来帮我们监控 Redis 节点运行状态，并自动实现故障转移，当 master 节点出现故障时，Sentinel 根据规则选一个 slave 升级为 master，确保集群可用性，在这个过程中不需要人工介入。



Redis Sentinel的主要功能是：

- 监控 redis 节点状态是否正常
- 故障转移，确保 redis 系统的可用性



一般主从复制+哨兵一起使用，使用3台哨兵+1个主从集群（1master，2slave）



`Redis Sentinel + 主从模式 存在的问题：` 

- Redis Sentinel 在主节点挂了之后，选举主节点中断时间达几秒甚至十几秒，期间无法进行写入操作
- 只有一个主节点对外提供服务，无法提供很高的并发，并且单个节点内存不宜设置过大，导致持久化文件过大，影响数据恢复和主从同步效率








## Redis 切片集群了解吗？

答：

Redis 切片集群是目前使用比较多的方案，Redis 切面集群支持多个主从集群进行横向扩容，架构如下：

![1697707689793](imgs/1697707689793.png)

**使用切片集群有什么好处？**

- 提升 Redis 读写性能，之前的主从模式中，只有 1 个 master 可以进行写操作，在切片集群中，多个 master 承担写操作压力

- 多个主从集群进行存储数据，比单个主从集群存储数据更多

  比如10G数据，1个主从集群的话，1个master需要存储10G，对有3个主从集群的切片集群来说，只需要master1存储3G，master2存储3G，master3存储4G即可

- 具备主从复制、故障转移

  切片集群中的每一个主从集群中，slave 节点不支持读，只做数据备份，因为已经有其他master节点分担压力





**切片集群支持水平扩容，可以无限扩容吗？**

不可以，官方推荐不要超过1000个，因为各个小集群之间需要互相进行通信，如果水平节点过多，会影响通信效率。





**切片集群中插入数据时，数据被放在哪个master中？**

Redis 切片集群中，数据是通过 `哈希槽分区` 来存储的，Redis 切片集群中有 `16384` 个哈希槽，每一个 master 会拿到一些槽位，在向切片集群中插入数据时，会根据 key 计算对应的哈希槽，插入到对应的哈希槽中，那么计算出来的哈希槽在哪个master中，数据当然也就被存放在对应的master上。（在切片集群中，只有master节点才有插槽，slave节点没有插槽）





**切片集群中如果一个master挂了，如何选举主节点？**

当 master 挂了之后，该 master 下的所有 slave 会向所有节点广播 `FAILOVER_AUTH_REQUEST` 信息，其他节点收到后，只有 master 响应，master 会相应第一个收到的 `FAILOVER_AUTH_REQUEST` 信息，并返回一个 ack，尝试发送 failover 的 slave 会收集 master 返回的  `FAILOVER_AUTH_ACK`，当 slave 收到超过半数的 master 的 ack 后，就会变成新的 master。



**这样会导致 slave 一直没收到超过半数的 master 的 ack，难道要一直选举吗？**

其实不会导致 slave 一直选举的，因为在 slave 知道 master 挂了之后，会经过一个延时时间 `delay` 之后再去给所有节点发送选举消息

延迟时间计算：`delay = 500ms + random(0~500ms) + slave_rank * 1000ms` （版本不同可能不一样，原理大致相同）

slave_rank表示slave已经从master复制数据的总量的rank，rank越小，表示复制的数据越新，该slave节点也就越先发起选举。

因此数据量越多的 slave 就越早发送选举消息，也就越早得到 master 的 ack，成为新的 master。



**集群切片中的脑裂问题了解吗？如何解决？**

比如一个主从集群：master1 对应两个从节点 slave1、slave2，如果master1和两个从节点出现网络分区，两个slave会选举出来一个新的master节点，客户端是可以感知到两个master，但是两个master之间因为网络分区无法感知，客户端会向这两个master都写入数据，之后如果网络分区恢复，其中一个master变为slave，就会导致数据丢失问题。



如何解决？添加redis配置：

```bash
min-slave-to-write 1 
```

表示写数据时，写入master之后，不立即返回客户端写成功，而是去slave同步数据，取值为1表示最少同步1个slave之后，才算数据写成功，如果同步的slave节点数量没有达到我们配置的值，就算数据写失败，取值建议：集群总共3个节点，可以取1，这样集群中超过半数（1个master + 1个slave）都写入数据成功，才算写成功。

但是这个配置为了数据的一致性，牺牲了一定的集群可用性，如果一个master的所有slave都挂了，这个小集群就不可用了，master无法写入数据。

`一般不使用，redis丢一点数据也影响不大，所以主要还是保证redis的可用性` 







**集群是否完整才能对外提供服务？**

当redis.conf配置 `cluster-require-full-coverage no` 时，表示如果一个主从集群全部挂掉之后，集群仍然可用，如果为 yes，表示不可用

比如有3个主从集群，其中一个主从集群全部瘫痪，配置为no，则整个集群仍然可以正常工作





**Redis集群如何对批量操作命令的支持？**

对于 mset、mget 这样的多个 key 的原生批量操作命令，redis 集群只支持所有key落在同一个slot的情况，如果多个key一定要用mset在redis集群上操作，则可以在key之前加上{XX}，这样就会根据大括号中的内容计算分片hash，确保不同的key落在同一slot内，例如：

```
mset {user1}: name zhuge {user1}:age 18
```

虽然name和age计算出来的 hash slot 值不同，但是前边加上大括号{user1}，redis集群就会根据 user1 计算插槽值，最后name和age可以放入同一插槽。



# Spring 



## Spring IOC 了解吗？

Spring IOC 即控制反转（Inversion of Control），目的就是解决类与类之间的耦合

**控制反转怎么去理解呢？**

在使用 Spring 之前，我们如果需要创建对象的话，需要通过构造方法来创建：`A a = new A();` ，那么此时类的创建是由我们自己来管理的

在使用 Spring 之后，如果需要创建对象，我们在类上添加 `@Component` 注解即可，Spring 会扫描带有这个注解的类，将这些类创建一份实例对象，放入到 Spring 容器中进行保存，那么此时类的创建是交给了 Spring 框架来管理，控制权从我们手中转移到了外部，因此称为`控制反转`





## @Bean 标注的方法如何创建对象呢？

参考文章：https://blog.csdn.net/qq_35971258/article/details/128241353

> 下边只讲一下 @Bean 注解标注的方法，是如何去进行创建 bean，以及流程是怎样的，如果需要看源码具体执行流程，可以查看上边`参考文章`

Spring 的 @Bean 标注的方法会被创建成 Bean，这个创建的过程在源码中是怎么执行的呢？

```java
@Configuration
public class MyConfiguration {

    @Bean
    public Blue blue() {
        System.out.println("创建 Blue 对象");
        return new Blue();
    }

}
```

当使用上述代码时，在 Spring 中扫描到 MyConfiguration 类时，会发现 MyConfiguration 的 BeanDefinition 包含了 FactoryMethod，也就是 Spring 为带有 @Bean 注解的方法创建了一个 FactoryMethod，这个 FactoryMethod 也就是 `blue()` 方法，最后通过执行 FactoryMethod`blue()` 就可以创建出 Blue 对象，再将 Blue 对象交给 Spring 进行管理，最后创建的 Blue 对象的 beanName 就是 @Bean 标注的方法名

**总结一下：**@Bean 标注的方法会是一个 FactoryMethod，最后通过执行这个 FactoryMethod 来创建 Bean 对象，beanName 为方法名   `{beanName: blud, }`



**Spring 的 @Bean 标注的方法的参数如何注入的呢？**

```java
@Configuration
public class MyConfiguration {

    @Bean
    public Blue blue(White white) {
        System.out.println("创建 Blue 对象");
        System.out.println("获取了 White 对象：" + white);
        return new Blue();
    }

    @Bean
    public White white() {
        White white = new White();
        System.out.println("创建的 White 对象：" + white);
        return white;
    }
    /**
     * 输出：
     * 创建的 White 对象：com.zqy.springbean.SpringBean.beans.White@32c726ee
     * 创建 Blue 对象
     * 获取了 White 对象：com.zqy.springbean.SpringBean.beans.White@32c726ee
     */
}
```

**参数注入流程为：**

1. 对上边的 @Bean 方法来说，如果方法存在参数，会先根据参数名到 Spring 容器中取出对应的 bean，即先根据 `white` 取出对应的 bean 
2. 如果根据 `white` 没有取到对应的 bean，就会根据对象的类型去取，如果 Spring 容器中，`White` 类型的 Bean 只有 1 个，就会取出来进行赋值
3. 但是如果 Spring 容器中 `White` 类型的 Bean 有多个时，Spring 内部就会进行决策判断，通过决策判断出来需要使用哪个 Bean 进行赋值，如果没有决策成功，Spring 就会报错 `Parameter 0 of method blue in com.zqy.springbean.SpringBean.MyConfiguration required a single bean, but 2 were found:`，即 Spring 根据类型找到了多个 Bean，但是并不知道使用哪一个！

这个决策过程为：

1. 如果 Spring 找到多个同类型的 Bean，会查看是否有 Bean 标注了 @Primary 注解，如果有，就使用该 Bean 进行赋值
2. 如果没有标注 @Primary，则会判断这几个 Bean 是否实现了 @Comparator 排序接口，如果有，就使用该 Bean
3. 如果都没有找到，再根据入参名称和 beanName 进行比较，如果相同就使用该 Bean，否则报错



**下面我们对这几种情况进行代码演示：**

1. 根据 beanName 找到入参

   会发现根据 beanName 成功找到 bean 对象，赋值给 blue() 方法的入参

   ```java
   @Configuration
   public class MyConfiguration {
       @Bean
       public Blue blue(White white1) {
           System.out.println("获取了 White 对象：" + white1);
           return new Blue();
       }
       @Bean
       public White white1() {
           White white = new White();
           System.out.println("创建的 White1 对象：" + white);
           return white;
       }
       /**
        * 输出：
        创建的 White1 对象：com.zqy.springbean.SpringBean.beans.White@2c07545f
        获取了 White 对象：com.zqy.springbean.SpringBean.beans.White@2c07545f
        */
   }
   ```



2. 如果有多个同类型的 bean，根据 @Primary 找到 Bean 进行赋值

   可以发现，入参注入的 bean 对象为携带了 @Primary 注解的 bean

   ```java

   @Configuration
   public class MyConfiguration {
       @Bean
       public Blue blue(White white) {
           System.out.println("获取了 White 对象：" + white);
           return new Blue();
       }
       @Bean
       @Primary
       public White white1() {
           White white = new White();
           System.out.println("创建的 White1 对象，携带 @Primary 注解：" + white);
           return white;
       }
       @Bean
       public White white2() {
           White white = new White();
           System.out.println("创建的 White2 对象：" + white);
           return white;
       }
       /**
        * 输出：
        创建的 White1 对象，携带 @Primary 注解：com.zqy.springbean.SpringBean.beans.White@2c4d1ac
        获取了 White 对象：com.zqy.springbean.SpringBean.beans.White@2c4d1ac
        创建的 White2 对象：com.zqy.springbean.SpringBean.beans.White@7f0d96f2
        */
   }
   ```

3. 有多个同类型的 bean，根据`入参名`和`beanName`比较进行赋值

   可以发现，入参名为 `white1` ，White 类型的 bean 有两个，beanName 分别为 `white1` 和 `white2`，根据比较，入参使用 `white1` 的 bean

   ```java
   @Configuration
   public class MyConfiguration {
       @Bean
       public Blue blue(White white1) {
           System.out.println("获取了 White 对象：" + white1);
           return new Blue();
       }
       @Bean
       public White white1() {
           White white = new White();
           System.out.println("创建的 White1 对象：" + white);
           return white;
       }
       @Bean
       public White white2() {
           White white = new White();
           System.out.println("创建的 White2 对象：" + white);
           return white;
       }
       /**
        * 输出：
        创建的 White1 对象：com.zqy.springbean.SpringBean.beans.White@2c07545f
        获取了 White 对象：com.zqy.springbean.SpringBean.beans.White@2c07545f
        创建的 White2 对象：com.zqy.springbean.SpringBean.beans.White@e57b96d
        */
   }
   ```

   ​



> 1. Spring 循环依赖了解吗，怎么解决？

参考文章：https://juejin.cn/post/7146458376505917447

https://mp.weixin.qq.com/s/cqkZEvmmh7jnNt2K5r_lXg

https://juejin.cn/post/7218080360403615804



循环依赖是指 Bean 对象循环引用，是两个或多个 Bean 之间相互持有对方的引用，例如 `A -> B -> A`

单个对象的自我依赖也会出现循环依赖，但这种概率极低

- SpringBoot 2.6x 以前是默认允许循环依赖的，即使出现了循环依赖也不会报错。
- 解决办法：三级缓存机制，提前暴露的对象存放在三级缓存中，二级缓存存放半成品的 bean，一级缓存存放最终的 bean。但是三级缓存也存在一些缺点，比如增加了内存开销，需要维护3个map，降低了性能（需要进行多次检查和转换），并且少部分情况下不支持循环依赖，如非单例的 bean 和 @Async 注解的 bean 无法支持循环依赖



> 2. Spring框架用的什么版本，5和6相比有什么新特性，其他类似的框架，Spring除了ioc和aop还用过什么特性？



> 3. Spring Bean 的生命周期？AOP 在哪一步实现？aop 的实现原理，细说一下动态代理种类？



> 4. 为什么 ServiceImpl 里直接调用 this.xxx() 事务不生效？



> 5. 介绍下Spring中bean的创建和初始化过程 





> 6. bean的循坏依赖何时成功，何时失败？ 





> 7. 两个bean，A和B，A的构造函数有B，B注入A，循环依赖能初始化成功吗？ 





> 8. Spring 和 SpringBoot 区别？





> 9. @Aotuwired和@Resource区别？





> 10. aop ioc原理应用及相关注解





> 11. Spring源码用到的设计模式





> 12. 拦截器有几个方法，分别在什么时候执行，对比过滤器





> 13. Spring容器的启动过程







> Spring bean 线程安全吗



# MyBatis

> 1. 如果给你优化mybatis怎么优化？



> 2. ${字符串替换符}和#{占位符 预编译处理}的区别





> 3. 二级缓存



# Spring Boot

> 1. 使用的数据库连接池是什么？
> 2. Spring Boot 默认的数据库连接池是什么？
> 3. Spring Boot 如果需要增加 Redis 功能，需要在 pom 增加什么依赖？
> 4. 鉴权常用的办法？
> 5. Spring Boot 中哪里用到了反射机制？



> 6. 说一下 springboot 的自动配置





> 7. 导入一个 jar 包怎么让 springboot 知道哪些需要自动配置(不确定是不是这样表达)













# Spring Cloud

> 1. Spring Cloud 和 Dubbo 的区别



> 2. Sentinel 熔断机制




> 3. Spring Gateway 网关的作用





> 4. 注册中心是做什么的？







# Dubbo

> 1. dubbo 的请求处理流程











# 分布式

> 1. raft 算法选举流程？





> 2. 怎么理解 Rpc？





> 3. RPC的延展技术，就是gRPC,Dubbo了解吗？





> 4. RPC 、Http、Rset 使用场景有什么不同？





> 5. rpc有多少dubbo的功能？如何暴露服务？服务挂了怎么办？





> 6. rpc过程和用户态内核态的关系








# 多线程

项目推荐：京东的 asyncTool 并发框架，大量使用到了 `CompletableFuture`。



> 1. 如何设计一个能够根据优先级来执行的线程池？

首先对于阻塞队列，可以考虑使用 `PriorityBlockingQueue `作为任务队列。

`PriorityBlockingQueue` 是一个支持优先级的无解阻塞队列，要想对任务进行排序，需要让提交到线程池的任务实现 `Comparable` 接口，并重写 `compareTo` 方法来指定任务之间的优先级比较规则，还有一种方式就是创建 `PriorityBlockingQueue` 时传入一个 Comparator 对象来制定任务之间的排序规则（推荐第二种方式）。

但是还存在几个问题：

1. 在使用优先级任务队列时，当生产者速度快于消费者时，时间长之后会造成 OOM，因为该队列并不会阻塞生产者，只会阻塞消费者，当没有任务消费时，会阻塞消费者
2. 会导致饥饿问题，即优先级低的任务长时间不执行
3. 由于对队列中的元素进行排序以及保证线程安全（并发控制采用的可重入锁 ReentrantLock），因此会降低性能

对于 OOM 问题，可以继承 `PriorityBlockingQueue` 并且重写 `offer` 方法，即入队逻辑，当插入的元素数量超过指定值就返回 false

饥饿问题可以通过优化设计来解决，比如等待时间过长的任务会被移除，并重新添加到队列中，并且提升优先级



> 2. 线程池原理？线程池如何回收线程？（怎么知道任务处理完毕？）依据什么去设置核心线程数？(CPU密集型和IO密集型）









> 3. threadlocal 底层实现







> 4. 为什么threadlocal的key是弱引用





> 5. 怎么处理线程安全的问题？（我说了死锁和threadlocal)





> 6. thread内存泄漏





> 7. 怎么获取子线程的返回值？





> 8. 子线程抛异常，主线程try-catch 是否可以获取到异常 。















# Linux

> 1. linux命令问题，如何在一个文件里搜索一关键字（grep）如果想找第一个呢？找最后一个呢？（head和tail?)





> 2. linux 常用命令







# 工作流相关资源（未整理）

总结了一些工作流引擎相关的优质文章和开源项目，分享一下，最近有好几个球友问到。内容较多，建议收藏，后续如果看到比较优质的工作流学习资源会持续同步到这里来。

一些讲解工作流引擎的优质文章：

1. 老板要我开发一个简单的工作流引擎（ [老板要我开发一个简单的工作流引擎 - MCTW - 博客园](https://www.cnblogs.com/duck-and-duck/p/14436373.html) ）：比较有意思的一篇文章，通过过关的形式讲解工作流引擎的开发设计。
2. 一文读懂工作流（ [一文读懂工作流 - 知乎](https://zhuanlan.zhihu.com/p/113387814) ）：网上关于工作流引擎有比较多的简介，也有很多工作流的实际应用场景。本文结合笔者多年对工作流的经验来阐述一下对工作流的理解。
3. 工作流引擎原理-打造一款适合自己的工作流引擎（ [工作流引擎原理-打造一款适合自己的工作流引擎 - 掘金](https://juejin.cn/post/6844904167463485453) ）：作为开发人员或多或少都会接触过工作流引擎，如 activiti、 jbpm 等，这些工作流引擎都会比较重。在小项目中引入就会显得不是很优雅。本文主要简单介绍工作流引擎的原理并讲述如何慢慢打造一款适合自己的工作流引擎。
4. Flowable 开篇，流程引擎扫盲（ [Flowable 开篇，流程引擎扫盲 - 掘金](https://juejin.cn/post/7148248663762927653) ） ：介绍了为什么需要流程引擎，以及开发工作流时用到的一些工具。
5. SpringBoot+Vue+Flowable，模拟一个请假审批流程（ [SpringBoot Vue Flowable，模拟一个请假审批流程！ - 掘金](https://juejin.cn/post/7130150061257785351) ） ：一个很不错的基于 Flowable 的请假审批流程实战。

开源工作流引擎：

1. Flowable ：[Open Source](https://flowable.com/open-source/)
2. Activiti：<https://www.activiti.org/>

开源工作流实战项目：

1. [RuoYi-flowable: 🌟 基于RuoYi-vue    flowable 6.7.2 的工...](https://gitee.com/tony2y/RuoYi-flowable)：基于RuoYi-Vue +Flowable6.x的工作流管理平台。
2. [RuoYi-Flowable-Plus: 本项目基于 RuoYi-Vue-Plus 进行二次开发扩展...](https://gitee.com/KonBAI-Q/ruoyi-flowable-plus)：基于 RuoYi-Vue-Plus 进行二次开发扩展Flowable工作流功能，支持在线表单设计和丰富的工作流程设计能力。

国内用的比较多的还是 Flowable 和 Activiti 这两个，参考资料也蛮多的。Camunda 也不错，更轻量，功能也很完善，性能和稳定性也很不错。关于开源流程引擎的选择，可以参考这篇文章：[开源流程引擎哪个好，如何选型？ - 知乎](https://zhuanlan.zhihu.com/p/369761832) 。

多提一点，国内比较火的工作流引擎 LiteFlow（ [LiteFlow](https://liteflow.cc/) ） 只做基于逻辑的流转，而不做基于角色任务的流转。如果你想做基于角色任务的流转，推荐使用 Flowable 和 Activiti 这两个框架。也就是说，像审批流（A 审批完应该是 B 审批，然后再流转到 C 角色）这种 LiteFlow 就不适合了。LiteFlow 适用于拥有复杂逻辑的业务，比如说价格引擎，下单流程等，这些业务往往都拥有很多步骤，这些步骤完全可以按照业务粒度拆分成一个个独立的组件，进行装配复用变更。







# 算法题

> 1. 单例模式实现

**静态内部类（推荐）**

```java
public class Singleton {
    // 私有化构造方法
    private Singleton() {
    }

    // 对外提供获取实例的公共方法
    public static Singleton getInstance() {
        return SingletonInner.INSTANCE;
    }

    // 定义静态内部类
    private static class SingletonInner{
        private final static Singleton INSTANCE = new Singleton();
    }
}
```

优点：

1. INSTANCE 的唯一性、创建过程的线程安全性都由 JVM 保证。
2. 支持延时加载，当 Singleton 类加载时，不会创建静态内部类，调用 `getInstance()` 方法时，才会创建实例。





**双重校验锁**



```java
public class Singleton {

    private volatile static Singleton uniqueInstance;

    // 私有化构造方法
    private Singleton() {
    }

    public  static Singleton getUniqueInstance() {
       //先判断对象是否已经实例过，没有实例化过才进入加锁代码
        if (uniqueInstance == null) {
            //类对象加锁
            synchronized (Singleton.class) {
                if (uniqueInstance == null) {
                    uniqueInstance = new Singleton();
                }
            }
        }
        return uniqueInstance;
    }
}
```

`uniqueInstance` 采用 `volatile` 关键字修饰也是很有必要的， `uniqueInstance = new Singleton();` 这段代码其实是分为三步执行：

1. 为 `uniqueInstance` 分配内存空间
2. 初始化 `uniqueInstance`
3. 将 `uniqueInstance` 指向分配的内存地址

但是由于 JVM 具有指令重排的特性，执行顺序有可能变成 1->3->2。指令重排在单线程环境下不会出现问题，但是在多线程环境下会导致一个线程获得还没有初始化的实例。例如，线程 T1 执行了 1 和 3，此时 T2 调用 `getUniqueInstance`() 后发现 `uniqueInstance` 不为空，因此返回 `uniqueInstance`，但此时 `uniqueInstance` 还未被初始化。





> 2. 算法:手撕LRU







> 3. 用程序实现两个线程交替打印 0~100 的奇偶数。





# k8s





https://t.zsxq.com/13G5IiAbm







# DDD

1、阿里技术专家详解 DDD 系列 第一讲- Domain Primitive：[阿里技术专家详解DDD系列第一讲-DomainPrimitive-知乎](https://zhuanlan.zhihu.com/p/340911587)
2、阿里技术专家详解 DDD 系列 第二讲 - 应用架构：[阿里技术专家详解DDD系列第二讲-应用架构-知乎](https://zhuanlan.zhihu.com/p/343388831)
3、阿里技术专家详解 DDD 系列 第三讲 - Repository 模式：[阿里技术专家详解DDD系列第三讲-Repository模式-知乎](https://zhuanlan.zhihu.com/p/348706530)
4、阿里技术专家详解 DDD 系列 第四讲 - 领域层设计规范：[阿里技术专家详解DDD系列第四讲-领域层设计规范-知乎](https://zhuanlan.zhihu.com/p/356518017)
5、阿里技术专家详解 DDD 系列 第五讲：聊聊如何避免写流水账代码：[阿里技术专家详解DDD系列第五讲聊聊如何避免写流水账代码-知乎](https://zhuanlan.zhihu.com/p/366395817)
6、DDD 系列文章 - ThoughtWorks 洞见：[您正搜索DDD-Thoughtworks洞见](https://insights.thoughtworks.cn/?s=DDD)
7、手把手教你落地 DDD : [手把手教你落地DDD](https://mp.weixin.qq.com/s/TU5G2o_HOctjWvuyOEK_tA)
8、探秘微信业务优化：DDD 从入门到实践：[探秘微信业务优化DDD从入门到实践](https://mp.weixin.qq.com/s/kFjfzwTOdaKA2ym63VR3DQ)
9、领域驱动设计 DDD ｜从入门到代码实践：[领域驱动设计DDD从入门到代码实践](https://mp.weixin.qq.com/s/HMLpjcE0UENUTfMK0Z9n8A)









# 性能压测

> 1. 性能压测怎么做的，性能如何？











# CQRS



今天下午认真看了一篇很不错的文章：[从1到亿，如何玩好异步消息？CQRS架构下的异步事件治理实践](https://mp.weixin.qq.com/s/mwZQYk1vJu6rOGzxuNeGyg) （从 1 到亿，如何玩好异步消息？CQRS 架构下的异步事件治理实践 - 哔哩哔哩技术 - 2023），分享给球友们。

这篇文章循序渐进，一步一步地教你构建微服务+ CQRS 的高并发架构，以点赞功能作为例子（见图1）。

这篇文章提到了核心之一在于 CQRS 模式的运用。CQRS 是“命令和查询责任分离（Command Query Responsibility Segregation）”的英文缩写，它是一种将数据存储的读取操作和更新操作分离的模式。

要理解 CQRS 模式，我们可以把它分成两个部分：命令（Command）和查询（Query）。命令是指对数据进行修改的操作，比如用户点赞行为。查询是指对数据进行读取的操作，比如用户查看点赞数。在传统的模式中，命令和查询都是直接对同一个数据库进行操作，这样会导致一些问题，比如数据库锁竞争、性能瓶颈等。

在 CQRS 模式中，命令和查询被分离到不同的服务或数据库中，使用消息队列或事件流来同步数据。你可以将 Command 作为一个独立的微服务，它主要的任务是不断地创建新事件比如用户点赞行为。这些事件会被存储下来（比如消息队列存储），并经过一系列的处理。通常处理这些事件的服务是另外的微服务。Command 服务不需要等待数据库的结果，就可以直接返回给用户。并且，即使数据库出现故障，也不会影响用户的操作，等数据库恢复之后再进行消费即可。

关于 CQRS 模式的详细介绍，可以参考下面这几篇文章：

- [CQRS 模式 - Azure Architecture Center | Microsoft Le...](https://learn.microsoft.com/zh-cn/azure/architecture/patterns/cqrs) 
- [CQRS](https://martinfowler.com/bliki/CQRS.html)

高并发下，使用 CQRS 模式能规避锁竞争以及解决写流程直接依赖 DB 的问题。

以点赞服务为例，CQRS 模式下：

- 用户点赞时，点赞服务将消息投递给 Kafka 后就直接返回，用户体验很好。新增了一个 Job 服务负责消费异步事件，从 Kafka 中获取到消息后处理和写入至后续的 MySQL 和缓存中。即使数据库出现故障，也不会影响用户点赞，等数据库恢复之后再进行消费即可。
- 对于数据库锁竞争的问题，只要避免对同一条记录同时写入就可以规避 ，设置 Kafka 的生产者分区策略为 Key-Ordering，然后将视频 id 当做消息键，这样同一个视频的消息只会被一个单个 job 消费串行处理，竞争问题解决了。

既然引入了 Job 服务，那我们就要考虑如何去保证 Job 服务的高可用？Job 服务出现故障或者重启，会不会导致消息的丢失或者重复处理？消费能力不足怎么办？

在这篇文章中：

- 消息重复消费问题是通过行为表（记录点赞信息比如用户点赞了哪条视频）来规避的，用户点赞的时候会先从行为表中检查是否已经点赞过了。
- 消费能力不足的问题是通过增加消费节点和单节点消费能力来解决的。
- 消息丢失问题是将消息有序串联起来并对处理完的消息标记 ACK 解决的（某个消息前面所有的事件都被处理后，才将消息设置 ACK 状态）。并且，文中还提到了如何减少 ACK 来进一步优化性能。
- MQ 系统虽然是多副本高可用的，但是总有例外会导致失效。针对 MQ 故障问题，这篇文章介绍了一种新的降级方式：正常情况下生产方发送消息到 kafka，被 job 节点消费，当 kafka 故障时，发送方自动切换到降级模式，将消息直接推向消费方

在数据库中会有一张专门的表用来记录某个视频的点赞数量 count。为了减少对数据库的操作，可以将同一个视频的 N 个点赞聚合起来，一次性对 count（点赞数量） 加 N。

为了应对热点事件，还可以对热点问题做一下事件隔离，避免影响到其他的视频。

最后，附送一张处理流程图，如下图所示（图二）。

![1696932241170](imgs/1696932241170.png)









# 计算机网络

给 JavaGuide 的各位小伙伴推荐一下我的计算机网络学习计划。

这是一篇我认真总结出来的计算机网络学习方法 & 学习计划，我也在亲身实践。事实证明精华的学习路线起码 10 年为一个周期的话，不会过时。

我觉得最重要的就是**看书(博客) + 实践**。

首先是看书。

## **书籍推荐**

书也分为不同的层次，最基础的入门书籍有

### **网络是怎样连接的**

这是我推荐给你的第一本书。![img](https://article-images.zsxq.com/Fi5KmAxAhMG-eg_XlbI2ruov0zw6)

这本书是日本人写的，它和《程序是怎样运行的》、《计算机是怎样跑起来的》统称为图解入门系列，最大的特点就是**风趣幽默，简单易懂**。这本书通过多图来解释浏览器中从输入网址开始，一路追踪了到显示出网页内容为止的整个过程，以图配文，讲解了网络的全貌，并重点介绍了实际的网络设备和软件是如何工作的。

本书图文并茂，通俗易懂，非常适合计算机、网络爱好者及相关从业人员阅读。

所以如果大家是新手的话，强烈推荐一下这本书。

日本人就爱图解，同样图解系列的入门书籍还有《图解 HTTP》、《图解 TCP/IP》。

### **图解 HTTP**

这是我推荐给你的第二本书。

![img](https://article-images.zsxq.com/Fu15zRD1g8Rx9rFdWaDk6QuFKH4G)

《图解 HTTP》是 HTTP 协议的入门书籍，当然 HTTP 也是属于计算机网络的范畴，这本书适合于**想要对 HTTP 有基本认知的程序员，同样也适合查漏补缺**。

这类书看起来就毫无难度了，不得不说图解系列是给小白的圣经，它能增强你的自信，让你觉得计算机其实没那么难，这是非常重要的。初学者，最怕的就是劝退了。

### **图解 TCP/IP**

这是我推荐给你的第三本书

![img](https://article-images.zsxq.com/Fvfv7f-5I7yz2W3O2UecN-VsZVFG)

上面的图解 HTTP 是针对 HTTP 协议的，那么《图解 TCP/IP》就是针对 TCP/IP 协议簇中的协议了，这本书我已经看了 80% 了，还是比较系统的一本书，**基本上涵盖了 TCP/IP 协议簇中的所有协议知识了，这本书看完了完全就可以直接深入理解 TCP/IP 协议簇了**。

对于新手来说，最重要的一点就是帮助你理解，怎么简单怎么来，这样才能快速入门，对于快餐式的社会来说，快速理解当然是当仁不让的首选了。

如果上面这几本书你都搞定了的话，那你就可以读一下 《计算机网络：自顶向下方法》这本书了，这本书可以作为基础书籍也可以作为进阶书籍，这里我归为了进阶书籍，因为里面有一些章节不是那么好理解，比如介绍网络层的时候，会分为数据平面和控制平面，介绍 TCP 和 UDP 的时候，也会聊到一些原理性问题。

### **计算机网络：自顶向下方法**

这是我推荐给你的第四本书![img](https://article-images.zsxq.com/FgqDuld1_xQerp1m_dzh2ny9dT_D)

这本书是一本计算机网络的圣经书籍，圣经就在于人人都应该读一下这本书，原著非常经典，翻译也很不错，我自己也马上就看完了，这本书会从顶层也就是网络层逐步下探到物理层，一层一层的带你入门，解释各层之间的协议，主要特征是什么，一个数据包的发送历程。这本书并不局限于某个具体的协议，而是从宏观的角度来看待计算机网络到底是什么，里面有一些专业名词，理解并掌握后会对深入学习计算机网络非常有用。

### **计算机网络：谢希仁版**

这是我推荐给你的第五本书籍![img](https://article-images.zsxq.com/FmFfU1xQ0q2_jCEQijiGdXWljPKp)

这本书是很多大学的教材，也是一本非常好的进阶书籍，这本书相对于自顶向下方法更多是对于通信网络的阐述。

这本书的特点是**突出基本原理和基本概念的阐述**，同时力图反映计算机网络的一些最新发展。本书可供电气信息类和计算机类专业的大学本科生和研究生使用，对从事计算机网络工作的工程技术人员也有参考价值

现在我们接着聊，如果上面这两本书随便一本看完了，那么恭喜你已经是一个老手了，你的网络基础能打败 90% 以上的人了，如果你还不满足的话，那你就需要继续深入，继续深入也是我推荐给你的提高书籍。

### **HTTP 权威指南**

这是我推荐给你的第六本书

![img](https://article-images.zsxq.com/FhvZLlIG1fJ-lQhfHQvIwqeUl8JF)

HTTP 权威指南是深入 HTTP 非常值得一看的书，这本书写的非常全了。

此书第一部分是 HTTP 的概述，如果你没有时间，通读第一部分就能让你应付普通的日常开发工作。

第二部分主要讲现实世界中 HTTP 的架构，也可以看作 HTTP 的全景图，包括 Web Server/Cache/Proxy/Gateway，是全书中精华的部分。

第三部分主要是 HTTP 安全，其中 Basic 和 Digest 概略看下即可，现实世界中用的应该不多。看 HTTPS 最好有一些计算机安全基础，这样会顺畅很多。

第四部分主要是关于 HTTP Message Body 的部分，包括 Content Negotiation，MIME Type，chunked encoding等，概略看下即可。

第五部分的内容，Web Hosting 可以认真看下，了解下 Virtual Host(话说我上学的时候一直搞不懂 Virtual Host，一个 IP 怎么能同时 Host 两个不同域名的 Web 页面呢)。

剩下三章已经过时，基本可以忽略。最后的附录，可以用作边用边学的字典，如果你自己来写 Web Server，那么这一部分是极有价值的参考。

总而言之，无论你是前端还是后端，只要是 Web 相关的，那么此书就是必读的。

**TCP/IP 详解**

这是我推荐给你的第七本书

这是一本被翻译耽误的经典书，两个硬核作者 Kevin R. Fall 和 W. Richard Stevens 被南开大学的某计算机系的译者给毁了。我第一开始读这本书以为是自己智商不够，原来是翻译瞎TM翻啊。语句不通且不说，您好歹走点心，改点措辞也行啊，纯粹是生搬硬套谷歌翻译啊，哎。

![img](https://article-images.zsxq.com/Fi0WBgF0rKGndKoWDp3nDvHTQnY4)

来看看豆瓣读者们对这本书的评价吧，比我有力量多了。

![img](https://article-images.zsxq.com/Fg9Eu7-eK7npY5pALWCMnAu23euC)

这个回答给我看乐了，嗯，把这本书当作一本 Google 词典确实是一种不错的选择。

不过这本书确实是一本非常好的书，这本书的关注点在于 TCP/IP 协议栈上，可以说把 TCP/IP 讲透讲细了，比如讲 TCP 就会分别从连接管理、TCP 超时重传、TCP 拥塞控制、TCP 保活机制来讲起，不管你是从事哪个技术栈的研究，不管你是程序员还是网络工程师，这本书都是你值得一读的一本，不过要读最好还是读英文版。

TCP/IP 详解有三本，第二本是

![img](https://article-images.zsxq.com/FlxbXQwuOhqAIwr0QToz1M7COsRh)

这本黑皮书主要是介绍如何实现 TCP/IP 协议的，这本书很难入门。书中给出了约 500 个图例，15000 行实际操作的 C 代码，采用举例教学的方法帮助你掌握 TCP/IP 实现。

本书不仅说明了插口 API 和协议族的关系以及主机实现与路由器实现的差别。还介绍了 4.4BSD-Lite 版的新的特点。本书适用于希望理解 TCP/IP 协议如何实现的人，包括编写网络应用程序的程序员以及利用 TCP/IP 维护计算机网络的系统管理员。

第三卷是 tcp 事务协议、http、nntp 和 unix 域协议

![img](https://article-images.zsxq.com/FhaN-sv3XrWaWLITcUB_0lz5qho_)

这本书看的人就更少了。

第 3 卷详细介绍了当今 TCP/IP 程序员和网络管理员必须非常熟悉的四个基本主题：TCP 的扩展、Hyper 文本传输协议、网络新闻传输协议和 UNIX 域协议。与前两卷一样，本书介绍了 4.4BSD-Lite 网络代码中的示例和实现细节。

嗯。。。有一些沉重了，其实这些深入协议底层的书籍我们 99% 的人都接触不到，但是为了回答的完整性，我就都列出来了，这样的好处是让你能系统了解。

上面都是一些理论书籍，下面是稍微偏实战一些的书籍了。

计算机网络实战最有效的当然就属于抓包了，有很多抓包工具比如 **wireshark、sniffer、httpwatch、iptool、fiddle** 等，但是我用的和使用频率最高的应该就是 wireshark 了，关于 wireshark 还有几本实战方面的书你需要知道

### **wireshark 数据包分析实战**

这是我推荐给你的第八本书

![img](https://article-images.zsxq.com/FvppUmxzMv81vsjmwi42Fqzhfx7z)

初学者必备，介绍了 wireshark 安装，嗅探网络流量，wireshark 的基本使用，用 wireshark 分析了一圈常用的TCP，UDP 协议，也简要分析了 HTTP 等应用层协议，概要介绍了一些 TCP 重传的机制，最后是无线分析。

整个书定位应该是入门级别的，基本上每章都是简要介绍，并没有特别深入大张阔斧地进行描述。文章行文思路清晰，译者的翻译水平也不错。**总的来说，是初步认识和了解 wireshark 的好书**。

### **wireshark 网络分析就是这么简单**

这是我推荐给你的第九本书

![img](https://article-images.zsxq.com/Fio4oZCET3lZHwWSKrjhn78UH4RC)

读的时候你会忍不住笑的，区别于《Wireshark数据包分析实战》，本书就像一本侦探小说集，以幽默风趣的语言风格，借助wireshark以理性的思考来不断探险，根据蛛丝马迹来侦破案情。总结，读完数据包分析实战来读这本。

### **Wireshark网络分析实战**

这是我推荐给你的第十本书

![img](https://article-images.zsxq.com/Fi9pm3t12EvjNbUSt0eKsh0OReAl)

其内容涵盖了 Wireshark 的基础知识，抓包过滤器的用法，显示过滤器的用法，基本/高级信息统计工具的用法，Expert Info 工具的用法，Wiresahrk 在 Ethernet、LAN 及无线 LAN 中的用法，ARP 和 IP 故障分析，TCP/UDP故障分析，HTTP 和 DNS 故障分析，企业网应用程序行为分析，SIP 多媒体和 IP电话，排除由低带宽或高延迟所引发的故障，认识网络安全等知识。

书籍推荐大概就是上面那些，除了书之外，还有一些视频、博客、官网网站可以学习

## **视频推荐**

很烦微信公众平台怎么不支持外链呢，这个体验就很差啊。

今天在 b 站看视频的时候，看到了一句话**众所周知，b 站是用来搞学习的**，对于我们学习编程的童鞋来说，b 站有着非常多的学习资源，但是有一些质量并不是很好，看了之后不容易理解，这也是写这一篇文章的原因，为大家分享一些质量超高的计算机基础的学习视频，往下看就完了。

### **1. 计算机网络微课堂（有字幕无背景音乐版）（陆续更新中......）_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili**

学习计算机网络，我首先推荐的 UP 主湖科大教书匠，他讲的计算机网络十分通俗易懂，重点的地方讲的十分细致，并且还有一些实验，更好的是有考研 408 的难题的讲解，也是非常适合考研党，除了课程内容外还有很多习题讲解视频，特别赞的一点是每天动态里都会更新一道考研题，播放量也非常的多。

![img](https://article-images.zsxq.com/FjjlsABnO7qTUgeOoC10xaHM2P0Z)

### **2. 2019 王道考研 计算机网络_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili**

既然说到了考研，那我就不得不提一下王道考研了，恭喜你发现了宝藏。王道考研的计算机网络视频，播放量非常多，而且老师是一位小姐姐，声音十分动听，声音这么好听的老师给你讲课，妈妈再也不用担心我的学习了呢，总之，这个视频的质量也非常高，弹幕全是对小姐姐的高度评价。（王道考研其他的视频也不错哦，暗示一下：操作系统，数据结构等等）

![img](https://article-images.zsxq.com/FsK4YYEnsrISptX8fiOVe9GN_n0W)

### **3. 韩立刚计算机网络 谢希仁 第7版 2020年12月_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili**

韩立刚老师所讲的计算机网络视频，内容比较多，但是讲解的通俗易懂，并且老师讲课的经验也十分的丰富。配套的教材是谢希仁老师的计算机网络教材，韩老师的最近的一个视频视频比较新，播放量还比较少，但是他讲的是真的不错，相比于王道考研所讲的计算机网络，韩老师更加细致一些。

![img](https://article-images.zsxq.com/Fm01rzTihy2SQ2RMZDUeWmmdh6NU)

### **4. 计算机网络（谢希仁第七版）-方老师_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili**

在计算机网络方面，我还想推荐的一位老师就是方老师，也是一位小姐姐老师。她的视频配套的教材也是谢老师的网络教材，在线看的小伙伴也超多，弹幕都是对方老师的评价。

![img](https://article-images.zsxq.com/FvxWF9wOLuuvx08IrdE61gp4UUY7)

## **博客推荐**

推荐几个不错的学习博客。

互联网协议入门-阮一峰：<http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i....>

网络协议-兰亭风雨：<http://blog.csdn.net/ns_code/article/category/1805481>

HTTP协议：<http://www.cnblogs.com/TankXiao/category/415412.html>

Unix 网络编程：<http://blog.csdn.net/chenhanzhun/article/category/2767131/2>

TCP/IP详解：<http://blog.csdn.net/chenhanzhun/article/category/2734921/1>

计算机网络面试题：<http://blog.csdn.net/shadowkiss/article/details/6552144>

国外优秀计算机网络站点：<http://www.tcpipguide.com/free/t_TCPSlidingWindowAcknowledgmentSystemForDataTranspo-6.htm>

当然最硬核的就是 RFC 文档了 <https://tools.ietf.org/rfc/index>

学习 HTTP ，必须要看一下 MDN 官网 <https://developer.mozilla.org/zh-CN/docs/Web/HTTP>

学习计算机网络，Cloudflare 你必须要去看 [https://www.cloudflare.com/zh-cn/learning/](https://www.cloudflare.com/zh-cn/learning)

GeeksforGeeks 学习计算机网络也非常不错 [https://www.geeksforgeeks.org/basics-computer-networking/](https://www.geeksforgeeks.org/basics-computer-networking)

Tutorialspoint 系统学习计算机，不仅仅局限于计算机网络 <https://www.tutorialspoint.com/computer_fundamentals/computer_networking.htm>

国外优秀的学习网站不能少了 javapoint <https://www.javatpoint.com/types-of-computer-network>

------------------------分割线------------------------

原文链接：[计算机网络学习计划](https://mp.weixin.qq.com/s?__biz=MzI0ODk2NDIyMQ==&mid=2247488287&idx=1&sn=cb169ed027447d691923dba9ae83d887&chksm=e999e60ddeee6f1b0ff69018aa8e4eb826306a99d8279ea4f8aa834bb870803f35cb2b69590d&token=1010811083&lang=zh_CN#rd) ，这篇文章的作者也是我的个人公众号，有兴趣的小伙伴们可以点个关注呀。